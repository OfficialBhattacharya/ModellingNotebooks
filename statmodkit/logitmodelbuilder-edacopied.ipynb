{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":34377,"databundleVersionId":3220602,"sourceType":"competition"}],"dockerImageVersionId":30527,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install Levenshtein","metadata":{"id":"Tf-3y1tS9wVt","outputId":"132ee859-cf41-495d-82b5-ec5da42faeeb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\ndef get_csv_column_names(folder_path, num_workers=4):\n    column_names_dict = {}\n    \n    # Function to read column names from a CSV file\n    def read_columns(file_path):\n        try:\n            df = pd.read_csv(file_path, nrows=0)\n            return file_path, df.columns.tolist()\n        except Exception as e:\n            print(f\"Error reading {file_path}: {e}\")\n            return file_path, []\n\n    # Traverse the directory and get all CSV file paths\n    csv_files = []\n    for root, _, files in os.walk(folder_path):\n        for file in files:\n            if file.endswith('.csv'):\n                csv_files.append(os.path.join(root, file))\n\n    # Use ThreadPoolExecutor for parallel processing\n    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n        future_to_file = {executor.submit(read_columns, file): file for file in csv_files}\n        \n        for future in as_completed(future_to_file):\n            file_path, columns = future.result()\n            file_name = os.path.basename(file_path)\n            column_names_dict[file_name] = columns\n\n    return column_names_dict\n\n# Example usage\nfolder_path = 'path/to/your/folder'\nnum_workers = 4\ncolumn_names_dict = get_csv_column_names(folder_path, num_workers)\nprint(column_names_dict)\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"column_names_dict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier, VotingClassifier\nfrom sklearn.metrics import accuracy_score","metadata":{"id":"lFKoG0Ov9by3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/spaceship-titanic/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/spaceship-titanic/test.csv\")\n# concatenating first train data and test data, helpfull for applying same imputation and feature enginnearing.\ndata = pd.concat([data, test], ignore_index=True)\ndata","metadata":{"id":"L9phwkDW9rmS","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['NoOfPassenger'] = data[\"PassengerId\"].apply(lambda x: int(x[-2:]))  # 03 passenger\ndata['PassengerId'] = data[\"PassengerId\"].apply(lambda x: x[:4])          # 0013 passenger id\ndata[['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']] = data[['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']].fillna(0)\ndata['ExpenseInShip'] = data['RoomService'] + data['FoodCourt'] + data['ShoppingMall'] + data['Spa'] + data['VRDeck']\nsbsp = data.groupby((\"PassengerId\")).NoOfPassenger.count()\ndata = pd.merge(data, sbsp, on='PassengerId', how='inner')\ndata['SibSp'] = data['NoOfPassenger_y']\ndata.drop(columns= ['NoOfPassenger_x', 'NoOfPassenger_y'], inplace=True)\ndata['surname'] = data.Name.apply(lambda x: x.split()[1] if type(x) == str else x)\ndata['Cabin'] = data['Cabin'].apply(lambda x: str(x).split('/') if pd.notna(x) and x != -1 else x)\ndata['deck'] = data['Cabin'].apply(lambda x: x[0] if isinstance(x, list) and len(x) > 0 else np.nan)\ndata['num'] = data['Cabin'].apply(lambda x: x[1] if isinstance(x, list) and len(x) > 1 else np.nan)\ndata['side'] = data['Cabin'].apply(lambda x: x[2] if isinstance(x, list) and len(x) > 2 else np.nan)","metadata":{"id":"AKc5XD-m9sZK","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.pie(data.HomePlanet.value_counts(),labels = ['Earth','Europa','Mars'])\nplt.title(\"HomePlanet\")\nplt.show()","metadata":{"id":"hj3e4efjczRR","outputId":"989a3a04-50b5-47b0-8ddc-3cba787ce98e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[data.VIP == True].HomePlanet.value_counts()","metadata":{"id":"T9YkFnnsdM2m","outputId":"5b79b036-7acf-4ba5-eb27-dc46d46ce573","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[(data.VIP == True) & (data.HomePlanet == \"Mars\")].CryoSleep.value_counts()","metadata":{"id":"wb8onyPKelkP","outputId":"be056ef2-7259-41e3-d2d3-12d2ab837e38","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function for fillng missing values\ndef fill_homeplanet(row):\n    if pd.notna(row['HomePlanet']):\n        return row['HomePlanet']\n\n    if (pd.notna(row['VIP']) and pd.notna(row['CryoSleep'])):\n        if row['VIP'] and row['CryoSleep'] == False:              # if VIP = True and Cryosleep = False\n            return 'Mars'\n\n    if pd.notna(row['VIP']):\n        return 'Europa' if row['VIP'] else 'Earth'\n    return row['HomePlanet']\n\ndata['HomePlanet'] = data.apply(fill_homeplanet, axis=1)","metadata":{"id":"ub4-JhV1c9TB","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.HomePlanet.isnull().sum()","metadata":{"id":"yFSLdAEfh04C","outputId":"335c654e-a346-4e87-eed1-4894fafb134b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.HomePlanet.fillna(\"Earth\", inplace=True)","metadata":{"id":"HoeBLOoDk_Sb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[(data.ExpenseInShip == 0.0) & (data.CryoSleep == False) & (data.Age <= 18)].VIP.value_counts()","metadata":{"id":"jG5gg6bkVadS","outputId":"648af89b-88bf-419f-c2ce-841c6eea4a00","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[(data.ExpenseInShip == 0.0) & (data.Age > 18)].CryoSleep.value_counts()","metadata":{"id":"dWKj5zOyYALD","outputId":"63d26f4c-01a8-4734-b5d7-213ad2eaa5c4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fill_cryosleep(row):\n    if pd.notna(row['CryoSleep']):\n        return row['CryoSleep']\n    if row['ExpenseInShip'] == 0 and row['Age'] <= 18 and row['VIP'] == False:\n        return False\n    if row['ExpenseInShip'] == 0:\n        return True\n    else:\n        return False\ndata['CryoSleep'] = data.apply(fill_cryosleep, axis=1)","metadata":{"id":"MPbW29UCnmzR","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.Cabin.isnull().sum()","metadata":{"id":"V94ISNds-4zZ","outputId":"f9152edc-3ebf-4f53-9d2e-3d30022bc2b5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[data.PassengerId == '0992']\n# see same passenger_id travelling in the same Cabin.","metadata":{"id":"uNUxnK4Lbs85","outputId":"db16376d-bbe0-4fd6-c44b-881a9733e2a5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# taking mode of Cabin, where same passenger id \ndef get_mode(data, passId):\n    subset = data[data.PassengerId == passId]\n    fill_with = subset.Cabin.mode()\n    if len(fill_with) <= 0:\n        return np.nan\n    return fill_with[0]\n\nsubset = data[data.Cabin.isnull()]\n\nmsID = subset.PassengerId\nfilled = []\nfor i in msID:\n    filled.append(get_mode(data,i))\n    \n\nmy_d = pd.DataFrame()\nmy_d['PassengerId'] = msID\nmy_d['filled_cabin'] = filled\n\nmerged_df = data.merge(my_d, on='PassengerId', how='left')\nmerged_df['filled_cabin'].fillna(merged_df['Cabin'], inplace=True)\nmerged_df.rename(columns={'filled_cabin': 'Cabin1'}, inplace=True)\n\nmerged_df.Cabin = merged_df.Cabin.astype(str)\nmerged_df.Cabin1 = merged_df.Cabin1.astype(str)\n\ndata.drop(columns = 'Cabin', inplace=True)\ndata.rename(columns={'Cabin1': 'Cabin'}, inplace=True)\n\nmerged_df = merged_df.drop_duplicates()\n\ndata = merged_df\ndata.drop(columns = 'Cabin', inplace=True)\ndata.rename(columns={'Cabin1': 'Cabin'}, inplace=True)","metadata":{"id":"37pnEx5jnHD5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ast\ndef convert_to_list(s):\n    if s != 'nan':  # Check if the value is not NaN\n        return ast.literal_eval(s)\n    return None\n\n\n# Apply the conversion function to the Series\ndata.Cabin = data.Cabin.apply(lambda x: convert_to_list(x))","metadata":{"id":"R-_tudVW8asT","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['deck'] = data.Cabin.apply(lambda x: x[0] if isinstance(x, list) else np.nan)\ndata['num'] = data.Cabin.apply(lambda x: x[1] if isinstance(x, list) else np.nan)\ndata['side'] = data.Cabin.apply(lambda x: x[2] if isinstance(x, list) else np.nan)","metadata":{"id":"ko73OdLDdKPF","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"id":"soKgpCQkpr4C","outputId":"0f8602c8-5bc7-4c90-c02a-afdc003a3b9d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.deck.fillna(data.deck.mode()[0], inplace=True)\nsubset_data = data[data['deck'] == 'F']\n# knn-imputation\nX = subset_data['num'].values.reshape(-1, 1)\nknn_imputer = KNNImputer(n_neighbors=4)\nimputed_values = knn_imputer.fit_transform(X)\ndata.loc[data['deck'] == 'F', 'num'] = imputed_values\ndata.side.fillna(data.side.mode()[0], inplace=True)\ndata.drop(columns = ['Cabin'], inplace = True)\nmissing_destination_surname = data[data.Destination.isnull()]['surname'].values[0]\nsubset_data = data[data['surname'] == missing_destination_surname]\nmode_destination = subset_data['Destination'].mode().values[0]\ndata.loc[data['Destination'].isnull(), 'Destination'] = mode_destination\ndata.drop(columns = ['Name', 'surname','PassengerId'], inplace=True)\ndata.Age.fillna(data.Age.mean(), inplace = True)\ndata.VIP.fillna(data.VIP.mode()[0], inplace = True)","metadata":{"id":"CS5I74ZeuSQG","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['CryoSleep'] = data['CryoSleep'].astype(object)\ndata['VIP'] = data['VIP'].astype(object)\ndata['SibSp'] = data['SibSp'].astype(object)\n# data['Transported'] = data['Transported'].astype(int)","metadata":{"id":"rFwzLf9AvSHF","outputId":"841ae478-29f2-450c-cc08-4a3d4a711c8c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['num'] = data['num'].astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = data.loc[:8692]\ntrain_data['Transported'] = train_data['Transported'].astype(int)\ntest_data = data.loc[8693:].drop(columns = 'Transported')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import LabelEncoder\nimport re\nfrom itertools import combinations\nimport Levenshtein\nfrom collections import defaultdict\nfrom scipy.interpolate import LSQUnivariateSpline\nimport matplotlib.pyplot as plt\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import roc_auc_score\n\n\nclass generic_Utilities:\n    '''Generic Utilities on Lists and Dataframes'''\n    def __init__(self):\n        self.allFunctions = {}\n        self.allFunctions['List_Operartion_1'] = str(\"Function: intersection_of_lists(list1, list2), Returns : list3\")\n        self.allFunctions['List_Operartion_2'] = str(\"Function: difference_of_lists(list1, list2), Returns : list4\")\n        \n        self.allFunctions['Folder_Operartion_1'] = str(\"Function: get_csv_column_names(folder_path, num_workers=4), Returns : column_names_dict\")\n        self.allFunctions['Folder_Operartion_2'] = str(\"Function: walk_through_folder(folder_path), Returns : None\") \n        \n        self.allFunctions['Plot_Operartion_1'] = str(\"Function: plot_NumericVscumSum(x, y), Returns : None\")\n        self.allFunctions['Plot_Operartion_2'] = str(\"Function: logistic_regression_with_roc(X, y), Returns: test_roc_auc\")\n        self.allFunctions['Plot_Operartion_3'] = str(\"Function: cart_with_roc(X, y), Returns: test_roc_auc\")\n        \n        self.allFunctions['Dictionary_Operartion_1'] = str(\"Function: filter_and_sort_subsets(subset_counts, threshold), Returns: sorted_subsets\")\n        \n        self.allFunctions['Dataframe_Operartion_1'] = str(\"Function: get_numeric_and_non_numeric_columns(df), Returns: list4, list5\")\n        self.allFunctions['Dataframe_Operartion_2'] = str(\"Function: remove_single_unique_or_all_nans(df), Returns: df\")\n        self.allFunctions['Dataframe_Operartion_3'] = str(\"Function: columns_with_missing_values(df), Returns: list6\")\n        self.allFunctions['Dataframe_Operartion_4'] = str(\"Function: fill_col_with_median(df, colNames), Returns: df\")\n        self.allFunctions['Dataframe_Operartion_5'] = str(\"Function: columns_with_more_than_X_percent_unique(df, colNames, perc), Returns: list7\")\n        self.allFunctions['Dataframe_Operartion_6'] = str(\"Function: convert_and_create_factorizedColumns(df, colNames), Returns: df, mapDict\")\n        self.allFunctions['Dataframe_Operartion_7'] = str(\"Function: fillMissing_predictFactorizedColumns(df, usable_cols, colName), Returns: df, mapDict\")\n        self.allFunctions['Dataframe_Operartion_9'] = str(\"Function: oneHotEncoded(df, columns_to_oneHot), Returns: new_df\")\n        \n        self.allFunctions['FeatureEngineering_Operartion_1'] = str(\"Function: train_subset_counts_oneHot(df, target, value, options= COUNT, WIG, empericalProb, n_jobs=1), Returns: subset_counts\")\n        self.allFunctions['FeatureEngineering_Operartion_2'] = str(\"Function: train_LeastSquareSpline_fit(df, target, variable, degree), Returns: breakpoints_original\")\n        self.allFunctions['FeatureEngineering_Operartion_3'] = str(\"Function: train_UnivariateSpline_fit(df, target, variable, threshold), Returns: breakpoints_original\")\n        self.allFunctions['FeatureEngineering_Operartion_4'] = str(\"Function: train_cart_bins_with_plot(df, variableCol, targetCol, max_n_bins, n_jobs=1), Returns: breakpoints_original\")\n        \n        self.allFunctions['FeatureCreation_Operartion_1'] = str(\"Function: create_combinedFeatures_df(df, required_cols, wanted_subsets,  options = INT, FRACTION), Returns: new_df\")\n        self.allFunctions['FeatureCreation_Operartion_2'] = str(\"Function: create_one_hot_encode_ranges(df, colName, required_columns, breakpoints), Returns: new_df\")\n       \n        self.loadedFunctions = {}\n        return None\n        \n\n    def walk_through_folder(self, folder_path):\n        self.loadedFunctions['Folder_Operartion_2'] = str(\"Function: walk_through_folder(folder_path), Returns : None\") \n        for root, dirs, files in os.walk(folder_path):\n            print(f\"Current directory: {root}\")\n            print(\"Subdirectories:\", dirs)\n            print(\"Files:\", files)\n            print()\n\n        \n    def get_csv_column_names(self, folder_path, num_workers=4):\n        self.loadedFunctions['Folder_Operartion_1'] = str(\"Function: get_csv_column_names(folder_path, num_workers=4), Returns : column_names_dict\")\n        column_names_dict = {}\n\n        # Function to read column names from a CSV file\n        def read_columns(file_path):\n            try:\n                df = pd.read_csv(file_path, nrows=0)\n                return file_path, df.columns.tolist()\n            except Exception as e:\n                print(f\"Error reading {file_path}: {e}\")\n                return file_path, []\n\n        # Traverse the directory and get all CSV file paths\n        csv_files = []\n        for root, _, files in os.walk(folder_path):\n            for file in files:\n                if file.endswith('.csv'):\n                    csv_files.append(os.path.join(root, file))\n\n        # Use ThreadPoolExecutor for parallel processing\n        with ThreadPoolExecutor(max_workers=num_workers) as executor:\n            future_to_file = {executor.submit(read_columns, file): file for file in csv_files}\n\n            for future in as_completed(future_to_file):\n                file_path, columns = future.result()\n                file_name = os.path.basename(file_path)\n                column_names_dict[file_name] = columns\n\n        return column_names_dict    \n        \n        \n    def intersection_of_lists(self, list1, list2):\n        self.loadedFunctions['List_Operartion_1'] = str(\"Function: intersection_of_lists(list1, list2), Returns : list3\")\n        return list(set(list1) & set(list2))\n\n\n    def difference_of_lists(self, list1, list2):\n        self.loadedFunctions['List_Operartion_2'] = str(\"Function: difference_of_lists(list1, list2), Returns : list4\")\n        return [item for item in list1 if item not in list2]\n    \n    \n    def plot_NumericVscumSum(self, x, y):\n        self.loadedFunctions['Plot_Operartion_1'] = str(\"Function: plot_NumericVscumSum(x, y), Returns : None\")\n        modified_y = np.cumsum(y)/(np.sum(y)+0.0000000000001)\n        plt.plot(x, modified_y, marker='o', linestyle='-', color='b')\n        plt.xlabel(\"X-axis\")\n        plt.ylabel(\"Y-axis\")\n        plt.grid(True)\n        plt.show()\n    \n    \n    def get_numeric_and_non_numeric_columns(self, df):\n        self.loadedFunctions['Dataframe_Operartion_1'] = str(\"Function: get_numeric_and_non_numeric_columns(df), Returns: list4, list5\")\n        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n        non_numeric_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()\n        print(f\"Numeric columns: {numeric_cols}\")\n        print(f\"Non-numeric columns: {non_numeric_cols}\")\n        return numeric_cols, non_numeric_cols\n\n\n    def remove_single_unique_or_all_nans(self, df):\n        self.loadedFunctions['Dataframe_Operartion_2'] = str(\"Function: remove_single_unique_or_all_nans(df), Returns: df\")\n        removed_columns = []\n        for column in df.columns:\n            if df[column].nunique() <= 1 or df[column].isna().all():\n                removed_columns.append(column)\n                df = df.drop(columns=[column])\n        print(f\"Removed columns due to all NaN or only 1 unique value: {removed_columns}\")\n        return df\n\n\n    def columns_with_missing_values(self, df):\n        self.loadedFunctions['Dataframe_Operartion_3'] = str(\"Function: columns_with_missing_values(df), Returns: list6\")\n        missing_cols = [col for col in df.columns if df[col].isna().any()]\n        print(f\"Missing data columns: {missing_cols}\")\n        return missing_cols\n\n\n    def fill_col_with_median(self, df, colNames):\n        self.loadedFunctions['Dataframe_Operartion_4'] = str(\"Function: fill_col_with_median(df, colNames), Returns: df\")\n        try:\n            for col in colNames:\n                median_value = df[col].median()\n                df.fillna({col: median_value}, inplace=True)\n                print(\"Done inputing missing numeric values with median for column :\" + str(col))\n            return df\n        except:\n            print(\"Columns that are not numeric might be included! Please Check. Returning original dataframe.\" )\n            return df\n            \n        \n\n    def columns_with_more_than_X_percent_unique(self, df, colNames, perc):\n        self.loadedFunctions['Dataframe_Operartion_5'] = str(\"Function: columns_with_more_than_X_percent_unique(df, colNames, perc), Returns: list7\")\n        total_rows = len(df)\n        threshold = total_rows * 0.01 * perc  \n        cols_with_high_uniques = [col for col in colNames if df[col].nunique() > threshold]\n        print(f\"Columns with high uniques , >= {perc} %  of number of rows in the data: {cols_with_high_uniques}\")\n        return cols_with_high_uniques\n    \n\n\n    def convert_and_create_factorizedColumns(self, df, colNames):\n        self.loadedFunctions['Dataframe_Operartion_6'] = str(\"Function: convert_and_create_factorizedColumns(df, colNames), Returns: df, mapDict\")\n        mapDict = {}\n        try:\n            for colName in colNames:\n                df[colName] = df[colName].astype('object')\n                df[colName], unique_values = pd.factorize(df[colName])\n                # Create a mapping dictionary for the column\n                mapDict[colName] = {value: i for i, value in enumerate(unique_values)}\n            return df, mapDict\n        except:\n            print(\"Columns with missing values might be included! Please Check. Returning original dataframe and a empty dictionary\" )\n            return df, mapDict\n\n        \n    def fillMissing_predictFactorizedColumns(self, df, usable_cols, colName):\n        self.loadedFunctions['Dataframe_Operartion_7'] = str(\"Function: fillMissing_predictFactorizedColumns(df, usable_cols, colName), Returns: df, mapDict\")\n        mapDict = {}\n        try:\n            df[colName] = df[colName].astype('object')\n            df[colName], unique_values = pd.factorize(df[colName])\n            mapDict[colName] = {value: i for i, value in enumerate(unique_values)}\n            # Train the model to predict missing values\n            non_missing_idx = df[colName] != -1  # Using -1 for factorized NaNs\n            missing_idx = df[colName] == -1\n            if missing_idx.sum() > 0:\n                X_train = df.loc[non_missing_idx, usable_cols]\n                y_train = df.loc[non_missing_idx, colName]\n                print(y_train)\n                X_test = df.loc[missing_idx,  usable_cols]\n                model = LogisticRegression(max_iter=1000, solver ='lbfgs',  multi_class='auto')\n                model.fit(X_train, y_train)\n                # Predict the missing values\n                predicted = model.predict(X_test)\n                print(predicted)\n                # Replace the missing values with the predicted values\n                df.loc[missing_idx, colName] = predicted\n            return df, mapDict\n        except:\n            print(\"Columns with non-numeric values might be included! Please Check. Returning original dataframe and a empty dictionary\" )\n            return df, mapDict\n    \n    \n    def apply_meanDistance(self, df, colName, string_list):\n        self.loadedFunctions['Dataframe_Operartion_8'] = str(\"Function: apply_meanDistance(df, colName, string_list), Returns: df\")\n        def calculate_meanDistanceFromAList(input_string, string_list):\n            def sorensen_dice(a, b):\n                def get_bigrams(string):\n                # Generate bigrams from a string\n                    return [string[i:i+2] for i in range(len(string)-1)]\n            # Sørensen-Dice coefficient for two sets\n                a_bigrams = set(get_bigrams(a))\n                b_bigrams = set(get_bigrams(b))\n                overlap = len(a_bigrams & b_bigrams)\n                total = len(a_bigrams) + len(b_bigrams)\n                if total == 0:\n                    return 1.0 if a == b else 0.0  # Handle identical empty strings\n                return 2 * overlap / total\n            sum_Levenshtein = 0\n            sum_sorensen_dice = 0\n            for string in string_list:\n                sum_Levenshtein = sum_Levenshtein + Levenshtein.distance(input_string, string)\n                sum_sorensen_dice = sum_sorensen_dice + sorensen_dice(input_string, string)\n            return float(sum_Levenshtein/len(string_list)),float(sum_sorensen_dice/len(string_list))\n        # Calculate mean distances for each row and add a new column\n        df[['mean_Levenshtein', 'mean_sorensen_dice']] = df[colName].apply(\n            lambda x: pd.Series(calculate_meanDistanceFromAList(x, string_list))\n        )\n        return df\n    \n    \n    def oneHotEncoded(self, df, columns_to_oneHot):\n        self.loadedFunctions['Dataframe_Operartion_9'] = str(\"Function: oneHotEncoded(df, columns_to_oneHot), Returns: new_df\")\n        # Perform one-hot encoding on specified columns\n        df_encoded = pd.get_dummies(df, columns=columns_to_oneHot, drop_first=True, dtype=int)\n        return df_encoded\n\n    \n    def filter_and_sort_subsets(self, subset_counts, threshold):\n        self.loadedFunctions['Dictionary_Operartion_1'] = str(\"Function: filter_and_sort_subsets(subset_counts, threshold), Returns: sorted_subsets\")\n        # Filter subsets based on the given threshold\n        filtered_subsets = {subset: count for subset, count in subset_counts.items() if count > threshold}\n        # Sort the filtered subsets based on their counts in descending order\n        sorted_subsets = sorted(filtered_subsets.items(), key=lambda item: item[1], reverse=True)\n        return sorted_subsets\n    \n    \n    def create_combinedFeatures_df(self, df, required_cols, wanted_subsets, options):\n        self.loadedFunctions['FeatureCreation_Operartion_1'] = str(\"Function: create_combinedFeatures_df(df, required_cols, wanted_subsets,  options = INT, FRACTION), Returns: new_df\")\n        orig_df = df.copy()\n        for subset, _ in wanted_subsets:\n            new_col_name = \"_\".join(subset) + \"_combined\"\n            if(options=='FRACTION'):\n                orig_df[new_col_name] = df[list(subset)].mean(axis=1)\n            else:\n                orig_df[new_col_name] = df[list(subset)].min(axis=1)\n\n        # Return a dataframe with the id, target, and new fraction columns\n        new_combined_columns = [(\"_\".join(subset) + \"_combined\") for subset, _ in wanted_subsets]\n        selected_columns = required_cols + new_combined_columns\n        return orig_df[selected_columns]\n    \n    \n    def create_one_hot_encode_ranges(self, df, colName, required_columns, breakpoints):\n        self.loadedFunctions['FeatureCreation_Operartion_2'] = str(\"Function: create_one_hot_encode_ranges(df, colName, required_columns, breakpoints), Returns: new_df\")\n        # Ensure breakpoints are sorted\n        breakpoints = sorted(breakpoints)\n\n        # Create a new DataFrame with the required columns\n        new_df = df[required_columns].copy()\n\n        # Create one-hot encoded columns based on breakpoints\n        for i in range(len(breakpoints) - 1):\n            lower_bound = breakpoints[i]\n            upper_bound = breakpoints[i + 1]\n            col_name = f\"{colName}_{lower_bound:.2f}to{upper_bound:.2f}\"\n            new_df[col_name] = np.where((df[colName] > lower_bound) & (df[colName] <= upper_bound), 1, 0)\n\n        return new_df\n    \n    \n    def train_subset_counts_oneHot(self, df, target, value, options, n_jobs=1):\n        self.loadedFunctions['FeatureEngineering_Operartion_1'] = str(\"Function: train_subset_counts_oneHot(df, target, value, options= COUNT, WIG, empericalProb , n_jobs=1), Returns: subset_counts\")\n        # Filter rows where the target equals value\n        df_target_value = df[df[target] == value].copy()\n        valSum = df_target_value[target].sum()\n\n        # Drop target column to get only the one-hot encoded columns\n        one_hot_columns = df_target_value.drop(columns=[target]).columns\n\n        # Dictionary to store subsets and their counts\n        subset_counts = defaultdict(int)\n\n        # Helper function to check if a subset has more than two one-hot columns from the same original column\n        def valid_subset(subset):\n            original_cols = [col.split('_')[0] for col in subset]\n            return all(original_cols.count(col) <= 1 for col in original_cols)\n\n        # Function to calculate counts for a subset\n        def calculate_count(subset):\n            subset_df = df_target_value[list(subset)]\n            count = (subset_df.sum(axis=1) == len(subset)).sum()\n\n            if options == 'COUNT':\n                return subset, count\n            elif options == 'WIG':\n                wig_value = (1 / (len(subset) + 1)) * (count / len(df_target_value)) - (1 / 2 ** len(subset))\n                return subset, wig_value\n            elif options == 'empericalProb':\n                emp_prob = count / valSum\n                return subset, emp_prob\n            else:\n                return subset, count\n\n        all_combinations = []\n        for r in range(1, len(one_hot_columns) + 1):\n            for subset in combinations(one_hot_columns, r):\n                if valid_subset(subset):\n                    all_combinations.append(subset)\n\n        with ThreadPoolExecutor(max_workers=n_jobs) as executor:\n            futures = [executor.submit(calculate_count, subset) for subset in all_combinations]\n            for future in as_completed(futures):\n                subset, count = future.result()\n                subset_counts[subset] = count\n\n        return dict(subset_counts)\n    \n    \n    def train_LeastSquareSpline_fit(self, df, target, variable, degree):\n        self.loadedFunctions['FeatureEngineering_Operartion_2'] = str(\"Function: train_LeastSquareSpline_fit(df, target, variable, degree), Returns: breakpoints_original\")\n        # 1. Sort the dataframe based on the variable column.\n        df_sorted = df.sort_values(by=variable)\n\n        # 2. Convert all values in the sorted variable column to values between [0,1].\n        df_sorted['TranformedVariable'] = (df_sorted[variable] - df_sorted[variable].min()) / (df_sorted[variable].max() - df_sorted[variable].min())\n\n        # 3. Convert the target column to Cumulative Sum divided By Total Sum so that it is also between [0,1].\n        df_sorted[target] = df_sorted[target].cumsum() / df_sorted[target].sum()\n\n        # 4. Fit the best linear spline on the modified target based on the modified variable.\n        # Define knot points (as degree + 1 points excluding the endpoints)\n        num_knots = degree \n        knots = np.linspace(0, 1, num_knots + 2)[1:-1]  # exclude 0 and 1 as knots\n\n        spline = LSQUnivariateSpline(df_sorted['TranformedVariable'], df_sorted[target], t=knots, k=degree)\n\n        # 5. Return a list which contains all the break points of the fitted spline based on the original variable column\n        breakpoints = spline.get_knots()\n        breakpoints_original = df_sorted[variable].min() + breakpoints * (df_sorted[variable].max() - df_sorted[variable].min())\n        return breakpoints_original\n    \n    \n    def train_UnivariateSpline_fit(self, df, target, variable, threshold):\n        self.loadedFunctions['FeatureEngineering_Operartion_3'] = str(\"Function: train_UnivariateSpline_fit(df, target, variable, threshold), Returns: breakpoints_original\")\n\n        # 1. Sort the dataframe based on the variable column.\n        df_sorted = df.sort_values(by=variable)\n\n        # 2. Convert all values in the sorted variable column to values between [0,1].\n        df_sorted['TranformedVariable'] = (df_sorted[variable] - df_sorted[variable].min()) / (df_sorted[variable].max() - df_sorted[variable].min())\n\n        # 3. Convert the target column to Cumulative Sum divided By Total Sum so that it is also between [0,1].\n        df_sorted[target] = df_sorted[target].cumsum() / df_sorted[target].sum()\n\n        spline = UnivariateSpline(df_sorted['TranformedVariable'], df_sorted[target], s=threshold)\n\n        # 5. Return a list which contains all the break points of the fitted spline based on the original variable column\n        breakpoints = spline.get_knots()\n        breakpoints_original = df_sorted[variable].min() + breakpoints * (df_sorted[variable].max() - df_sorted[variable].min())\n\n        return breakpoints_original\n    \n    \n    def train_cart_bins_with_plot(self, df, variableCol, targetCol, max_n_bins, n_jobs=1):\n        self.loadedFunctions['FeatureEngineering_Operartion_4'] = str(\"Function: train_cart_bins_with_plot(df, variableCol, targetCol, max_n_bins, n_jobs=1), Returns: breakpoints_original\")\n        best_auc = 0\n        best_bins = []\n        best_model = None\n\n        def fit_cart_model(leaf_nodes):\n            cart_model = DecisionTreeClassifier(max_leaf_nodes=leaf_nodes, random_state=42)\n            cart_model.fit(df[[variableCol]], df[targetCol])\n            predictions = cart_model.predict_proba(df[[variableCol]])[:, 1]\n            auc = roc_auc_score(df[targetCol], predictions)\n            thresholds = cart_model.tree_.threshold\n            thresholds = thresholds[thresholds != -2]  # Remove dummy thresholds\n            bins = sorted(thresholds)\n            bins = [df[variableCol].min()] + bins + [df[variableCol].max()]\n            return auc, cart_model, bins\n\n        with ThreadPoolExecutor(max_workers=n_jobs) as executor:\n            futures = {executor.submit(fit_cart_model, leaf_nodes): leaf_nodes for leaf_nodes in range(max_n_bins, 1, -1)}\n\n            for future in futures:\n                try:\n                    auc, cart_model, bins = future.result()\n                    if auc > best_auc:\n                        best_auc = auc\n                        best_model = cart_model\n                        best_bins = bins\n                    else:\n                        break\n                except Exception as e:\n                    print(f\"An error occurred with leaf_nodes {futures[future]}: {e}\")\n\n        # Plotting\n        plt.figure(figsize=(10, 6))\n        plt.scatter(df[variableCol], df[targetCol], color='blue', label='Actual Values')\n\n        for i in range(len(best_bins) - 1):\n            df_pred = pd.DataFrame({variableCol: [best_bins[i], best_bins[i+1]]})\n            plt.plot([best_bins[i], best_bins[i+1]], [best_model.predict(df_pred)[0], best_model.predict(df_pred)[1]], color='red', linewidth=2)\n\n        plt.xlabel(variableCol)\n        plt.ylabel(targetCol)\n        plt.title('CART Fitted Model vs Actual Values')\n        plt.legend()\n        plt.show()\n        return best_bins\n    \n    \n    def logistic_regression_with_roc(self, X, y):\n        self.loadedFunctions['Plot_Operartion_2'] = str(\"Function: logistic_regression_with_roc(X, y), Returns: test_roc_auc\")\n        # 1. Split the data into X, y train and X, y test with a proportion of test 0.2 randomly\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        # 2. Fit the data on the training set using sm.Logit, print the summary of the fit using lgbfs\n        logit_model = sm.Logit(y_train, sm.add_constant(X_train)).fit(method='lbfgs')\n        print(logit_model.summary())\n\n        # 3. Predict on the Test Data\n        y_train_pred = logit_model.predict(sm.add_constant(X_train))\n        y_test_pred = logit_model.predict(sm.add_constant(X_test))\n\n        # 4. Print The Test and Train roc_auc\n        train_roc_auc = roc_auc_score(y_train, y_train_pred)\n        test_roc_auc = roc_auc_score(y_test, y_test_pred)\n        print(f\"Train ROC AUC: {train_roc_auc}\")\n        print(f\"Test ROC AUC: {test_roc_auc}\")\n\n        # 5. Plot the ROC_AUC for the model\n        fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred)\n        fpr_test, tpr_test, _ = roc_curve(y_test, y_test_pred)\n\n        plt.figure(figsize=(10, 6))\n        plt.plot(fpr_train, tpr_train, label=f\"Train ROC AUC = {train_roc_auc:.2f}\", color='blue')\n        plt.plot(fpr_test, tpr_test, label=f\"Test ROC AUC = {test_roc_auc:.2f}\", color='red')\n        plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title('ROC Curve')\n        plt.legend()\n        plt.show()\n\n        # 6. Return the test roc_auc as a float\n        return test_roc_auc\n    \n    \n    def cart_with_roc(self, X, y):\n        self.loadedFunctions['Plot_Operartion_3'] = str(\"Function: cart_with_roc(X, y), Returns: test_roc_auc\")\n        # 1. Split the data into X, y train and X, y test with a proportion of test 0.2 randomly\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        # 2. Fit the data on the training set using CART\n        cart_model = DecisionTreeClassifier(max_leaf_nodes=2*X_train.shape[1], random_state=42)\n        cart_model.fit(X_train, y_train)\n\n        # 3. Predict on the Test Data\n        y_train_pred = cart_model.predict_proba(X_train)[:, 1]\n        y_test_pred = cart_model.predict_proba(X_test)[:, 1]\n\n        # 4. Print The Test and Train roc_auc\n        train_roc_auc = roc_auc_score(y_train, y_train_pred)\n        test_roc_auc = roc_auc_score(y_test, y_test_pred)\n        print(f\"Train ROC AUC: {train_roc_auc}\")\n        print(f\"Test ROC AUC: {test_roc_auc}\")\n\n        # 5. Plot the ROC_AUC for the model\n        fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred)\n        fpr_test, tpr_test, _ = roc_curve(y_test, y_test_pred)\n\n        plt.figure(figsize=(10, 6))\n        plt.plot(fpr_train, tpr_train, label=f\"Train ROC AUC = {train_roc_auc:.2f}\", color='blue')\n        plt.plot(fpr_test, tpr_test, label=f\"Test ROC AUC = {test_roc_auc:.2f}\", color='red')\n        plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title('ROC Curve')\n        plt.legend()\n        plt.show()\n\n        # 6. Return the test roc_auc as a float\n        return test_roc_auc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"space_titanic_utils = generic_Utilities()\nspace_titanic_utils.allFunctions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"space_titanic_utils.columns_with_missing_values(test_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport statsmodels.api as sm\nfrom sklearn.metrics import roc_auc_score, roc_curve\n\n\nclass logitModelBuilder_BinaryClassification:\n    '''Takes in the train, test , target and identifier columns'''\n    def __init__(self, train, test, target, identifier_columns):\n        self.origTrain = train\n        self.origTest = test\n        self.target = target\n        self.identifiers = identifier_columns \n        self.genericUtils = generic_Utilities()\n        self.numericCols , self.nonNumericCols = self.genericUtils.get_numeric_and_non_numeric_columns(self.origTest)\n        self.masterDF = self.genericUtils.oneHotEncoded(pd.concat([self.origTrain,self.origTest], axis=0), self.nonNumericCols)\n        self.baselineCols = self.genericUtils.difference_of_lists(self.masterDF.columns, self.numericCols+[self.target])\n        self.loadedModels = self.genericUtils.train_subset_counts_oneHot\n        return None\n\n    \n    def update_BaselineLogit(self):\n        self.baseline_logit_ROC_AUC = self.genericUtils.logistic_regression_with_roc(self.masterDF[self.baselineCols].loc[:len(self.origTrain)-1], self.masterDF[self.target].loc[:len(self.origTrain)-1])\n        self.baseline_cart_ROC_AUC = self.genericUtils.cart_with_roc(self.masterDF[self.baselineCols].loc[:len(self.origTrain)-1], self.masterDF[self.target].loc[:len(self.origTrain)-1])\n    \n    \n    def merge_to_Master(self, master_df, new_df):\n        if len(master_df) != len(new_df):\n            print(\"The lengths of the DataFrames do not match.\")\n            return master_df\n\n        # Concatenate DataFrames side by side\n        combined_df = pd.concat([master_df, new_df], axis=1)\n\n        # Remove duplicate columns, keeping only the column from master_df\n        combined_df = combined_df.loc[:, ~combined_df.columns.duplicated()]\n\n        for col in new_df.columns:\n            if col in master_df.columns:\n                combined_df.drop(columns=[col], inplace=True)\n                combined_df[col] = master_df[col]\n\n        return combined_df\n    \n    \n    def addNumericalBinHots(self, colName, max_n_bins, mode, n_jobs=1):\n        breakpoints = self.genericUtils.train_cart_bins_with_plot(self.origTrain, colName, self.target, max_n_bins, n_jobs=1)\n        if(mode=='Research'):\n            print(\"Only for checks. Master Dataframe not changed\")\n            return None\n        else:\n            self.currentNumericBinnedDF = self.genericUtils.create_one_hot_encode_ranges(self.masterDF, colName, [self.target], breakpoints)\n            self.masterDF = self.merge_to_Master(self.masterDF, self.currentNumericBinnedDF)\n            self.baselineCols = self.baselineCols + self.genericUtils.difference_of_lists(list(self.currentNumericBinnedDF.columns), [self.target])\n            print(\"Done merging with Master Dataframe!\")\n            return None\n    \n    \n    def addInteractionOneHots(self, colNames, threshold, options, mode):\n        wanted_subsets = self.genericUtils.train_subset_counts_oneHot(self.masterDF.loc[:8692], self.target, 1, options, n_jobs=1)\n        sorted_subsets = self.genericUtils.filter_and_sort_subsets(wanted_subsets, threshold)\n        if(mode=='Research'):\n            print(\"Only for checks. Master Dataframe not changed\")\n            return None\n        else:\n            self.currentInteractionDF = self.genericUtils.create_combinedFeatures_df(df, [], sorted_subsets,  'INT')\n            self.masterDF = self.merge_to_Master(self.masterDF, self.currentInteractionDF)\n            self.baselineCols = self.baselineCols + self.genericUtils.difference_of_lists(list(self.currentInteractionDF.columns), [self.target])\n            print(\"Done merging with Master Dataframe!\")\n            return None\n\n        \n    def saveMaster_to_Path(self, path):\n        fileLoc = \"/working/\"+ \"Master_Data.csv\"\n        self.masterDF.to_csv(fileLoc, index=False)\n        print(\"Saved the master data!\")\n        return \n    \n    \n    def fitOsave_BaselineModel(self, save_path):\n        fitted_models = {}\n        print(\"Fitting Logit using StatsModels...\")\n        logit_model = sm.Logit(self.masterDF[self.target].loc[:len(self.origTrain)-1], \n                               sm.add_constant(self.masterDF[self.baselineCols].loc[:len(self.origTrain)-1])).fit(method='lbfgs')\n        print(logit_model.summary())\n        model_filename = f\"{save_path}_LogitProb.joblib\"\n        joblib.dump(logit_model, model_filename)\n        \n        logit_y_train_pred = logit_model.predict(sm.add_constant(self.masterDF[self.baselineCols].loc[:len(self.origTrain)-1]))\n\n        logit_roc_auc = roc_auc_score(self.masterDF[self.target].loc[:len(self.origTrain)-1], logit_y_train_pred)\n\n        fitted_models[f\"LogitProb_Features_{len(self.baselineCols)}\"] = logit_roc_auc\n        \n        print(\"Fitting Decision Tree Classifier using Scipy...\")\n        cart_model = DecisionTreeClassifier(max_leaf_nodes=3*len(self.baselineCols), random_state=42)\n        cart_model.fit(self.masterDF[self.baselineCols].loc[:len(self.origTrain)-1], self.masterDF[self.target].loc[:len(self.origTrain)-1])\n        model_filename = f\"{save_path}_DecisionTreeClassifier.joblib\"\n        joblib.dump(cart_model, model_filename)\n        \n        cart_y_train_pred = cart_model.predict_proba(self.masterDF[self.baselineCols].loc[:len(self.origTrain)-1])\n\n        cart_roc_auc = roc_auc_score(self.masterDF[self.target].loc[:len(self.origTrain)-1], cart_y_train_pred)\n        \n        fitted_models[f\"DecisionTreeClassifier_Features_{len(self.baselineCols)}\"] = cart_roc_auc\n\n        self.currentFittedModelsDict = fitted_models\n        print(\"Done saving the current version of the fits! Fetched the roc_auc for each model.\")\n        return fitted_models\n    \n    \n    def load_SavedModels(self, models_dict, save_path='/working/'):\n        for model_name in models_dict.keys():\n            try:\n                # Load the fitted model\n                model_filename = f\"{save_path}{model_name}.joblib\"\n                self.loadedModels[model_name] = joblib.load(model_filename)\n            except:\n                pass\n        return None","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spaceTitanic = logitModelBuilder_BinaryClassification(train_data, test_data, 'Transported', [])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(spaceTitanic.origTrain)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spaceTitanic.update_BaselineLogit()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spaceTitanic.addNumericalBinHots('num', 5, 'Dev', n_jobs=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spaceTitanic.addNumericalBinHots('RoomService', 3, 'Dev', n_jobs=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spaceTitanic.addNumericalBinHots('Age', 4, 'Dev', n_jobs=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spaceTitanic.addNumericalBinHots('ExpenseInShip', 3, 'Dev', n_jobs=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spaceTitanic.addNumericalBinHots('Spa', 5, 'Dev', n_jobs=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spaceTitanic.addNumericalBinHots('VRDeck', 3, 'Dev', n_jobs=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spaceTitanic.addNumericalBinHots('FoodCourt', 6, 'Dev', n_jobs=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spaceTitanic.addNumericalBinHots('ShoppingMall', 7, 'Dev', n_jobs=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spaceTitanic.update_BaselineLogit()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spaceTitanic.baselineCols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(spaceTitanic.masterDF.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"interactions = ['HomePlanet_Europa',\n 'HomePlanet_Mars',\n 'CryoSleep_True',\n 'Destination_PSO J318.5-22',\n 'Destination_TRAPPIST-1e',\n 'VIP_True',\n 'SibSp_2',\n 'SibSp_3',\n 'SibSp_4',\n 'SibSp_5',\n 'SibSp_6',\n 'SibSp_7',\n 'SibSp_8',\n 'deck_B',\n 'deck_C',\n 'deck_D',\n 'deck_E',\n 'deck_F',\n 'deck_G',\n 'deck_T',\n 'side_S',\n 'num_0.00to334.50',\n 'num_334.50to602.50',\n 'num_602.50to824.50',\n 'num_824.50to1162.50',\n 'num_1162.50to1894.00',\n 'RoomService_0.00to0.50',\n 'RoomService_0.50to346.50',\n 'RoomService_346.50to14327.00',\n 'Age_0.00to4.50',\n 'Age_4.50to17.50',\n 'Age_17.50to24.50',\n 'Age_24.50to79.00',\n 'ExpenseInShip_0.00to0.50',\n 'ExpenseInShip_0.50to2384.50',\n 'ExpenseInShip_2384.50to35987.00',\n 'Spa_0.00to0.50',\n 'Spa_0.50to266.50',\n 'Spa_266.50to2446.50',\n 'Spa_2446.50to2462.50',\n 'Spa_2462.50to22408.00',\n 'VRDeck_0.00to0.50',\n 'VRDeck_0.50to613.50',\n 'VRDeck_613.50to24133.00',\n 'FoodCourt_0.00to0.50',\n 'FoodCourt_0.50to60.50',\n 'FoodCourt_60.50to668.50',\n 'FoodCourt_668.50to2507.50',\n 'FoodCourt_2507.50to4071.00',\n 'FoodCourt_4071.00to29813.00',\n 'ShoppingMall_0.00to0.50',\n 'ShoppingMall_0.50to130.50',\n 'ShoppingMall_130.50to303.00',\n 'ShoppingMall_303.00to627.50',\n 'ShoppingMall_627.50to1248.50',\n 'ShoppingMall_1248.50to1823.00',\n 'ShoppingMall_1823.00to23492.00']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spaceTitanic.addInteractionOneHots(interactions, 100, 'COUNT', 'Research')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spaceTitanic.baselineCols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logit_model = sm.Logit(spaceTitanic.masterDF[spaceTitanic.target].loc[:8692], sm.add_constant(spaceTitanic.masterDF[spaceTitanic.baselineCols].loc[:8692])).fit(method='lbfgs')\nprint(logit_model.summary())\ncart_model = DecisionTreeClassifier(max_leaf_nodes=180, random_state=42)\ncart_model.fit(spaceTitanic.masterDF[spaceTitanic.baselineCols].loc[:8692], spaceTitanic.masterDF[spaceTitanic.target].loc[:8692])\n# 3. Predict on the Test Data\ny_pred = cart_model.predict(spaceTitanic.masterDF[spaceTitanic.baselineCols].loc[8693:])\n# threshold = 0.39\n# y_prob = logit_model.predict(sm.add_constant(spaceTitanic.masterDF[spaceTitanic.baselineCols].loc[8693:]))\n# y_pred = (y_prob >= threshold).astype(bool)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission 🎉","metadata":{"id":"2YwH6cXcHvdz"}},{"cell_type":"code","source":"test_id = pd.read_csv(\"/kaggle/input/spaceship-titanic/test.csv\").PassengerId\ntest_id.shape","metadata":{"id":"ykOFHg9PHkLG","outputId":"92a40a68-da1f-40a3-809e-01b2a16055ec","scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit = pd.DataFrame()\nsubmit['PassengerId'] = test_id\nsubmit['Transported'] = y_pred.astype(bool)","metadata":{"id":"GGDiZkFTIIm_","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit.to_csv(\"submission.csv\", index=False)","metadata":{"id":"NcyFao4hIZd3","trusted":true},"execution_count":null,"outputs":[]}]}
