{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":29507,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_kg_hide-input":false,"trusted":true,"execution":{"iopub.status.busy":"2024-10-22T13:36:15.644954Z","iopub.execute_input":"2024-10-22T13:36:15.645404Z","iopub.status.idle":"2024-10-22T13:36:15.980277Z","shell.execute_reply.started":"2024-10-22T13:36:15.645332Z","shell.execute_reply":"2024-10-22T13:36:15.979125Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/titanic/train.csv\n/kaggle/input/titanic/test.csv\n/kaggle/input/titanic/gender_submission.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import LabelEncoder\nimport Levenshtein\nimport re\n\n\ndef intersection_of_lists(list1, list2):\n    return list(set(list1) & set(list2))\n\n\ndef difference_of_lists(list1, list2):\n    return [item for item in list1 if item not in list2]\n\n\ndef get_numeric_and_non_numeric_columns(df):\n    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n    non_numeric_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()\n    print(f\"Numeric columns: {numeric_cols}\")\n    print(f\"Non-numeric columns: {non_numeric_cols}\")\n    return numeric_cols, non_numeric_cols\n\n\ndef remove_single_unique_or_all_nans(df):\n    removed_columns = []\n    for column in df.columns:\n        if df[column].nunique() <= 1 or df[column].isna().all():\n            removed_columns.append(column)\n            df = df.drop(columns=[column])\n    print(f\"Removed columns due to all NaN or only 1 unique value: {removed_columns}\")\n    return df, removed_columns\n\n\ndef columns_with_missing_values(df):\n    missing_cols = [col for col in df.columns if df[col].isna().any()]\n    print(f\"Missing data columns: {missing_cols}\")\n    return missing_cols\n\n\ndef fill_missingNumeric_with_median(df, missing_cols, numeric_cols):\n    for col in intersection_of_lists(missing_cols, numeric_cols):\n        median_value = df[col].median()\n        df[col].fillna(median_value, inplace=True)\n    print(\"Done inputing missing numeric values with median!\")\n    return df\n\n\ndef columnsCategory_with_more_than_X_percent_unique(df, categoric_cols, perc):\n    total_rows = len(df)\n    threshold = total_rows * 0.01 * perc  # 10% of the total number of rows\n    cols_with_high_uniques = [col for col in categoric_cols if df[col].nunique() > threshold]\n    print(f\"Columns with high uniques: {cols_with_high_uniques}\")\n    return cols_with_high_uniques\n    \n\n\ndef convert_and_create_integer_columns(df, new_columns, mappings, colName):\n    df[colName] = df[colName].astype('object')\n    df[colName], unique_values = pd.factorize(df[colName])\n    # Add the new column name to the list\n    new_columns.append(colName)\n    # Create a mapping dictionary for the column\n    mappings[colName] = {value: i for i, value in enumerate(unique_values)}\n    return df, new_columns, mappings\n\n\ndef fill_missing_and_predict(df, new_columns, mappings, usable_cols, column_name):\n    # Convert and create integer column\n    df, new_columns, mappings = convert_and_create_integer_columns(df, new_columns, mappings, column_name)\n    # Train the model to predict missing values\n    non_missing_idx = df[column_name] != -1  # Using -1 for factorized NaNs\n    missing_idx = df[column_name] == -1\n    if missing_idx.sum() > 0:\n        X_train = df.loc[non_missing_idx, usable_cols]\n        y_train = df.loc[non_missing_idx, column_name]\n        X_test = df.loc[missing_idx,  usable_cols]\n        model = LogisticRegression(max_iter=1000, solver ='lbfgs',  multi_class='auto')\n        model.fit(X_train, y_train)\n        # Predict the missing values\n        predicted = model.predict(X_test)\n        # Replace the missing values with the predicted values\n        df.loc[missing_idx, column_name] = predicted\n    return df, new_columns, mappings\n    \n\ndef get_bigrams(string):\n    # Generate bigrams from a string\n    return [string[i:i+2] for i in range(len(string)-1)]\n\ndef sorensen_dice(a, b):\n    # Sørensen-Dice coefficient for two sets\n    a_bigrams = set(get_bigrams(a))\n    b_bigrams = set(get_bigrams(b))\n    overlap = len(a_bigrams & b_bigrams)\n    total = len(a_bigrams) + len(b_bigrams)\n    if total == 0:\n        return 1.0 if a == b else 0.0  # Handle identical empty strings\n    return 2 * overlap / total\n\n\ndef calculate_meanDistanceFromAList(input_string, string_list):\n    sum_Levenshtein = 0\n    sum_sorensen_dice = 0\n    for string in string_list:\n        sum_Levenshtein = sum_Levenshtein + Levenshtein.distance(input_string, string)\n        sum_sorensen_dice = sum_sorensen_dice + sorensen_dice(input_string, string)\n    return float(sum_Levenshtein/len(string_list)),float(sum_sorensen_dice/len(string_list))\n    \n\ndef takeOut_stringList(df, target, variableCol):\n    return list(df[df[f\"{target}\"]==1][f\"{variableCol}\"].unique()),list(df[df[f\"{target}\"]==0][f\"{variableCol}\"].unique())\n\n\ndef apply_meanDistance(df, column_name, string_list):\n    # Calculate mean distances for each row and add a new column\n    df[['mean_Levenshtein', 'mean_sorensen_dice']] = df[column_name].apply(\n        lambda x: pd.Series(calculate_meanDistanceFromAList(x, string_list))\n    )\n    return df\n\n\ndef create_DistanceMetric(df, new_columns, usable_cols, colName, target, orig_data):\n    df[colName] = df[colName].astype('str')\n    true_NameList, false_NameList = takeOut_stringList(orig_data, target, colName)\n    new_columns.append(colName)\n    colName_true_lev = str(colName+\"_true_lev\")\n    colName_true_reg = str(colName+\"_true_reg\")\n    df[[colName_true_lev, colName_true_reg]] = df[colName].apply(\n        lambda x: pd.Series(calculate_meanDistanceFromAList(x, true_NameList))\n    )\n    colName_false_lev = str(colName+\"_false_lev\")\n    colName_false_reg = str(colName+\"_false_reg\")\n    df[[colName_false_lev, colName_false_reg]] = df[colName].apply(\n        lambda x: pd.Series(calculate_meanDistanceFromAList(x, false_NameList))\n    )\n    usable_cols = usable_cols + [colName_true_lev, colName_true_reg, colName_false_lev, colName_false_reg]\n    return df, new_columns, usable_cols\n\n\ndef convert_All_integer_columns(df, numeric_cols, missing_cols, categoric_cols, cols_with_high_uniques, target, orig_data):\n    new_columns = []\n    mappings = {}\n    usable_cols = numeric_cols \n    \n    categoric_nonNA_cols = difference_of_lists(categoric_cols, missing_cols)\n\n    categoric_nonNA_Few_cols = difference_of_lists(categoric_nonNA_cols, cols_with_high_uniques)\n    \n    categoric_nonNA_Multiple_cols = difference_of_lists(categoric_nonNA_cols, categoric_nonNA_Few_cols)\n    \n    categoric_NA_Few_cols = difference_of_lists(missing_cols, cols_with_high_uniques)\n\n    categoric_NA_Multiple_cols = difference_of_lists(missing_cols, categoric_NA_Few_cols)\n    \n    for col in categoric_nonNA_Few_cols:\n        df, new_columns, mappings = convert_and_create_integer_columns(df, new_columns, mappings, col)\n        print(f\"[No NA values][Less Unique Values] Categoric columns Converted to Integer: {col}\")\n    usable_cols = usable_cols + categoric_nonNA_Few_cols\n    for col in  categoric_NA_Few_cols:   \n        df, new_columns, mappings = fill_missing_and_predict(df, new_columns, mappings, usable_cols, col)\n        print(f\"[NA values][Less Unique Values] Categoric columns Converted to Integer and Missing Are Predicted: {col}\")\n        usable_cols = usable_cols + [col]    \n    for col in categoric_nonNA_Multiple_cols:   \n        df, new_columns, usable_cols = create_DistanceMetric(df, new_columns, usable_cols, col, target, orig_data)\n        df = df.drop(columns=[col])\n        print(f\"[No NA values][Multiple Unique Values] Categoric columns Converted to Distance Based On Cluster: {col}\")\n    for col in  categoric_NA_Multiple_cols:\n        df = df.drop(columns=[col])\n        print(f\"[NA values][Multiple Unique Values] Categoric columns Converted to Distance Based On Cluster and Missing Are Predicted: {col}\")  \n    print(f\"Mappings: {mappings}\")\n    return df, new_columns, mappings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-22T13:36:15.983418Z","iopub.execute_input":"2024-10-22T13:36:15.983759Z","iopub.status.idle":"2024-10-22T13:36:17.137360Z","shell.execute_reply.started":"2024-10-22T13:36:15.983708Z","shell.execute_reply":"2024-10-22T13:36:17.136389Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class tangiD_BinaryClassification:\n    '''Takes in the data , target and features'''\n    def __init__(self, data, train, target, type, features):\n        self.data = data\n        self.target = target\n        self.type = type\n        if(self.type!=\"TestData!\"):\n            self.origData = self.data.copy()\n        else:\n            self.origData = train\n        self.allFeatures = features\n    \n    def medianIntifying(self, highUniq = 10):\n        self.cleanDF, self.removed_columns = remove_single_unique_or_all_nans(self.data[self.allFeatures].copy())\n        self.numeric_cols, self.non_numeric_cols = get_numeric_and_non_numeric_columns(self.cleanDF)\n        self.missing_cols = columns_with_missing_values(self.cleanDF)\n        self.filledNumeric_df = fill_missingNumeric_with_median(self.cleanDF, self.missing_cols, self.numeric_cols)\n        self.missing_cols = columns_with_missing_values(self.filledNumeric_df)\n        self.high_uniques = columnsCategory_with_more_than_X_percent_unique(self.filledNumeric_df, self.non_numeric_cols, highUniq)\n        self.updated_df, self.new_columns, self.mappings = convert_All_integer_columns(self.filledNumeric_df, self.numeric_cols, self.missing_cols, self.non_numeric_cols, self.high_uniques, self.target, self.origData)\n        if(self.type!=\"TestData!\"):\n            self.updated_df = pd.concat([self.updated_df, self.data[self.target]], axis=1)\n            return self.updated_df\n        else:\n            return self.updated_df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-22T13:36:17.139252Z","iopub.execute_input":"2024-10-22T13:36:17.139564Z","iopub.status.idle":"2024-10-22T13:36:17.156914Z","shell.execute_reply.started":"2024-10-22T13:36:17.139502Z","shell.execute_reply":"2024-10-22T13:36:17.155629Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntrainClass = tangiD_BinaryClassification(train_data, None, \"Survived\", \"TrainData!\", ['Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch','Ticket', 'Fare', 'Cabin', 'Embarked'])\ntrainData = trainClass.medianIntifying(10)\ntrainData","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-22T13:36:17.158603Z","iopub.execute_input":"2024-10-22T13:36:17.158978Z","iopub.status.idle":"2024-10-22T13:36:42.611496Z","shell.execute_reply.started":"2024-10-22T13:36:17.158915Z","shell.execute_reply":"2024-10-22T13:36:42.610555Z"}},"outputs":[{"name":"stdout","text":"Removed columns due to all NaN or only 1 unique value: []\nNumeric columns: ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\nNon-numeric columns: ['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked']\nMissing data columns: ['Age', 'Cabin', 'Embarked']\nDone inputing missing numeric values with median!\nMissing data columns: ['Cabin', 'Embarked']\nColumns with high uniques: ['Name', 'Ticket', 'Cabin']\n[No NA values][Less Unique Values] Categoric columns Converted to Integer: Sex\n[NA values][Less Unique Values] Categoric columns Converted to Integer and Missing Are Predicted: Embarked\n[No NA values][Multiple Unique Values] Categoric columns Converted to Distance Based On Cluster: Name\n[No NA values][Multiple Unique Values] Categoric columns Converted to Distance Based On Cluster: Ticket\n[NA values][Multiple Unique Values] Categoric columns Converted to Distance Based On Cluster and Missing Are Predicted: Cabin\nMappings: {'Sex': {'male': 0, 'female': 1}, 'Embarked': {'S': 0, 'C': 1, 'Q': 2}}\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"     Pclass  Sex   Age  SibSp  Parch     Fare  Embarked  Name_true_lev  \\\n0         3    0  22.0      1      0   7.2500         0      23.330409   \n1         1    1  38.0      1      0  71.2833         1      39.064327   \n2         3    1  26.0      0      0   7.9250         0      23.897661   \n3         1    1  35.0      1      0  53.1000         0      33.649123   \n4         3    0  35.0      0      0   8.0500         0      23.242690   \n..      ...  ...   ...    ...    ...      ...       ...            ...   \n886       2    0  27.0      0      0  13.0000         0      25.824561   \n887       1    1  19.0      0      0  30.0000         0      24.210526   \n888       3    1  28.0      1      2  23.4500         0      30.067251   \n889       1    0  26.0      0      0  30.0000         1      23.023392   \n890       3    0  32.0      0      0   7.7500         2      23.175439   \n\n     Name_true_reg  Name_false_lev  Name_false_reg  Ticket_true_lev  \\\n0         0.240875       18.391621        0.280404         8.076923   \n1         0.222573       39.794171        0.199117         6.919231   \n2         0.268535       20.091075        0.222354        13.261538   \n3         0.225927       33.755920        0.199984         5.938462   \n4         0.256739       18.566485        0.311537         6.100000   \n..             ...             ...             ...              ...   \n886       0.129386       20.513661        0.148259         5.811538   \n887       0.275001       22.284153        0.211829         5.846154   \n888       0.288539       30.038251        0.231516         9.200000   \n889       0.228507       18.191257        0.271455         5.961538   \n890       0.216775       17.273224        0.278064         5.938462   \n\n     Ticket_true_reg  Ticket_false_lev  Ticket_false_reg  Survived  \n0           0.061181          8.010638          0.059223         0  \n1           0.101571          7.493617          0.049166         1  \n2           0.064463         13.236170          0.071896         1  \n3           0.078559          6.265957          0.055481         1  \n4           0.059899          6.093617          0.097651         0  \n..               ...               ...               ...       ...  \n886         0.061989          6.168085          0.067192         0  \n887         0.060876          6.104255          0.057290         1  \n888         0.032052          9.157447          0.038314         0  \n889         0.093097          6.304255          0.077616         1  \n890         0.051723          5.942553          0.053340         0  \n\n[891 rows x 16 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pclass</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Fare</th>\n      <th>Embarked</th>\n      <th>Name_true_lev</th>\n      <th>Name_true_reg</th>\n      <th>Name_false_lev</th>\n      <th>Name_false_reg</th>\n      <th>Ticket_true_lev</th>\n      <th>Ticket_true_reg</th>\n      <th>Ticket_false_lev</th>\n      <th>Ticket_false_reg</th>\n      <th>Survived</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>7.2500</td>\n      <td>0</td>\n      <td>23.330409</td>\n      <td>0.240875</td>\n      <td>18.391621</td>\n      <td>0.280404</td>\n      <td>8.076923</td>\n      <td>0.061181</td>\n      <td>8.010638</td>\n      <td>0.059223</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>71.2833</td>\n      <td>1</td>\n      <td>39.064327</td>\n      <td>0.222573</td>\n      <td>39.794171</td>\n      <td>0.199117</td>\n      <td>6.919231</td>\n      <td>0.101571</td>\n      <td>7.493617</td>\n      <td>0.049166</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>3</td>\n      <td>1</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.9250</td>\n      <td>0</td>\n      <td>23.897661</td>\n      <td>0.268535</td>\n      <td>20.091075</td>\n      <td>0.222354</td>\n      <td>13.261538</td>\n      <td>0.064463</td>\n      <td>13.236170</td>\n      <td>0.071896</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>53.1000</td>\n      <td>0</td>\n      <td>33.649123</td>\n      <td>0.225927</td>\n      <td>33.755920</td>\n      <td>0.199984</td>\n      <td>5.938462</td>\n      <td>0.078559</td>\n      <td>6.265957</td>\n      <td>0.055481</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>3</td>\n      <td>0</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8.0500</td>\n      <td>0</td>\n      <td>23.242690</td>\n      <td>0.256739</td>\n      <td>18.566485</td>\n      <td>0.311537</td>\n      <td>6.100000</td>\n      <td>0.059899</td>\n      <td>6.093617</td>\n      <td>0.097651</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <td>886</td>\n      <td>2</td>\n      <td>0</td>\n      <td>27.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>13.0000</td>\n      <td>0</td>\n      <td>25.824561</td>\n      <td>0.129386</td>\n      <td>20.513661</td>\n      <td>0.148259</td>\n      <td>5.811538</td>\n      <td>0.061989</td>\n      <td>6.168085</td>\n      <td>0.067192</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>887</td>\n      <td>1</td>\n      <td>1</td>\n      <td>19.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>30.0000</td>\n      <td>0</td>\n      <td>24.210526</td>\n      <td>0.275001</td>\n      <td>22.284153</td>\n      <td>0.211829</td>\n      <td>5.846154</td>\n      <td>0.060876</td>\n      <td>6.104255</td>\n      <td>0.057290</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>888</td>\n      <td>3</td>\n      <td>1</td>\n      <td>28.0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>23.4500</td>\n      <td>0</td>\n      <td>30.067251</td>\n      <td>0.288539</td>\n      <td>30.038251</td>\n      <td>0.231516</td>\n      <td>9.200000</td>\n      <td>0.032052</td>\n      <td>9.157447</td>\n      <td>0.038314</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>889</td>\n      <td>1</td>\n      <td>0</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>30.0000</td>\n      <td>1</td>\n      <td>23.023392</td>\n      <td>0.228507</td>\n      <td>18.191257</td>\n      <td>0.271455</td>\n      <td>5.961538</td>\n      <td>0.093097</td>\n      <td>6.304255</td>\n      <td>0.077616</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>3</td>\n      <td>0</td>\n      <td>32.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.7500</td>\n      <td>2</td>\n      <td>23.175439</td>\n      <td>0.216775</td>\n      <td>17.273224</td>\n      <td>0.278064</td>\n      <td>5.938462</td>\n      <td>0.051723</td>\n      <td>5.942553</td>\n      <td>0.053340</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>891 rows × 16 columns</p>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"test_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ntestClass = tangiD_BinaryClassification(test_data, train_data, \"Survived\", \"TestData!\", ['Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch','Ticket', 'Fare', 'Cabin', 'Embarked'])\ntestData = testClass.medianIntifying(10)\ntestData","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-22T13:36:42.615440Z","iopub.execute_input":"2024-10-22T13:36:42.615848Z","iopub.status.idle":"2024-10-22T13:36:54.395473Z","shell.execute_reply.started":"2024-10-22T13:36:42.615773Z","shell.execute_reply":"2024-10-22T13:36:54.394483Z"}},"outputs":[{"name":"stdout","text":"Removed columns due to all NaN or only 1 unique value: []\nNumeric columns: ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\nNon-numeric columns: ['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked']\nMissing data columns: ['Age', 'Fare', 'Cabin']\nDone inputing missing numeric values with median!\nMissing data columns: ['Cabin']\nColumns with high uniques: ['Name', 'Ticket', 'Cabin']\n[No NA values][Less Unique Values] Categoric columns Converted to Integer: Sex\n[No NA values][Less Unique Values] Categoric columns Converted to Integer: Embarked\n[No NA values][Multiple Unique Values] Categoric columns Converted to Distance Based On Cluster: Name\n[No NA values][Multiple Unique Values] Categoric columns Converted to Distance Based On Cluster: Ticket\n[NA values][Multiple Unique Values] Categoric columns Converted to Distance Based On Cluster and Missing Are Predicted: Cabin\nMappings: {'Sex': {'male': 0, 'female': 1}, 'Embarked': {'Q': 0, 'S': 1, 'C': 2}}\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"     Pclass  Sex   Age  SibSp  Parch      Fare  Embarked  Name_true_lev  \\\n0         3    0  34.5      0      0    7.8292         0      22.912281   \n1         3    1  47.0      1      0    7.0000         1      25.918129   \n2         2    0  62.0      0      0    9.6875         0      24.078947   \n3         3    0  27.0      0      0    8.6625         1      22.967836   \n4         3    1  22.0      1      1   12.2875         1      33.301170   \n..      ...  ...   ...    ...    ...       ...       ...            ...   \n413       3    0  27.0      0      0    8.0500         1      24.210526   \n414       1    1  39.0      0      0  108.9000         2      27.643275   \n415       3    0  38.5      0      0    7.2500         1      24.786550   \n416       3    0  27.0      0      0    8.0500         1      22.692982   \n417       3    0  27.0      1      1   22.3583         2      23.941520   \n\n     Name_true_reg  Name_false_lev  Name_false_reg  Ticket_true_lev  \\\n0         0.238211       16.633880        0.306957         6.153846   \n1         0.239974       24.313297        0.229048         5.996154   \n2         0.230024       19.597450        0.270204         5.926923   \n3         0.236747       17.063752        0.297216         5.869231   \n4         0.255400       33.191257        0.222822         6.250000   \n..             ...             ...             ...              ...   \n413       0.207600       17.950820        0.269737         8.253846   \n414       0.157274       24.136612        0.154155         6.823077   \n415       0.229698       20.810565        0.269645        15.423077   \n416       0.253124       17.306011        0.310065         6.180769   \n417       0.253106       20.278689        0.268949         5.769231   \n\n     Ticket_true_reg  Ticket_false_lev  Ticket_false_reg  \n0           0.068473          6.159574          0.047670  \n1           0.039048          5.887234          0.045391  \n2           0.035986          5.951064          0.039179  \n3           0.034021          5.912766          0.040689  \n4           0.065359          6.374468          0.064780  \n..               ...               ...               ...  \n413         0.049999          8.093617          0.064082  \n414         0.107360          7.487234          0.051602  \n415         0.062341         15.153191          0.076489  \n416         0.044993          5.985106          0.038481  \n417         0.051263          6.040426          0.046729  \n\n[418 rows x 15 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pclass</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Fare</th>\n      <th>Embarked</th>\n      <th>Name_true_lev</th>\n      <th>Name_true_reg</th>\n      <th>Name_false_lev</th>\n      <th>Name_false_reg</th>\n      <th>Ticket_true_lev</th>\n      <th>Ticket_true_reg</th>\n      <th>Ticket_false_lev</th>\n      <th>Ticket_false_reg</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>34.5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.8292</td>\n      <td>0</td>\n      <td>22.912281</td>\n      <td>0.238211</td>\n      <td>16.633880</td>\n      <td>0.306957</td>\n      <td>6.153846</td>\n      <td>0.068473</td>\n      <td>6.159574</td>\n      <td>0.047670</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>47.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>7.0000</td>\n      <td>1</td>\n      <td>25.918129</td>\n      <td>0.239974</td>\n      <td>24.313297</td>\n      <td>0.229048</td>\n      <td>5.996154</td>\n      <td>0.039048</td>\n      <td>5.887234</td>\n      <td>0.045391</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>62.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>9.6875</td>\n      <td>0</td>\n      <td>24.078947</td>\n      <td>0.230024</td>\n      <td>19.597450</td>\n      <td>0.270204</td>\n      <td>5.926923</td>\n      <td>0.035986</td>\n      <td>5.951064</td>\n      <td>0.039179</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>3</td>\n      <td>0</td>\n      <td>27.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8.6625</td>\n      <td>1</td>\n      <td>22.967836</td>\n      <td>0.236747</td>\n      <td>17.063752</td>\n      <td>0.297216</td>\n      <td>5.869231</td>\n      <td>0.034021</td>\n      <td>5.912766</td>\n      <td>0.040689</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>3</td>\n      <td>1</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>12.2875</td>\n      <td>1</td>\n      <td>33.301170</td>\n      <td>0.255400</td>\n      <td>33.191257</td>\n      <td>0.222822</td>\n      <td>6.250000</td>\n      <td>0.065359</td>\n      <td>6.374468</td>\n      <td>0.064780</td>\n    </tr>\n    <tr>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <td>413</td>\n      <td>3</td>\n      <td>0</td>\n      <td>27.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8.0500</td>\n      <td>1</td>\n      <td>24.210526</td>\n      <td>0.207600</td>\n      <td>17.950820</td>\n      <td>0.269737</td>\n      <td>8.253846</td>\n      <td>0.049999</td>\n      <td>8.093617</td>\n      <td>0.064082</td>\n    </tr>\n    <tr>\n      <td>414</td>\n      <td>1</td>\n      <td>1</td>\n      <td>39.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>108.9000</td>\n      <td>2</td>\n      <td>27.643275</td>\n      <td>0.157274</td>\n      <td>24.136612</td>\n      <td>0.154155</td>\n      <td>6.823077</td>\n      <td>0.107360</td>\n      <td>7.487234</td>\n      <td>0.051602</td>\n    </tr>\n    <tr>\n      <td>415</td>\n      <td>3</td>\n      <td>0</td>\n      <td>38.5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.2500</td>\n      <td>1</td>\n      <td>24.786550</td>\n      <td>0.229698</td>\n      <td>20.810565</td>\n      <td>0.269645</td>\n      <td>15.423077</td>\n      <td>0.062341</td>\n      <td>15.153191</td>\n      <td>0.076489</td>\n    </tr>\n    <tr>\n      <td>416</td>\n      <td>3</td>\n      <td>0</td>\n      <td>27.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8.0500</td>\n      <td>1</td>\n      <td>22.692982</td>\n      <td>0.253124</td>\n      <td>17.306011</td>\n      <td>0.310065</td>\n      <td>6.180769</td>\n      <td>0.044993</td>\n      <td>5.985106</td>\n      <td>0.038481</td>\n    </tr>\n    <tr>\n      <td>417</td>\n      <td>3</td>\n      <td>0</td>\n      <td>27.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>22.3583</td>\n      <td>2</td>\n      <td>23.941520</td>\n      <td>0.253106</td>\n      <td>20.278689</td>\n      <td>0.268949</td>\n      <td>5.769231</td>\n      <td>0.051263</td>\n      <td>6.040426</td>\n      <td>0.046729</td>\n    </tr>\n  </tbody>\n</table>\n<p>418 rows × 15 columns</p>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntrain_data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-22T13:36:54.397337Z","iopub.execute_input":"2024-10-22T13:36:54.397662Z","iopub.status.idle":"2024-10-22T13:36:54.428918Z","shell.execute_reply.started":"2024-10-22T13:36:54.397598Z","shell.execute_reply":"2024-10-22T13:36:54.427751Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   PassengerId  Survived  Pclass  \\\n0            1         0       3   \n1            2         1       1   \n2            3         1       3   \n3            4         1       1   \n4            5         0       3   \n\n                                                Name     Sex   Age  SibSp  \\\n0                            Braund, Mr. Owen Harris    male  22.0      1   \n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n2                             Heikkinen, Miss. Laina  female  26.0      0   \n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n4                           Allen, Mr. William Henry    male  35.0      0   \n\n   Parch            Ticket     Fare Cabin Embarked  \n0      0         A/5 21171   7.2500   NaN        S  \n1      0          PC 17599  71.2833   C85        C  \n2      0  STON/O2. 3101282   7.9250   NaN        S  \n3      0            113803  53.1000  C123        S  \n4      0            373450   8.0500   NaN        S  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Braund, Mr. Owen Harris</td>\n      <td>male</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>A/5 21171</td>\n      <td>7.2500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n      <td>female</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PC 17599</td>\n      <td>71.2833</td>\n      <td>C85</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>Heikkinen, Miss. Laina</td>\n      <td>female</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>STON/O2. 3101282</td>\n      <td>7.9250</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n      <td>female</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>113803</td>\n      <td>53.1000</td>\n      <td>C123</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>5</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Allen, Mr. William Henry</td>\n      <td>male</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>373450</td>\n      <td>8.0500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"test_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ntest_data.head()\ntest_data.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-22T13:36:54.430514Z","iopub.execute_input":"2024-10-22T13:36:54.430810Z","iopub.status.idle":"2024-10-22T13:36:54.448291Z","shell.execute_reply.started":"2024-10-22T13:36:54.430761Z","shell.execute_reply":"2024-10-22T13:36:54.446982Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"Index(['PassengerId', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch',\n       'Ticket', 'Fare', 'Cabin', 'Embarked'],\n      dtype='object')"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"import pandas as pd\n\n\n\ndef intersection_of_lists(list1, list2):\n    return list(set(list1) & set(list2))\n\n\ndef difference_of_lists(list1, list2):\n    return [item for item in list1 if item not in list2]\n\n\ndef get_numeric_and_non_numeric_columns(df):\n    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n    non_numeric_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()\n    print(f\"Numeric columns: {numeric_cols}\")\n    print(f\"Non-numeric columns: {non_numeric_cols}\")\n    return numeric_cols, non_numeric_cols\n\n\ndef remove_single_unique_or_all_nans(df):\n    removed_columns = []\n    for column in df.columns:\n        if df[column].nunique() <= 1 or df[column].isna().all():\n            removed_columns.append(column)\n            df = df.drop(columns=[column])\n    print(f\"Removed columns due to all NaN or only 1 unique value: {removed_columns}\")\n    return df, removed_columns\n\n\ndef columns_with_missing_values(df):\n    missing_cols = [col for col in df.columns if df[col].isna().any()]\n    print(f\"Missing data columns: {missing_cols}\")\n    return missing_cols\n\n\ndef fill_missingNumeric_with_median(df, missing_cols, numeric_cols):\n    for col in intersection_of_lists(missing_cols, numeric_cols):\n        median_value = df[col].median()\n        df[col].fillna(median_value, inplace=True)\n    print(\"Done inputing missing numeric values with median!\")\n    return df\n\n\ndef columnsCategory_with_more_than_X_percent_unique(df, categoric_cols, perc):\n    total_rows = len(df)\n    threshold = total_rows * 0.01 * perc  # 10% of the total number of rows\n    cols_with_high_uniques = [col for col in categoric_cols if df[col].nunique() > threshold]\n    print(f\"Columns with high uniques: {cols_with_high_uniques}\")\n    return cols_with_high_uniques\n    \n\n\ndef convert_and_create_integer_columns(df, new_columns, mappings, colName):\n    df[colName] = df[colName].astype('object')\n    df[colName], unique_values = pd.factorize(df[colName])\n    # Add the new column name to the list\n    new_columns.append(colName)\n    # Create a mapping dictionary for the column\n    mappings[colName] = {value: i for i, value in enumerate(unique_values)}\n    return df, new_columns, mappings\n\n\ndef fill_missing_and_predict(df, new_columns, mappings, usable_cols, column_name):\n    # Convert and create integer column\n    df, new_columns, mappings = convert_and_create_integer_columns(df, new_columns, mappings, column_name)\n    # Train the model to predict missing values\n    non_missing_idx = df[column_name] != -1  # Using -1 for factorized NaNs\n    missing_idx = df[column_name] == -1\n    if missing_idx.sum() > 0:\n        X_train = df.loc[non_missing_idx, usable_cols]\n        y_train = df.loc[non_missing_idx, column_name]\n        X_test = df.loc[missing_idx,  usable_cols]\n        model = LogisticRegression(max_iter=1000, solver ='lbfgs',  multi_class='auto')\n        model.fit(X_train, y_train)\n        # Predict the missing values\n        predicted = model.predict(X_test)\n        # Replace the missing values with the predicted values\n        df.loc[missing_idx, column_name] = predicted\n    return df, new_columns, mappings\n    \n\ndef get_bigrams(string):\n    # Generate bigrams from a string\n    return [string[i:i+2] for i in range(len(string)-1)]\n\ndef sorensen_dice(a, b):\n    # Sørensen-Dice coefficient for two sets\n    a_bigrams = set(get_bigrams(a))\n    b_bigrams = set(get_bigrams(b))\n    overlap = len(a_bigrams & b_bigrams)\n    total = len(a_bigrams) + len(b_bigrams)\n    if total == 0:\n        return 1.0 if a == b else 0.0  # Handle identical empty strings\n    return 2 * overlap / total\n\n\ndef calculate_meanDistanceFromAList(input_string, string_list):\n    sum_Levenshtein = 0\n    sum_sorensen_dice = 0\n    for string in string_list:\n        sum_Levenshtein = sum_Levenshtein + Levenshtein.distance(input_string, string)\n        sum_sorensen_dice = sum_sorensen_dice + sorensen_dice(input_string, string)\n    return float(sum_Levenshtein/len(string_list)),float(sum_sorensen_dice/len(string_list))\n    \n\ndef takeOut_stringList(df, target, variableCol):\n    return list(df[df[f\"{target}\"]==1][f\"{variableCol}\"].unique()),list(df[df[f\"{target}\"]==0][f\"{variableCol}\"].unique())\n\n\ndef apply_meanDistance(df, column_name, string_list):\n    # Calculate mean distances for each row and add a new column\n    df[['mean_Levenshtein', 'mean_sorensen_dice']] = df[column_name].apply(\n        lambda x: pd.Series(calculate_meanDistanceFromAList(x, string_list))\n    )\n    return df\n\n\ndef create_DistanceMetric(df, new_columns, usable_cols, colName, target, orig_data):\n    df[colName] = df[colName].astype('str')\n    true_NameList, false_NameList = takeOut_stringList(orig_data, target, colName)\n    new_columns.append(colName)\n    colName_true_lev = str(colName+\"_true_lev\")\n    colName_true_reg = str(colName+\"_true_reg\")\n    df[[colName_true_lev, colName_true_reg]] = df[colName].apply(\n        lambda x: pd.Series(calculate_meanDistanceFromAList(x, true_NameList))\n    )\n    colName_false_lev = str(colName+\"_false_lev\")\n    colName_false_reg = str(colName+\"_false_reg\")\n    df[[colName_false_lev, colName_false_reg]] = df[colName].apply(\n        lambda x: pd.Series(calculate_meanDistanceFromAList(x, false_NameList))\n    )\n    usable_cols = usable_cols + [colName_true_lev, colName_true_reg, colName_false_lev, colName_false_reg]\n    return df, new_columns, usable_cols\n\n\ndef convert_All_integer_columns(df, numeric_cols, missing_cols, categoric_cols, cols_with_high_uniques, target, orig_data):\n    new_columns = []\n    mappings = {}\n    usable_cols = numeric_cols \n    \n    categoric_nonNA_cols = difference_of_lists(categoric_cols, missing_cols)\n\n    categoric_nonNA_Few_cols = difference_of_lists(categoric_nonNA_cols, cols_with_high_uniques)\n    \n    categoric_nonNA_Multiple_cols = difference_of_lists(categoric_nonNA_cols, categoric_nonNA_Few_cols)\n    \n    categoric_NA_Few_cols = difference_of_lists(missing_cols, cols_with_high_uniques)\n\n    categoric_NA_Multiple_cols = difference_of_lists(missing_cols, categoric_NA_Few_cols)\n    \n    for col in categoric_nonNA_Few_cols:\n        df, new_columns, mappings = convert_and_create_integer_columns(df, new_columns, mappings, col)\n        print(f\"[No NA values][Less Unique Values] Categoric columns Converted to Integer: {col}\")\n    usable_cols = usable_cols + categoric_nonNA_Few_cols\n    for col in  categoric_NA_Few_cols:   \n        df, new_columns, mappings = fill_missing_and_predict(df, new_columns, mappings, usable_cols, col)\n        print(f\"[NA values][Less Unique Values] Categoric columns Converted to Integer and Missing Are Predicted: {col}\")\n        usable_cols = usable_cols + [col]    \n    for col in categoric_nonNA_Multiple_cols:   \n        df, new_columns, usable_cols = create_DistanceMetric(df, new_columns, usable_cols, col, target, orig_data)\n        df = df.drop(columns=[col])\n        print(f\"[No NA values][Multiple Unique Values] Categoric columns Converted to Distance Based On Cluster: {col}\")\n    for col in  categoric_NA_Multiple_cols:\n        df = df.drop(columns=[col])\n        print(f\"[NA values][Multiple Unique Values] Categoric columns Converted to Distance Based On Cluster and Missing Are Predicted: {col}\")  \n    print(f\"Mappings: {mappings}\")\n    return df, new_columns, mappings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-22T13:36:54.449996Z","iopub.execute_input":"2024-10-22T13:36:54.450378Z","iopub.status.idle":"2024-10-22T13:36:54.507863Z","shell.execute_reply.started":"2024-10-22T13:36:54.450313Z","shell.execute_reply":"2024-10-22T13:36:54.506688Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"df_cleaned, removed_columns = remove_single_unique_or_all_nans(train_data[['Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']].copy())\nnumeric_cols, non_numeric_cols = get_numeric_and_non_numeric_columns(df_cleaned)\nmissing_cols = columns_with_missing_values(df_cleaned)\nfilledNumeric_df = fill_missingNumeric_with_median(df_cleaned, missing_cols, numeric_cols)\nmissing_cols = columns_with_missing_values(filledNumeric_df)\nhigh_uniques = columnsCategory_with_more_than_X_percent_unique(filledNumeric_df, non_numeric_cols, 10)\nupdated_df_train, new_columns_train, mappings_train = convert_All_integer_columns(filledNumeric_df, numeric_cols, missing_cols, non_numeric_cols, high_uniques, 'Survived', train_data)\ntrain_df = pd.concat([updated_df_train, train_data['Survived']], axis=1)\ntrain_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-22T13:36:54.509734Z","iopub.execute_input":"2024-10-22T13:36:54.510104Z","iopub.status.idle":"2024-10-22T13:37:19.651217Z","shell.execute_reply.started":"2024-10-22T13:36:54.510038Z","shell.execute_reply":"2024-10-22T13:37:19.650233Z"}},"outputs":[{"name":"stdout","text":"Removed columns due to all NaN or only 1 unique value: []\nNumeric columns: ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\nNon-numeric columns: ['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked']\nMissing data columns: ['Age', 'Cabin', 'Embarked']\nDone inputing missing numeric values with median!\nMissing data columns: ['Cabin', 'Embarked']\nColumns with high uniques: ['Name', 'Ticket', 'Cabin']\n[No NA values][Less Unique Values] Categoric columns Converted to Integer: Sex\n[NA values][Less Unique Values] Categoric columns Converted to Integer and Missing Are Predicted: Embarked\n[No NA values][Multiple Unique Values] Categoric columns Converted to Distance Based On Cluster: Name\n[No NA values][Multiple Unique Values] Categoric columns Converted to Distance Based On Cluster: Ticket\n[NA values][Multiple Unique Values] Categoric columns Converted to Distance Based On Cluster and Missing Are Predicted: Cabin\nMappings: {'Sex': {'male': 0, 'female': 1}, 'Embarked': {'S': 0, 'C': 1, 'Q': 2}}\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"     Pclass  Sex   Age  SibSp  Parch     Fare  Embarked  Name_true_lev  \\\n0         3    0  22.0      1      0   7.2500         0      23.330409   \n1         1    1  38.0      1      0  71.2833         1      39.064327   \n2         3    1  26.0      0      0   7.9250         0      23.897661   \n3         1    1  35.0      1      0  53.1000         0      33.649123   \n4         3    0  35.0      0      0   8.0500         0      23.242690   \n..      ...  ...   ...    ...    ...      ...       ...            ...   \n886       2    0  27.0      0      0  13.0000         0      25.824561   \n887       1    1  19.0      0      0  30.0000         0      24.210526   \n888       3    1  28.0      1      2  23.4500         0      30.067251   \n889       1    0  26.0      0      0  30.0000         1      23.023392   \n890       3    0  32.0      0      0   7.7500         2      23.175439   \n\n     Name_true_reg  Name_false_lev  Name_false_reg  Ticket_true_lev  \\\n0         0.240875       18.391621        0.280404         8.076923   \n1         0.222573       39.794171        0.199117         6.919231   \n2         0.268535       20.091075        0.222354        13.261538   \n3         0.225927       33.755920        0.199984         5.938462   \n4         0.256739       18.566485        0.311537         6.100000   \n..             ...             ...             ...              ...   \n886       0.129386       20.513661        0.148259         5.811538   \n887       0.275001       22.284153        0.211829         5.846154   \n888       0.288539       30.038251        0.231516         9.200000   \n889       0.228507       18.191257        0.271455         5.961538   \n890       0.216775       17.273224        0.278064         5.938462   \n\n     Ticket_true_reg  Ticket_false_lev  Ticket_false_reg  Survived  \n0           0.061181          8.010638          0.059223         0  \n1           0.101571          7.493617          0.049166         1  \n2           0.064463         13.236170          0.071896         1  \n3           0.078559          6.265957          0.055481         1  \n4           0.059899          6.093617          0.097651         0  \n..               ...               ...               ...       ...  \n886         0.061989          6.168085          0.067192         0  \n887         0.060876          6.104255          0.057290         1  \n888         0.032052          9.157447          0.038314         0  \n889         0.093097          6.304255          0.077616         1  \n890         0.051723          5.942553          0.053340         0  \n\n[891 rows x 16 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pclass</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Fare</th>\n      <th>Embarked</th>\n      <th>Name_true_lev</th>\n      <th>Name_true_reg</th>\n      <th>Name_false_lev</th>\n      <th>Name_false_reg</th>\n      <th>Ticket_true_lev</th>\n      <th>Ticket_true_reg</th>\n      <th>Ticket_false_lev</th>\n      <th>Ticket_false_reg</th>\n      <th>Survived</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>7.2500</td>\n      <td>0</td>\n      <td>23.330409</td>\n      <td>0.240875</td>\n      <td>18.391621</td>\n      <td>0.280404</td>\n      <td>8.076923</td>\n      <td>0.061181</td>\n      <td>8.010638</td>\n      <td>0.059223</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>71.2833</td>\n      <td>1</td>\n      <td>39.064327</td>\n      <td>0.222573</td>\n      <td>39.794171</td>\n      <td>0.199117</td>\n      <td>6.919231</td>\n      <td>0.101571</td>\n      <td>7.493617</td>\n      <td>0.049166</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>3</td>\n      <td>1</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.9250</td>\n      <td>0</td>\n      <td>23.897661</td>\n      <td>0.268535</td>\n      <td>20.091075</td>\n      <td>0.222354</td>\n      <td>13.261538</td>\n      <td>0.064463</td>\n      <td>13.236170</td>\n      <td>0.071896</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>53.1000</td>\n      <td>0</td>\n      <td>33.649123</td>\n      <td>0.225927</td>\n      <td>33.755920</td>\n      <td>0.199984</td>\n      <td>5.938462</td>\n      <td>0.078559</td>\n      <td>6.265957</td>\n      <td>0.055481</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>3</td>\n      <td>0</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8.0500</td>\n      <td>0</td>\n      <td>23.242690</td>\n      <td>0.256739</td>\n      <td>18.566485</td>\n      <td>0.311537</td>\n      <td>6.100000</td>\n      <td>0.059899</td>\n      <td>6.093617</td>\n      <td>0.097651</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <td>886</td>\n      <td>2</td>\n      <td>0</td>\n      <td>27.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>13.0000</td>\n      <td>0</td>\n      <td>25.824561</td>\n      <td>0.129386</td>\n      <td>20.513661</td>\n      <td>0.148259</td>\n      <td>5.811538</td>\n      <td>0.061989</td>\n      <td>6.168085</td>\n      <td>0.067192</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>887</td>\n      <td>1</td>\n      <td>1</td>\n      <td>19.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>30.0000</td>\n      <td>0</td>\n      <td>24.210526</td>\n      <td>0.275001</td>\n      <td>22.284153</td>\n      <td>0.211829</td>\n      <td>5.846154</td>\n      <td>0.060876</td>\n      <td>6.104255</td>\n      <td>0.057290</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>888</td>\n      <td>3</td>\n      <td>1</td>\n      <td>28.0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>23.4500</td>\n      <td>0</td>\n      <td>30.067251</td>\n      <td>0.288539</td>\n      <td>30.038251</td>\n      <td>0.231516</td>\n      <td>9.200000</td>\n      <td>0.032052</td>\n      <td>9.157447</td>\n      <td>0.038314</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>889</td>\n      <td>1</td>\n      <td>0</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>30.0000</td>\n      <td>1</td>\n      <td>23.023392</td>\n      <td>0.228507</td>\n      <td>18.191257</td>\n      <td>0.271455</td>\n      <td>5.961538</td>\n      <td>0.093097</td>\n      <td>6.304255</td>\n      <td>0.077616</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>3</td>\n      <td>0</td>\n      <td>32.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.7500</td>\n      <td>2</td>\n      <td>23.175439</td>\n      <td>0.216775</td>\n      <td>17.273224</td>\n      <td>0.278064</td>\n      <td>5.938462</td>\n      <td>0.051723</td>\n      <td>5.942553</td>\n      <td>0.053340</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>891 rows × 16 columns</p>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"import pandas as pd\nimport dask.dataframe as dd\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nimport statsmodels.api as sm\n\ndef very_fast_backward_feature_selection(df, target, n_features_to_select, models_dict, n_jobs):\n    # Convert pandas DataFrame to Dask DataFrame\n    data = dd.from_pandas(df, npartitions=10)\n    \n    # Prepare the feature matrix and target vector\n    X = data.drop(columns=[target])\n    y = data[target]\n    \n    results = {}\n    all_selected_features = set()\n    \n    for model_name, model in models_dict.items():\n        try:\n            # Initialize SFS with the model\n            sfs = SFS(model, \n                      k_features=n_features_to_select, \n                      forward=False, \n                      floating=False, \n                      scoring='accuracy', \n                      cv=2, \n                      n_jobs=n_jobs)\n            \n            # Convert Dask DataFrame to pandas for fitting\n            X_pandas = X.compute()\n            y_pandas = y.compute()\n            \n            # Fit SFS\n            sfs = sfs.fit(X_pandas, y_pandas)\n            \n            # Get the names of the selected features\n            selected_features = list(sfs.k_feature_names_)\n            \n            # Add selected features to the union set\n            all_selected_features.update(selected_features)\n            \n            # Fit model using statsmodels for p-values and coefficients\n            X_selected = sm.add_constant(X_pandas[selected_features])\n            sm_model = sm.OLS(y_pandas, X_selected).fit()\n            \n            summary = sm_model.summary2().tables[1]\n            \n            # Print the summary\n            print(f\"Model: {model_name}\")\n            print(sm_model.summary())\n            \n            # Store the selected features and model summary\n            results[model_name] = {\n                'selected_features': selected_features,\n                'model_summary': summary\n            }\n            \n        except Exception as e:\n            print(f\"Error processing model {model_name}: {e}\")\n    \n    # Convert the set of all selected features to a list\n    all_selected_features = list(all_selected_features)\n    print(\"Union of all selected features:\", all_selected_features)\n    return results, all_selected_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-22T13:44:06.606152Z","iopub.execute_input":"2024-10-22T13:44:06.606651Z","iopub.status.idle":"2024-10-22T13:44:06.626661Z","shell.execute_reply.started":"2024-10-22T13:44:06.606570Z","shell.execute_reply":"2024-10-22T13:44:06.625323Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"FS_models_dict = {\n    'NaiveBayes': GaussianNB(),\n    'DecisionTreeClassifier': DecisionTreeClassifier(),\n    'GradientBoostingClassifier': GradientBoostingClassifier(learning_rate=0.01),\n    'RandomForestClassifier': RandomForestClassifier(n_estimators=10, max_depth=2, random_state=42)\n}\nresults, selected_features = very_fast_backward_feature_selection(train_df, 'Survived', n_features_to_select=7, models_dict=FS_models_dict, n_jobs=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-22T13:42:53.046050Z","iopub.execute_input":"2024-10-22T13:42:53.046483Z","iopub.status.idle":"2024-10-22T13:43:17.111389Z","shell.execute_reply.started":"2024-10-22T13:42:53.046419Z","shell.execute_reply":"2024-10-22T13:43:17.110304Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning:\n\nMethod .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n\n","output_type":"stream"},{"name":"stdout","text":"Model: NaiveBayes\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               Survived   R-squared:                       0.365\nModel:                            OLS   Adj. R-squared:                  0.360\nMethod:                 Least Squares   F-statistic:                     72.48\nDate:                Tue, 22 Oct 2024   Prob (F-statistic):           9.07e-83\nTime:                        13:42:53   Log-Likelihood:                -419.71\nNo. Observations:                 891   AIC:                             855.4\nDf Residuals:                     883   BIC:                             893.8\nDf Model:                           7                                         \nCovariance Type:            nonrobust                                         \n====================================================================================\n                       coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------------\nconst                0.5920      0.279      2.124      0.034       0.045       1.139\nSex                  0.0858      0.075      1.143      0.253      -0.062       0.233\nSibSp               -0.0583      0.012     -4.751      0.000      -0.082      -0.034\nName_true_lev       -0.0317      0.024     -1.336      0.182      -0.078       0.015\nName_true_reg        4.0582      0.998      4.068      0.000       2.100       6.016\nName_false_lev       0.0339      0.019      1.827      0.068      -0.003       0.070\nName_false_reg      -3.7886      0.932     -4.064      0.000      -5.618      -1.959\nTicket_false_reg    -2.4067      0.614     -3.918      0.000      -3.612      -1.201\n==============================================================================\nOmnibus:                       12.200   Durbin-Watson:                   1.986\nProb(Omnibus):                  0.002   Jarque-Bera (JB):               12.507\nSkew:                           0.290   Prob(JB):                      0.00192\nKurtosis:                       2.972   Cond. No.                     3.45e+03\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 3.45e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning:\n\nMethod .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n\n","output_type":"stream"},{"name":"stdout","text":"Model: DecisionTreeClassifier\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               Survived   R-squared:                       0.399\nModel:                            OLS   Adj. R-squared:                  0.394\nMethod:                 Least Squares   F-statistic:                     83.76\nDate:                Tue, 22 Oct 2024   Prob (F-statistic):           2.89e-93\nTime:                        13:42:54   Log-Likelihood:                -395.10\nNo. Observations:                 891   AIC:                             806.2\nDf Residuals:                     883   BIC:                             844.5\nDf Model:                           7                                         \nCovariance Type:            nonrobust                                         \n====================================================================================\n                       coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------------\nconst                0.5390      0.120      4.480      0.000       0.303       0.775\nPclass              -0.1337      0.023     -5.835      0.000      -0.179      -0.089\nSex                  0.4712      0.029     16.278      0.000       0.414       0.528\nAge                 -0.0050      0.001     -4.744      0.000      -0.007      -0.003\nFare             -7.454e-05      0.000     -0.237      0.813      -0.001       0.001\nName_true_lev        0.0068      0.003      2.222      0.027       0.001       0.013\nTicket_true_reg      2.4963      0.859      2.908      0.004       0.811       4.181\nTicket_false_reg    -3.1094      0.726     -4.281      0.000      -4.535      -1.684\n==============================================================================\nOmnibus:                       26.783   Durbin-Watson:                   1.930\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               28.669\nSkew:                           0.439   Prob(JB):                     5.95e-07\nKurtosis:                       3.011   Cond. No.                     5.10e+03\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 5.1e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning:\n\nMethod .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n\n","output_type":"stream"},{"name":"stdout","text":"Model: GradientBoostingClassifier\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               Survived   R-squared:                       0.394\nModel:                            OLS   Adj. R-squared:                  0.389\nMethod:                 Least Squares   F-statistic:                     81.89\nDate:                Tue, 22 Oct 2024   Prob (F-statistic):           1.47e-91\nTime:                        13:43:13   Log-Likelihood:                -399.10\nNo. Observations:                 891   AIC:                             814.2\nDf Residuals:                     883   BIC:                             852.5\nDf Model:                           7                                         \nCovariance Type:            nonrobust                                         \n====================================================================================\n                       coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------------\nconst                0.1489      0.070      2.128      0.034       0.012       0.286\nSex                  0.4513      0.031     14.464      0.000       0.390       0.513\nAge                 -0.0042      0.001     -4.101      0.000      -0.006      -0.002\nSibSp               -0.0589      0.012     -4.787      0.000      -0.083      -0.035\nFare                 0.0009      0.000      2.865      0.004       0.000       0.001\nName_false_lev       0.0093      0.002      3.866      0.000       0.005       0.014\nTicket_true_reg      4.6050      0.752      6.121      0.000       3.129       6.082\nTicket_false_reg    -4.4866      0.668     -6.721      0.000      -5.797      -3.176\n==============================================================================\nOmnibus:                       21.104   Durbin-Watson:                   1.968\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               22.258\nSkew:                           0.387   Prob(JB):                     1.47e-05\nKurtosis:                       2.956   Cond. No.                     4.32e+03\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 4.32e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning:\n\nMethod .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n\n","output_type":"stream"},{"name":"stdout","text":"Model: RandomForestClassifier\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               Survived   R-squared:                       0.384\nModel:                            OLS   Adj. R-squared:                  0.379\nMethod:                 Least Squares   F-statistic:                     78.49\nDate:                Tue, 22 Oct 2024   Prob (F-statistic):           1.96e-88\nTime:                        13:43:17   Log-Likelihood:                -406.43\nNo. Observations:                 891   AIC:                             828.9\nDf Residuals:                     883   BIC:                             867.2\nDf Model:                           7                                         \nCovariance Type:            nonrobust                                         \n====================================================================================\n                       coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------------\nconst                0.6118      0.165      3.700      0.000       0.287       0.936\nSex                  0.4231      0.042     10.106      0.000       0.341       0.505\nSibSp               -0.0430      0.012     -3.613      0.000      -0.066      -0.020\nName_false_reg      -1.6498      0.538     -3.068      0.002      -2.705      -0.594\nTicket_true_lev     -0.3422      0.110     -3.123      0.002      -0.557      -0.127\nTicket_true_reg      1.9610      1.204      1.629      0.104      -0.401       4.324\nTicket_false_lev     0.3467      0.112      3.097      0.002       0.127       0.566\nTicket_false_reg    -1.8670      1.118     -1.670      0.095      -4.061       0.327\n==============================================================================\nOmnibus:                       20.805   Durbin-Watson:                   1.955\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               21.883\nSkew:                           0.384   Prob(JB):                     1.77e-05\nKurtosis:                       2.987   Cond. No.                     1.22e+03\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.22e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"selected_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-22T13:44:11.330526Z","iopub.execute_input":"2024-10-22T13:44:11.330934Z","iopub.status.idle":"2024-10-22T13:44:11.337518Z","shell.execute_reply.started":"2024-10-22T13:44:11.330876Z","shell.execute_reply":"2024-10-22T13:44:11.336406Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"['Name_true_reg',\n 'Ticket_true_lev',\n 'Fare',\n 'Ticket_false_lev',\n 'Pclass',\n 'Age',\n 'SibSp',\n 'Ticket_true_reg',\n 'Sex',\n 'Name_false_reg',\n 'Name_true_lev',\n 'Ticket_false_reg',\n 'Name_false_lev']"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"import pandas as pd\nimport dask.dataframe as dd\nfrom dask_ml.model_selection import GridSearchCV\nimport optuna\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import make_scorer, accuracy_score\nfrom dask.distributed import Client\n\n# Start Dask client\nclient = Client()\n\ndef optimize_models(models_dict, X, y, n_trials=20, n_jobs=1):\n    best_models = {}\n    best_scores = {}\n\n    # Convert to Dask DataFrame\n    X = dd.from_pandas(X, npartitions=10)\n    y = dd.from_pandas(y, npartitions=10)\n\n    # Objective function to optimize\n    def objective(trial, model_name):\n        model = models_dict[model_name]\n\n        if model_name == 'RandomForestClassifier':\n            param_grid = {\n                'n_estimators': trial.suggest_int('n_estimators', 1, 10),\n                'max_depth': trial.suggest_int('max_depth', 1, 4),\n                'min_samples_split': trial.suggest_int('min_samples_split', 2, 4),\n                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 4)\n            }\n        elif model_name == 'GradientBoostingClassifier':\n            param_grid = {\n                'n_estimators': trial.suggest_int('n_estimators', 1, 10),\n                'max_depth': trial.suggest_int('max_depth', 1, 4)\n            }\n        \n        # Perform cross-validation\n        gs = GridSearchCV(model, param_grid, cv=3, scoring=make_scorer(accuracy_score), n_jobs=n_jobs)\n        gs.fit(X.compute(), y.compute())\n        return gs.best_score_\n\n    # Create a study object and optimize the objective function for each model\n    for model_name in models_dict.keys():\n        print(f\"Optimizing {model_name}...\")\n        study = optuna.create_study(direction='maximize')\n        study.optimize(lambda trial: objective(trial, model_name), n_trials=n_trials, n_jobs=n_jobs)\n\n        # Store best hyperparameters and score\n        best_models[model_name] = study.best_params\n        best_scores[model_name] = study.best_value\n\n        # Print best hyperparameters and score\n        print(f\"Best hyperparameters for {model_name}: {study.best_params}\")\n        print(f\"Best score for {model_name}: {study.best_value}\\n\")\n\n    return best_models, best_scores\n\ndef update_model_params(models_dict, best_params):\n    # Update each model with the best hyperparameters\n    for model_name, params in best_params.items():\n        model = models_dict[model_name]\n        model.set_params(**params)\n    return models_dict\n\n# Example usage\ndata = pd.DataFrame({\n    'feature_1': np.random.normal(size=1000000),\n    'feature_2': np.random.normal(size=1000000),\n    'feature_3': np.random.normal(size=1000000),\n    'response': np.random.randint(0, 2, size=1000000)\n})\n\nX = data.drop(columns=['response'])\ny = data['response']\n\nmodels_dict = {\n    'RandomForestClassifier': RandomForestClassifier(),\n    'GradientBoostingClassifier': GradientBoostingClassifier()\n}\n\nbest_models, best_scores = optimize_models(models_dict, X, y, n_trials=20, n_jobs=1)\nprint(\"Best models:\", best_models)\nprint(\"Best scores:\", best_scores)-1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-22T14:00:49.295337Z","iopub.execute_input":"2024-10-22T14:00:49.295750Z","iopub.status.idle":"2024-10-22T14:00:50.218381Z","shell.execute_reply.started":"2024-10-22T14:00:49.295681Z","shell.execute_reply":"2024-10-22T14:00:50.208595Z"}},"outputs":[{"name":"stdout","text":"Optimizing RandomForestClassifier...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-5c3cfdccf58b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     78\u001b[0m }\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m \u001b[0mbest_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimize_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best models:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_models\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best scores:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-27-5c3cfdccf58b>\u001b[0m in \u001b[0;36moptimize_models\u001b[0;34m(models_dict, X, y, n_trials)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         )\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0msearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# Store best hyperparameters and score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/dask_ml/model_selection/_incremental.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0mAdditional\u001b[0m \u001b[0mpartial\u001b[0m \u001b[0mfit\u001b[0m \u001b[0mkeyword\u001b[0m \u001b[0marguments\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m         \"\"\"\n\u001b[0;32m--> 608\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdefault_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mif_delegate_has_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"best_estimator_\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"estimator\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/distributed/client.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(self, func, asynchronous, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m             return sync(\n\u001b[0;32m--> 753\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m             )\n\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/distributed/utils.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/distributed/utils.py\u001b[0m in \u001b[0;36mf\u001b[0;34m()\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcallback_timeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseconds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32myield\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tornado/gen.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhad_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tornado/gen.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                     \u001b[0morig_stack_contexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontexts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m                     \u001b[0myielded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mstack_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontexts\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0morig_stack_contexts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m                         \u001b[0myielded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_future\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/dask_ml/model_selection/_hyperband.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoroutine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mbrackets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_hyperband_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggressiveness\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/dask_ml/model_selection/_incremental.py\u001b[0m in \u001b[0;36m_validate_parameters\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    446\u001b[0m             )\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/dask_ml/model_selection/_incremental.py\u001b[0m in \u001b[0;36m_check_array\u001b[0;34m(self, X, **kwargs)\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/dask_ml/utils.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, *args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0maccept_dask_dataframe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"This estimator does not support dask dataframes.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0;31m# TODO: sample?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: This estimator does not support dask dataframes."],"ename":"TypeError","evalue":"This estimator does not support dask dataframes.","output_type":"error"}],"execution_count":27},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_cleaned, removed_columns = remove_single_unique_or_all_nans(test_data[['Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']].copy())\nnumeric_cols, non_numeric_cols = get_numeric_and_non_numeric_columns(df_cleaned)\nmissing_cols = columns_with_missing_values(df_cleaned)\nfilledNumeric_df = fill_missingNumeric_with_median(df_cleaned, missing_cols, numeric_cols)\nmissing_cols = columns_with_missing_values(filledNumeric_df)\nhigh_uniques = columnsCategory_with_more_than_X_percent_unique(filledNumeric_df, non_numeric_cols, 10)\nupdated_df_test, new_columns_test, mappings_test = convert_All_integer_columns(filledNumeric_df, numeric_cols, missing_cols, non_numeric_cols, high_uniques, 'Survived', train_data)\nupdated_df_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-22T13:37:19.652953Z","iopub.execute_input":"2024-10-22T13:37:19.653356Z","iopub.status.idle":"2024-10-22T13:37:31.449968Z","shell.execute_reply.started":"2024-10-22T13:37:19.653287Z","shell.execute_reply":"2024-10-22T13:37:31.448929Z"}},"outputs":[{"name":"stdout","text":"Removed columns due to all NaN or only 1 unique value: []\nNumeric columns: ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\nNon-numeric columns: ['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked']\nMissing data columns: ['Age', 'Fare', 'Cabin']\nDone inputing missing numeric values with median!\nMissing data columns: ['Cabin']\nColumns with high uniques: ['Name', 'Ticket', 'Cabin']\n[No NA values][Less Unique Values] Categoric columns Converted to Integer: Sex\n[No NA values][Less Unique Values] Categoric columns Converted to Integer: Embarked\n[No NA values][Multiple Unique Values] Categoric columns Converted to Distance Based On Cluster: Name\n[No NA values][Multiple Unique Values] Categoric columns Converted to Distance Based On Cluster: Ticket\n[NA values][Multiple Unique Values] Categoric columns Converted to Distance Based On Cluster and Missing Are Predicted: Cabin\nMappings: {'Sex': {'male': 0, 'female': 1}, 'Embarked': {'Q': 0, 'S': 1, 'C': 2}}\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"     Pclass  Sex   Age  SibSp  Parch      Fare  Embarked  Name_true_lev  \\\n0         3    0  34.5      0      0    7.8292         0      22.912281   \n1         3    1  47.0      1      0    7.0000         1      25.918129   \n2         2    0  62.0      0      0    9.6875         0      24.078947   \n3         3    0  27.0      0      0    8.6625         1      22.967836   \n4         3    1  22.0      1      1   12.2875         1      33.301170   \n..      ...  ...   ...    ...    ...       ...       ...            ...   \n413       3    0  27.0      0      0    8.0500         1      24.210526   \n414       1    1  39.0      0      0  108.9000         2      27.643275   \n415       3    0  38.5      0      0    7.2500         1      24.786550   \n416       3    0  27.0      0      0    8.0500         1      22.692982   \n417       3    0  27.0      1      1   22.3583         2      23.941520   \n\n     Name_true_reg  Name_false_lev  Name_false_reg  Ticket_true_lev  \\\n0         0.238211       16.633880        0.306957         6.153846   \n1         0.239974       24.313297        0.229048         5.996154   \n2         0.230024       19.597450        0.270204         5.926923   \n3         0.236747       17.063752        0.297216         5.869231   \n4         0.255400       33.191257        0.222822         6.250000   \n..             ...             ...             ...              ...   \n413       0.207600       17.950820        0.269737         8.253846   \n414       0.157274       24.136612        0.154155         6.823077   \n415       0.229698       20.810565        0.269645        15.423077   \n416       0.253124       17.306011        0.310065         6.180769   \n417       0.253106       20.278689        0.268949         5.769231   \n\n     Ticket_true_reg  Ticket_false_lev  Ticket_false_reg  \n0           0.068473          6.159574          0.047670  \n1           0.039048          5.887234          0.045391  \n2           0.035986          5.951064          0.039179  \n3           0.034021          5.912766          0.040689  \n4           0.065359          6.374468          0.064780  \n..               ...               ...               ...  \n413         0.049999          8.093617          0.064082  \n414         0.107360          7.487234          0.051602  \n415         0.062341         15.153191          0.076489  \n416         0.044993          5.985106          0.038481  \n417         0.051263          6.040426          0.046729  \n\n[418 rows x 15 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pclass</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Fare</th>\n      <th>Embarked</th>\n      <th>Name_true_lev</th>\n      <th>Name_true_reg</th>\n      <th>Name_false_lev</th>\n      <th>Name_false_reg</th>\n      <th>Ticket_true_lev</th>\n      <th>Ticket_true_reg</th>\n      <th>Ticket_false_lev</th>\n      <th>Ticket_false_reg</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>34.5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.8292</td>\n      <td>0</td>\n      <td>22.912281</td>\n      <td>0.238211</td>\n      <td>16.633880</td>\n      <td>0.306957</td>\n      <td>6.153846</td>\n      <td>0.068473</td>\n      <td>6.159574</td>\n      <td>0.047670</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>47.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>7.0000</td>\n      <td>1</td>\n      <td>25.918129</td>\n      <td>0.239974</td>\n      <td>24.313297</td>\n      <td>0.229048</td>\n      <td>5.996154</td>\n      <td>0.039048</td>\n      <td>5.887234</td>\n      <td>0.045391</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>62.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>9.6875</td>\n      <td>0</td>\n      <td>24.078947</td>\n      <td>0.230024</td>\n      <td>19.597450</td>\n      <td>0.270204</td>\n      <td>5.926923</td>\n      <td>0.035986</td>\n      <td>5.951064</td>\n      <td>0.039179</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>3</td>\n      <td>0</td>\n      <td>27.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8.6625</td>\n      <td>1</td>\n      <td>22.967836</td>\n      <td>0.236747</td>\n      <td>17.063752</td>\n      <td>0.297216</td>\n      <td>5.869231</td>\n      <td>0.034021</td>\n      <td>5.912766</td>\n      <td>0.040689</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>3</td>\n      <td>1</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>12.2875</td>\n      <td>1</td>\n      <td>33.301170</td>\n      <td>0.255400</td>\n      <td>33.191257</td>\n      <td>0.222822</td>\n      <td>6.250000</td>\n      <td>0.065359</td>\n      <td>6.374468</td>\n      <td>0.064780</td>\n    </tr>\n    <tr>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <td>413</td>\n      <td>3</td>\n      <td>0</td>\n      <td>27.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8.0500</td>\n      <td>1</td>\n      <td>24.210526</td>\n      <td>0.207600</td>\n      <td>17.950820</td>\n      <td>0.269737</td>\n      <td>8.253846</td>\n      <td>0.049999</td>\n      <td>8.093617</td>\n      <td>0.064082</td>\n    </tr>\n    <tr>\n      <td>414</td>\n      <td>1</td>\n      <td>1</td>\n      <td>39.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>108.9000</td>\n      <td>2</td>\n      <td>27.643275</td>\n      <td>0.157274</td>\n      <td>24.136612</td>\n      <td>0.154155</td>\n      <td>6.823077</td>\n      <td>0.107360</td>\n      <td>7.487234</td>\n      <td>0.051602</td>\n    </tr>\n    <tr>\n      <td>415</td>\n      <td>3</td>\n      <td>0</td>\n      <td>38.5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.2500</td>\n      <td>1</td>\n      <td>24.786550</td>\n      <td>0.229698</td>\n      <td>20.810565</td>\n      <td>0.269645</td>\n      <td>15.423077</td>\n      <td>0.062341</td>\n      <td>15.153191</td>\n      <td>0.076489</td>\n    </tr>\n    <tr>\n      <td>416</td>\n      <td>3</td>\n      <td>0</td>\n      <td>27.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8.0500</td>\n      <td>1</td>\n      <td>22.692982</td>\n      <td>0.253124</td>\n      <td>17.306011</td>\n      <td>0.310065</td>\n      <td>6.180769</td>\n      <td>0.044993</td>\n      <td>5.985106</td>\n      <td>0.038481</td>\n    </tr>\n    <tr>\n      <td>417</td>\n      <td>3</td>\n      <td>0</td>\n      <td>27.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>22.3583</td>\n      <td>2</td>\n      <td>23.941520</td>\n      <td>0.253106</td>\n      <td>20.278689</td>\n      <td>0.268949</td>\n      <td>5.769231</td>\n      <td>0.051263</td>\n      <td>6.040426</td>\n      <td>0.046729</td>\n    </tr>\n  </tbody>\n</table>\n<p>418 rows × 15 columns</p>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport joblib\nimport statsmodels.api as sm\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nimport optuna\nfrom sklearn.model_selection import cross_val_score\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\ndef very_fast_backward_feature_selection(data, target, n_features_to_select, models_dict, n_jobs):\n    # Prepare the feature matrix and target vector\n    X = data.drop(columns=[target])\n    y = data[target]\n    \n    results = {}\n    \n    for model_name, model in models_dict.items():\n        # Initialize SFS with the model\n        sfs = SFS(model, \n                  k_features=n_features_to_select, \n                  forward=False, \n                  floating=False, \n                  scoring='accuracy', \n                  cv=2, \n                  n_jobs=n_jobs)\n        \n        # Fit SFS\n        sfs = sfs.fit(X, y)\n        \n        # Get the names of the selected features\n        selected_features = list(sfs.k_feature_names_)\n        \n        # Fit model using statsmodels for p-values and coefficients\n        X_selected = sm.add_constant(X[selected_features])\n        sm_model = sm.OLS(y, X_selected).fit()\n        \n        summary = sm_model.summary2().tables[1]\n        \n        # Print the summary\n        print(f\"Model: {model_name}\")\n        print(sm_model.summary())\n        \n        # Store the selected features and model summary\n        results[model_name] = {\n            'selected_features': selected_features,\n            'model_summary': summary\n        }\n    \n    return results\n\n\ndef optimize_models(models_dict, X, y, n_trials=20, n_jobs=1):\n    best_models = {}\n    best_scores = {}\n\n    # Objective function to optimize\n    def objective(trial, model_name):\n        model = models_dict[model_name]\n\n        if model_name == 'RandomForestClassifier':\n            model.set_params(\n                n_estimators=trial.suggest_int('n_estimators', 1, 10),\n                max_depth=trial.suggest_int('max_depth', 1, 4),\n                min_samples_split=trial.suggest_int('min_samples_split', 2, 4),\n                min_samples_leaf=trial.suggest_int('min_samples_leaf', 1, 4)\n            )\n            \n        elif model_name == 'GradientBoostingClassifier':\n            model.set_params(\n                n_estimators=trial.suggest_int('n_estimators', 1, 10),\n                # learning_rate=trial.suggest_float('learning_rate', 0.01, 0.1),\n                max_depth=trial.suggest_int('max_depth', 1, 4)\n            )\n            \n        elif model_name == 'XGBClassifier':\n            model.set_params(\n                n_estimators=trial.suggest_int('n_estimators', 1, 10),\n                # learning_rate=trial.suggest_float('learning_rate', 0.01, 0.1),\n                max_depth=trial.suggest_int('max_depth', 1, 4)\n            )\n            \n        elif model_name == 'LGBMClassifier':\n            model.set_params(\n                n_estimators=trial.suggest_int('n_estimators', 1, 10),\n                # learning_rate=trial.suggest_float('learning_rate', 0.01, 0.1),\n                max_depth=trial.suggest_int('max_depth', 1, 4)\n            )\n            \n        elif model_name == 'KNeighborsClassifier':\n            model.set_params(\n                n_neighbors=trial.suggest_int('n_neighbors', 1, 10),\n                leaf_size=trial.suggest_int('leaf_size', 10, 30),\n                p=trial.suggest_int('p', 1, 2)\n            )\n            \n        elif model_name == 'SupportVectorClassifier':\n            model.set_params(\n                # C=trial.suggest_float('C', 0.1, 10.0),\n                kernel=trial.suggest_categorical('kernel', ['linear', 'rbf']),\n                gamma=trial.suggest_categorical('gamma', ['scale', 'auto'])\n            )\n\n        elif model_name == 'DecisionTreeClassifier':\n            model.set_params(\n                max_depth=trial.suggest_int('max_depth', 1, 3),\n                min_samples_split=trial.suggest_int('min_samples_split', 2, 4),\n                min_samples_leaf=trial.suggest_int('min_samples_leaf', 1, 4)\n            )\n\n        # Perform cross-validation\n        scores = cross_val_score(model, X, y, cv=3, scoring='accuracy')\n        return scores.mean()\n\n    # Create a study object and optimize the objective function for each model\n    for model_name in models_dict.keys():\n        print(f\"Optimizing {model_name}...\")\n        study = optuna.create_study(direction='maximize')\n        study.optimize(lambda trial: objective(trial, model_name), n_trials=n_trials, n_jobs=n_jobs)  # Use n_jobs for parallel execution\n        \n        # Store best hyperparameters and score\n        best_models[model_name] = study.best_params\n        best_scores[model_name] = study.best_value\n\n        # Print best hyperparameters and score\n        print(f\"Best hyperparameters for {model_name}: {study.best_params}\")\n        print(f\"Best score for {model_name}: {study.best_value}\\n\")\n\n    return best_models, best_scores\n\n\ndef update_model_params(models_dict, best_params):\n    # Update each model with the best hyperparameters\n    for model_name, params in best_params.items():\n        model = models_dict[model_name]\n        model.set_params(**params)\n    return models_dict\n\n\ndef fit_models(models_dict, X, y, save_path='/kaggle/working/'):\n    fitted_models = {}\n\n    for model_name, model in models_dict.items():\n        print(f\"Fitting {model_name}...\")\n        # Fit the model\n        model.fit(X, y)\n        \n        # Save the fitted model\n        model_filename = f\"{save_path}{model_name}.joblib\"\n        joblib.dump(model, model_filename)\n        \n        # Store the model in the dictionary\n        fitted_models[model_name] = model\n\n    return fitted_models","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-22T13:37:33.361779Z","iopub.execute_input":"2024-10-22T13:37:33.362271Z","iopub.status.idle":"2024-10-22T13:37:33.571596Z","shell.execute_reply.started":"2024-10-22T13:37:33.362198Z","shell.execute_reply":"2024-10-22T13:37:33.570175Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import pandas as pd\nimport dask.dataframe as dd\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nimport statsmodels.api as sm\n\ndef very_fast_backward_feature_selection(df, target, n_features_to_select, models_dict, n_jobs):\n    # Convert pandas DataFrame to Dask DataFrame\n    data = dd.from_pandas(df, npartitions=10)\n    \n    # Prepare the feature matrix and target vector\n    X = data.drop(columns=[target])\n    y = data[target]\n    \n    results = {}\n    all_selected_features = set()\n    \n    for model_name, model in models_dict.items():\n        try:\n            # Initialize SFS with the model\n            sfs = SFS(model, \n                      k_features=n_features_to_select, \n                      forward=False, \n                      floating=False, \n                      scoring='accuracy', \n                      cv=2, \n                      n_jobs=n_jobs)\n            \n            # Convert Dask DataFrame to pandas for fitting\n            X_pandas = X.compute()\n            y_pandas = y.compute()\n            \n            # Fit SFS\n            sfs = sfs.fit(X_pandas, y_pandas)\n            \n            # Get the names of the selected features\n            selected_features = list(sfs.k_feature_names_)\n            \n            # Add selected features to the union set\n            all_selected_features.update(selected_features)\n            \n            # Fit model using statsmodels for p-values and coefficients\n            X_selected = sm.add_constant(X_pandas[selected_features])\n            sm_model = sm.OLS(y_pandas, X_selected).fit()\n            \n            summary = sm_model.summary2().tables[1]\n            \n            # Print the summary\n            print(f\"Model: {model_name}\")\n            print(sm_model.summary())\n            \n            # Store the selected features and model summary\n            results[model_name] = {\n                'selected_features': selected_features,\n                'model_summary': summary\n            }\n            \n        except Exception as e:\n            print(f\"Error processing model {model_name}: {e}\")\n    \n    # Convert the set of all selected features to a list\n    all_selected_features = list(all_selected_features)\n    \n    return results, all_selected_features\n\n# Example usage\n# Sample large dataset\ndata = pd.DataFrame({\n    'feature_1': np.random.normal(size=1000000),\n    'feature_2': np.random.normal(size=1000000),\n    'feature_3': np.random.normal(size=1000000),\n    'response': np.random.randint(0, 2, size=1000000)\n})\n\nmodels_dict = {\n    'LogisticRegression': LogisticRegression(max_iter=1000),\n    'RandomForestClassifier': RandomForestClassifier(n_estimators=10, max_depth=2, random_state=42),\n}\n\nresults, all_selected_features = very_fast_backward_feature_selection(data, 'response', n_features_to_select=2, models_dict=models_dict, n_jobs=1)\nprint(\"Results:\", results)\nprint(\"Union of all selected features:\", all_selected_features)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-22T13:39:23.794881Z","iopub.execute_input":"2024-10-22T13:39:23.795312Z","iopub.status.idle":"2024-10-22T13:40:00.464312Z","shell.execute_reply.started":"2024-10-22T13:39:23.795251Z","shell.execute_reply":"2024-10-22T13:40:00.463149Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning:\n\nDefault solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning:\n\nDefault solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning:\n\nDefault solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning:\n\nDefault solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning:\n\nDefault solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning:\n\nDefault solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning:\n\nDefault solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning:\n\nDefault solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n\n/opt/conda/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning:\n\nMethod .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n\n","output_type":"stream"},{"name":"stdout","text":"Model: LogisticRegression\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               response   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                    0.7814\nDate:                Tue, 22 Oct 2024   Prob (F-statistic):              0.458\nTime:                        13:39:32   Log-Likelihood:            -7.2579e+05\nNo. Observations:             1000000   AIC:                         1.452e+06\nDf Residuals:                  999997   BIC:                         1.452e+06\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.4995      0.001    998.999      0.000       0.499       0.500\nfeature_2      0.0006      0.001      1.230      0.219      -0.000       0.002\nfeature_3      0.0001      0.001      0.224      0.823      -0.001       0.001\n==============================================================================\nOmnibus:                  3405199.118   Durbin-Watson:                   2.001\nProb(Omnibus):                  0.000   Jarque-Bera (JB):           166665.625\nSkew:                           0.002   Prob(JB):                         0.00\nKurtosis:                       1.000   Cond. No.                         1.00\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning:\n\nMethod .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n\n","output_type":"stream"},{"name":"stdout","text":"Model: RandomForestClassifier\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               response   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                    0.7814\nDate:                Tue, 22 Oct 2024   Prob (F-statistic):              0.458\nTime:                        13:40:00   Log-Likelihood:            -7.2579e+05\nNo. Observations:             1000000   AIC:                         1.452e+06\nDf Residuals:                  999997   BIC:                         1.452e+06\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.4995      0.001    998.999      0.000       0.499       0.500\nfeature_2      0.0006      0.001      1.230      0.219      -0.000       0.002\nfeature_3      0.0001      0.001      0.224      0.823      -0.001       0.001\n==============================================================================\nOmnibus:                  3405199.118   Durbin-Watson:                   2.001\nProb(Omnibus):                  0.000   Jarque-Bera (JB):           166665.625\nSkew:                           0.002   Prob(JB):                         0.00\nKurtosis:                       1.000   Cond. No.                         1.00\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nResults: {'LogisticRegression': {'selected_features': ['feature_2', 'feature_3'], 'model_summary':               Coef.  Std.Err.           t     P>|t|    [0.025    0.975]\nconst      0.499500    0.0005  998.998937  0.000000  0.498520  0.500480\nfeature_2  0.000615    0.0005    1.230167  0.218635 -0.000365  0.001595\nfeature_3  0.000112    0.0005    0.223990  0.822765 -0.000868  0.001092}, 'RandomForestClassifier': {'selected_features': ['feature_2', 'feature_3'], 'model_summary':               Coef.  Std.Err.           t     P>|t|    [0.025    0.975]\nconst      0.499500    0.0005  998.998937  0.000000  0.498520  0.500480\nfeature_2  0.000615    0.0005    1.230167  0.218635 -0.000365  0.001595\nfeature_3  0.000112    0.0005    0.223990  0.822765 -0.000868  0.001092}}\nUnion of all selected features: ['feature_3', 'feature_2']\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"FS_models_dict = {\n    'LogisticRegression': LogisticRegression(max_iter=1000, solver='liblinear', penalty='l2'),\n    'SupportVectorClassifier': SVC(probability=True, gamma='auto')\n}\nfinal_models_dict = {\n    'LogisticRegression': LogisticRegression(max_iter=1000, solver='liblinear', penalty='l2'),\n    'RandomForestClassifier': RandomForestClassifier(),\n    'SupportVectorClassifier': SVC(probability=True, C=1),\n    'KNeighborsClassifier': KNeighborsClassifier(),\n    'NaiveBayes': GaussianNB(),\n    'DecisionTreeClassifier': DecisionTreeClassifier(),\n    'GradientBoostingClassifier': GradientBoostingClassifier(learning_rate=0.01),\n    'XGBClassifier': XGBClassifier(use_label_encoder=False, eval_metric='logloss', learning_rate=0.01),\n    'LGBMClassifier': LGBMClassifier()\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-22T00:13:19.426858Z","iopub.execute_input":"2024-10-22T00:13:19.427196Z","iopub.status.idle":"2024-10-22T00:13:19.435783Z","shell.execute_reply.started":"2024-10-22T00:13:19.427134Z","shell.execute_reply":"2024-10-22T00:13:19.434567Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"results = very_fast_backward_feature_selection(train_df, 'Survived', n_features_to_select=1, models_dict=FS_models_dict, n_jobs=1)\nX = train_df.drop(columns=['Survived'])\ny = train_df['Survived']\nbest_models, best_scores = optimize_models(final_models_dict, X, y, n_trials=1, n_jobs=1)  # Using n_jobs=4 for parallel execution\nprint(\"Best models:\", best_models)\nprint(\"Best scores:\", best_scores)\nto_fitModels = update_model_params(final_models_dict, best_models)\nfitted_models = fit_models(to_fitModels, X, y, save_path='/kaggle/working/')\nprint(\"Models fitted and saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-22T00:13:19.437500Z","iopub.execute_input":"2024-10-22T00:13:19.437825Z","iopub.status.idle":"2024-10-22T00:13:19.450770Z","shell.execute_reply.started":"2024-10-22T00:13:19.437770Z","shell.execute_reply":"2024-10-22T00:13:19.449663Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import joblib\nimport pandas as pd\n\ndef predict_with_models(models_dict, X_new, save_path='/kaggle/working/'):\n    predictions = {}\n    \n    for model_name in models_dict.keys():\n        try:\n            # Load the fitted model\n            model_filename = f\"{save_path}{model_name}.joblib\"\n            model = joblib.load(model_filename)\n            \n            # Make predictions\n            predictions[model_name] = model.predict(X_new)\n        except:\n            pass\n    \n    # Convert predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions)\n    \n    return predictions_df\npredictions_df = predict_with_models(final_models_dict, updated_df_test, save_path='/kaggle/working/')\npredictions_df.head(418)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-22T00:13:19.453050Z","iopub.execute_input":"2024-10-22T00:13:19.453582Z","iopub.status.idle":"2024-10-22T00:13:19.464740Z","shell.execute_reply.started":"2024-10-22T00:13:19.453453Z","shell.execute_reply":"2024-10-22T00:13:19.463620Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"estimators1 = [\n    ('LogisticRegression', LogisticRegression(max_iter=1000, solver='liblinear', penalty='l2')),\n    ('RandomForestClassifier', RandomForestClassifier(n_estimators=2, max_depth=3, min_samples_split=4, min_samples_leaf=3)),\n    ('SupportVectorClassifier', SVC(probability=True, C=1, kernel='linear', gamma='scale')),\n    ('KNeighborsClassifier', KNeighborsClassifier(n_neighbors=7, leaf_size=23, p=2)),\n    ('NaiveBayes', GaussianNB()),\n    ('DecisionTreeClassifier', DecisionTreeClassifier(max_depth=1, min_samples_split=2, min_samples_leaf=3)),\n    ('GradientBoostingClassifier', GradientBoostingClassifier(n_estimators=5, max_depth=2, learning_rate=0.01)),\n    ('XGBClassifier', XGBClassifier(use_label_encoder=False, eval_metric='logloss',n_estimators=7, max_depth=3,learning_rate=0.01)),\n    ('LGBMClassifier', LGBMClassifier(n_estimators=2, max_depth=4))\n]\n\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nX = train_df.drop(columns=['Survived'])\ny = train_df['Survived']\nVotingClass = VotingClassifier(estimators=estimators1, voting='hard')\nVotingClass.fit(X, y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-22T00:14:04.089376Z","iopub.execute_input":"2024-10-22T00:14:04.089742Z","iopub.status.idle":"2024-10-22T00:14:14.431549Z","shell.execute_reply.started":"2024-10-22T00:14:04.089689Z","shell.execute_reply":"2024-10-22T00:14:14.430522Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"VotingClassifier(estimators=[('LogisticRegression',\n                              LogisticRegression(C=1.0, class_weight=None,\n                                                 dual=False, fit_intercept=True,\n                                                 intercept_scaling=1,\n                                                 l1_ratio=None, max_iter=1000,\n                                                 multi_class='warn',\n                                                 n_jobs=None, penalty='l2',\n                                                 random_state=None,\n                                                 solver='liblinear', tol=0.0001,\n                                                 verbose=0, warm_start=False)),\n                             ('RandomForestClassifier',\n                              RandomForestClassifier(bootstra...\n                                             importance_type='split',\n                                             learning_rate=0.1, max_depth=4,\n                                             min_child_samples=20,\n                                             min_child_weight=0.001,\n                                             min_split_gain=0.0, n_estimators=2,\n                                             n_jobs=-1, num_leaves=31,\n                                             objective=None, random_state=None,\n                                             reg_alpha=0.0, reg_lambda=0.0,\n                                             silent=True, subsample=1.0,\n                                             subsample_for_bin=200000,\n                                             subsample_freq=0))],\n                 flatten_transform=True, n_jobs=None, voting='hard',\n                 weights=None)"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"predictions = VotingClass.predict(updated_df_test)\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-21T23:58:16.078502Z","iopub.execute_input":"2024-10-21T23:58:16.078899Z","iopub.status.idle":"2024-10-21T23:58:18.364997Z","shell.execute_reply.started":"2024-10-21T23:58:16.078828Z","shell.execute_reply":"2024-10-21T23:58:18.363523Z"}},"outputs":[{"name":"stdout","text":"Your submission was successfully saved!\n","output_type":"stream"}],"execution_count":27}]}