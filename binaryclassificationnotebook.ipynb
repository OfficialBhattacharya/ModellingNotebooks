{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":29507,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_kg_hide-input":false,"trusted":true,"execution":{"iopub.status.busy":"2024-10-22T11:03:56.501994Z","iopub.execute_input":"2024-10-22T11:03:56.502359Z","iopub.status.idle":"2024-10-22T11:03:56.792346Z","shell.execute_reply.started":"2024-10-22T11:03:56.502294Z","shell.execute_reply":"2024-10-22T11:03:56.791282Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/titanic/train.csv\n/kaggle/input/titanic/test.csv\n/kaggle/input/titanic/gender_submission.csv\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import LabelEncoder\nimport Levenshtein\nimport re\n\n\ndef intersection_of_lists(list1, list2):\n    return list(set(list1) & set(list2))\n\n\ndef difference_of_lists(list1, list2):\n    return [item for item in list1 if item not in list2]\n\n\ndef get_numeric_and_non_numeric_columns(df):\n    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n    non_numeric_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()\n    print(f\"Numeric columns: {numeric_cols}\")\n    print(f\"Non-numeric columns: {non_numeric_cols}\")\n    return numeric_cols, non_numeric_cols\n\n\ndef remove_single_unique_or_all_nans(df):\n    removed_columns = []\n    for column in df.columns:\n        if df[column].nunique() <= 1 or df[column].isna().all():\n            removed_columns.append(column)\n            df = df.drop(columns=[column])\n    print(f\"Removed columns due to all NaN or only 1 unique value: {removed_columns}\")\n    return df, removed_columns\n\n\ndef columns_with_missing_values(df):\n    missing_cols = [col for col in df.columns if df[col].isna().any()]\n    print(f\"Missing data columns: {missing_cols}\")\n    return missing_cols\n\n\ndef fill_missingNumeric_with_median(df, missing_cols, numeric_cols):\n    for col in intersection_of_lists(missing_cols, numeric_cols):\n        median_value = df[col].median()\n        df[col].fillna(median_value, inplace=True)\n    print(\"Done inputing missing numeric values with median!\")\n    return df\n\n\ndef columnsCategory_with_more_than_X_percent_unique(df, categoric_cols, perc):\n    total_rows = len(df)\n    threshold = total_rows * 0.01 * perc  # 10% of the total number of rows\n    cols_with_high_uniques = [col for col in categoric_cols if df[col].nunique() > threshold]\n    print(f\"Columns with high uniques: {cols_with_high_uniques}\")\n    return cols_with_high_uniques\n    \n\n\ndef convert_and_create_integer_columns(df, new_columns, mappings, colName):\n    df[colName] = df[colName].astype('object')\n    df[colName], unique_values = pd.factorize(df[colName])\n    # Add the new column name to the list\n    new_columns.append(colName)\n    # Create a mapping dictionary for the column\n    mappings[colName] = {value: i for i, value in enumerate(unique_values)}\n    return df, new_columns, mappings\n\n\ndef fill_missing_and_predict(df, new_columns, mappings, usable_cols, column_name):\n    # Convert and create integer column\n    df, new_columns, mappings = convert_and_create_integer_columns(df, new_columns, mappings, column_name)\n    # Train the model to predict missing values\n    non_missing_idx = df[column_name] != -1  # Using -1 for factorized NaNs\n    missing_idx = df[column_name] == -1\n    if missing_idx.sum() > 0:\n        X_train = df.loc[non_missing_idx, usable_cols]\n        y_train = df.loc[non_missing_idx, column_name]\n        X_test = df.loc[missing_idx,  usable_cols]\n        model = LogisticRegression(max_iter=1000, solver ='lbfgs',  multi_class='auto')\n        model.fit(X_train, y_train)\n        # Predict the missing values\n        predicted = model.predict(X_test)\n        # Replace the missing values with the predicted values\n        df.loc[missing_idx, column_name] = predicted\n    return df, new_columns, mappings\n    \n\ndef get_bigrams(string):\n    # Generate bigrams from a string\n    return [string[i:i+2] for i in range(len(string)-1)]\n\ndef sorensen_dice(a, b):\n    # SÃ¸rensen-Dice coefficient for two sets\n    a_bigrams = set(get_bigrams(a))\n    b_bigrams = set(get_bigrams(b))\n    overlap = len(a_bigrams & b_bigrams)\n    total = len(a_bigrams) + len(b_bigrams)\n    if total == 0:\n        return 1.0 if a == b else 0.0  # Handle identical empty strings\n    return 2 * overlap / total\n\n\ndef calculate_meanDistanceFromAList(input_string, string_list):\n    sum_Levenshtein = 0\n    sum_sorensen_dice = 0\n    for string in string_list:\n        sum_Levenshtein = sum_Levenshtein + Levenshtein.distance(input_string, string)\n        sum_sorensen_dice = sum_sorensen_dice + sorensen_dice(input_string, string)\n    return float(sum_Levenshtein/len(string_list)),float(sum_sorensen_dice/len(string_list))\n    \n\ndef takeOut_stringList(df, target, variableCol):\n    return list(df[df[f\"{target}\"]==1][f\"{variableCol}\"].unique()),list(df[df[f\"{target}\"]==0][f\"{variableCol}\"].unique())\n\n\ndef apply_meanDistance(df, column_name, string_list):\n    # Calculate mean distances for each row and add a new column\n    df[['mean_Levenshtein', 'mean_sorensen_dice']] = df[column_name].apply(\n        lambda x: pd.Series(calculate_meanDistanceFromAList(x, string_list))\n    )\n    return df\n\n\ndef create_DistanceMetric(df, new_columns, usable_cols, colName, target, orig_data):\n    df[colName] = df[colName].astype('str')\n    true_NameList, false_NameList = takeOut_stringList(orig_data, target, colName)\n    new_columns.append(colName)\n    colName_true_lev = str(colName+\"_true_lev\")\n    colName_true_reg = str(colName+\"_true_reg\")\n    df[[colName_true_lev, colName_true_reg]] = df[colName].apply(\n        lambda x: pd.Series(calculate_meanDistanceFromAList(x, true_NameList))\n    )\n    colName_false_lev = str(colName+\"_false_lev\")\n    colName_false_reg = str(colName+\"_false_reg\")\n    df[[colName_false_lev, colName_false_reg]] = df[colName].apply(\n        lambda x: pd.Series(calculate_meanDistanceFromAList(x, false_NameList))\n    )\n    usable_cols = usable_cols + [colName_true_lev, colName_true_reg, colName_false_lev, colName_false_reg]\n    return df, new_columns, usable_cols\n\n\ndef convert_All_integer_columns(df, numeric_cols, missing_cols, categoric_cols, cols_with_high_uniques, target, orig_data):\n    new_columns = []\n    mappings = {}\n    usable_cols = numeric_cols \n    \n    categoric_nonNA_cols = difference_of_lists(categoric_cols, missing_cols)\n\n    categoric_nonNA_Few_cols = difference_of_lists(categoric_nonNA_cols, cols_with_high_uniques)\n    \n    categoric_nonNA_Multiple_cols = difference_of_lists(categoric_nonNA_cols, categoric_nonNA_Few_cols)\n    \n    categoric_NA_Few_cols = difference_of_lists(missing_cols, cols_with_high_uniques)\n\n    categoric_NA_Multiple_cols = difference_of_lists(missing_cols, categoric_NA_Few_cols)\n    \n    for col in categoric_nonNA_Few_cols:\n        df, new_columns, mappings = convert_and_create_integer_columns(df, new_columns, mappings, col)\n        print(f\"[No NA values][Less Unique Values] Categoric columns Converted to Integer: {col}\")\n    usable_cols = usable_cols + categoric_nonNA_Few_cols\n    for col in  categoric_NA_Few_cols:   \n        df, new_columns, mappings = fill_missing_and_predict(df, new_columns, mappings, usable_cols, col)\n        print(f\"[NA values][Less Unique Values] Categoric columns Converted to Integer and Missing Are Predicted: {col}\")\n        usable_cols = usable_cols + [col]    \n    for col in categoric_nonNA_Multiple_cols:   \n        df, new_columns, usable_cols = create_DistanceMetric(df, new_columns, usable_cols, col, target, orig_data)\n        df = df.drop(columns=[col])\n        print(f\"[No NA values][Multiple Unique Values] Categoric columns Converted to Distance Based On Cluster: {col}\")\n    for col in  categoric_NA_Multiple_cols:\n        df = df.drop(columns=[col])\n        print(f\"[NA values][Multiple Unique Values] Categoric columns Converted to Distance Based On Cluster and Missing Are Predicted: {col}\")  \n    print(f\"Mappings: {mappings}\")\n    return df, new_columns, mappings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-22T11:09:28.724086Z","iopub.execute_input":"2024-10-22T11:09:28.724485Z","iopub.status.idle":"2024-10-22T11:09:28.783797Z","shell.execute_reply.started":"2024-10-22T11:09:28.724434Z","shell.execute_reply":"2024-10-22T11:09:28.782417Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class tangiD_BinaryClassification:\n    '''Takes in the data , target and features'''\n    def __init__(self, data, train, target, type, features):\n        self.data = data\n        self.target = target\n        self.type = type\n        if(self.type!=\"TestData!\"):\n            self.origData = self.data.copy()\n        else:\n            self.origData = train\n        self.allFeatures = features\n    \n    def medianIntifying(self, highUniq = 10):\n        self.cleanDF, self.removed_columns = remove_single_unique_or_all_nans(self.data[self.allFeatures].copy())\n        self.numeric_cols, self.non_numeric_cols = get_numeric_and_non_numeric_columns(self.cleanDF)\n        self.missing_cols = columns_with_missing_values(self.cleanDF)\n        self.filledNumeric_df = fill_missingNumeric_with_median(self.cleanDF, self.missing_cols, self.numeric_cols)\n        self.missing_cols = columns_with_missing_values(self.filledNumeric_df)\n        self.high_uniques = columnsCategory_with_more_than_X_percent_unique(self.filledNumeric_df, self.non_numeric_cols, highUniq)\n        self.updated_df, self.new_columns, self.mappings = convert_All_integer_columns(self.filledNumeric_df, self.numeric_cols, self.missing_cols, self.non_numeric_cols, self.high_uniques, self.target, self.origData)\n        if(self.type!=\"TestData!\"):\n            self.updated_df = pd.concat([self.updated_df, self.data[self.target]], axis=1)\n            return self.updated_df\n        else:\n            return self.updated_df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-22T11:43:35.755822Z","iopub.execute_input":"2024-10-22T11:43:35.756238Z","iopub.status.idle":"2024-10-22T11:43:35.771866Z","shell.execute_reply.started":"2024-10-22T11:43:35.756171Z","shell.execute_reply":"2024-10-22T11:43:35.770953Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntrainClass = tangiD_BinaryClassification(train_data, None, \"Survived\", \"TrainData!\", ['Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch','Ticket', 'Fare', 'Cabin', 'Embarked'])\ntrainData = trainClass.medianIntifying(10)\ntrainData","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-22T11:43:54.309308Z","iopub.execute_input":"2024-10-22T11:43:54.309735Z","iopub.status.idle":"2024-10-22T11:44:19.971985Z","shell.execute_reply.started":"2024-10-22T11:43:54.309658Z","shell.execute_reply":"2024-10-22T11:44:19.970820Z"}},"outputs":[{"name":"stdout","text":"Removed columns due to all NaN or only 1 unique value: []\nNumeric columns: ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\nNon-numeric columns: ['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked']\nMissing data columns: ['Age', 'Cabin', 'Embarked']\nDone inputing missing numeric values with median!\nMissing data columns: ['Cabin', 'Embarked']\nColumns with high uniques: ['Name', 'Ticket', 'Cabin']\n[No NA values][Less Unique Values] Categoric columns Converted to Integer: Sex\n[NA values][Less Unique Values] Categoric columns Converted to Integer and Missing Are Predicted: Embarked\n[No NA values][Multiple Unique Values] Categoric columns Converted to Distance Based On Cluster: Name\n[No NA values][Multiple Unique Values] Categoric columns Converted to Distance Based On Cluster: Ticket\n[NA values][Multiple Unique Values] Categoric columns Converted to Distance Based On Cluster and Missing Are Predicted: Cabin\nMappings: {'Sex': {'male': 0, 'female': 1}, 'Embarked': {'S': 0, 'C': 1, 'Q': 2}}\n","output_type":"stream"},{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"     Pclass  Sex   Age  SibSp  Parch     Fare  Embarked  Name_true_lev  \\\n0         3    0  22.0      1      0   7.2500         0      23.330409   \n1         1    1  38.0      1      0  71.2833         1      39.064327   \n2         3    1  26.0      0      0   7.9250         0      23.897661   \n3         1    1  35.0      1      0  53.1000         0      33.649123   \n4         3    0  35.0      0      0   8.0500         0      23.242690   \n..      ...  ...   ...    ...    ...      ...       ...            ...   \n886       2    0  27.0      0      0  13.0000         0      25.824561   \n887       1    1  19.0      0      0  30.0000         0      24.210526   \n888       3    1  28.0      1      2  23.4500         0      30.067251   \n889       1    0  26.0      0      0  30.0000         1      23.023392   \n890       3    0  32.0      0      0   7.7500         2      23.175439   \n\n     Name_true_reg  Name_false_lev  Name_false_reg  Ticket_true_lev  \\\n0         0.240875       18.391621        0.280404         8.076923   \n1         0.222573       39.794171        0.199117         6.919231   \n2         0.268535       20.091075        0.222354        13.261538   \n3         0.225927       33.755920        0.199984         5.938462   \n4         0.256739       18.566485        0.311537         6.100000   \n..             ...             ...             ...              ...   \n886       0.129386       20.513661        0.148259         5.811538   \n887       0.275001       22.284153        0.211829         5.846154   \n888       0.288539       30.038251        0.231516         9.200000   \n889       0.228507       18.191257        0.271455         5.961538   \n890       0.216775       17.273224        0.278064         5.938462   \n\n     Ticket_true_reg  Ticket_false_lev  Ticket_false_reg  Survived  \n0           0.061181          8.010638          0.059223         0  \n1           0.101571          7.493617          0.049166         1  \n2           0.064463         13.236170          0.071896         1  \n3           0.078559          6.265957          0.055481         1  \n4           0.059899          6.093617          0.097651         0  \n..               ...               ...               ...       ...  \n886         0.061989          6.168085          0.067192         0  \n887         0.060876          6.104255          0.057290         1  \n888         0.032052          9.157447          0.038314         0  \n889         0.093097          6.304255          0.077616         1  \n890         0.051723          5.942553          0.053340         0  \n\n[891 rows x 16 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pclass</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Fare</th>\n      <th>Embarked</th>\n      <th>Name_true_lev</th>\n      <th>Name_true_reg</th>\n      <th>Name_false_lev</th>\n      <th>Name_false_reg</th>\n      <th>Ticket_true_lev</th>\n      <th>Ticket_true_reg</th>\n      <th>Ticket_false_lev</th>\n      <th>Ticket_false_reg</th>\n      <th>Survived</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>7.2500</td>\n      <td>0</td>\n      <td>23.330409</td>\n      <td>0.240875</td>\n      <td>18.391621</td>\n      <td>0.280404</td>\n      <td>8.076923</td>\n      <td>0.061181</td>\n      <td>8.010638</td>\n      <td>0.059223</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>71.2833</td>\n      <td>1</td>\n      <td>39.064327</td>\n      <td>0.222573</td>\n      <td>39.794171</td>\n      <td>0.199117</td>\n      <td>6.919231</td>\n      <td>0.101571</td>\n      <td>7.493617</td>\n      <td>0.049166</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>3</td>\n      <td>1</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.9250</td>\n      <td>0</td>\n      <td>23.897661</td>\n      <td>0.268535</td>\n      <td>20.091075</td>\n      <td>0.222354</td>\n      <td>13.261538</td>\n      <td>0.064463</td>\n      <td>13.236170</td>\n      <td>0.071896</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>53.1000</td>\n      <td>0</td>\n      <td>33.649123</td>\n      <td>0.225927</td>\n      <td>33.755920</td>\n      <td>0.199984</td>\n      <td>5.938462</td>\n      <td>0.078559</td>\n      <td>6.265957</td>\n      <td>0.055481</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>3</td>\n      <td>0</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8.0500</td>\n      <td>0</td>\n      <td>23.242690</td>\n      <td>0.256739</td>\n      <td>18.566485</td>\n      <td>0.311537</td>\n      <td>6.100000</td>\n      <td>0.059899</td>\n      <td>6.093617</td>\n      <td>0.097651</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <td>886</td>\n      <td>2</td>\n      <td>0</td>\n      <td>27.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>13.0000</td>\n      <td>0</td>\n      <td>25.824561</td>\n      <td>0.129386</td>\n      <td>20.513661</td>\n      <td>0.148259</td>\n      <td>5.811538</td>\n      <td>0.061989</td>\n      <td>6.168085</td>\n      <td>0.067192</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>887</td>\n      <td>1</td>\n      <td>1</td>\n      <td>19.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>30.0000</td>\n      <td>0</td>\n      <td>24.210526</td>\n      <td>0.275001</td>\n      <td>22.284153</td>\n      <td>0.211829</td>\n      <td>5.846154</td>\n      <td>0.060876</td>\n      <td>6.104255</td>\n      <td>0.057290</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>888</td>\n      <td>3</td>\n      <td>1</td>\n      <td>28.0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>23.4500</td>\n      <td>0</td>\n      <td>30.067251</td>\n      <td>0.288539</td>\n      <td>30.038251</td>\n      <td>0.231516</td>\n      <td>9.200000</td>\n      <td>0.032052</td>\n      <td>9.157447</td>\n      <td>0.038314</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>889</td>\n      <td>1</td>\n      <td>0</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>30.0000</td>\n      <td>1</td>\n      <td>23.023392</td>\n      <td>0.228507</td>\n      <td>18.191257</td>\n      <td>0.271455</td>\n      <td>5.961538</td>\n      <td>0.093097</td>\n      <td>6.304255</td>\n      <td>0.077616</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>3</td>\n      <td>0</td>\n      <td>32.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.7500</td>\n      <td>2</td>\n      <td>23.175439</td>\n      <td>0.216775</td>\n      <td>17.273224</td>\n      <td>0.278064</td>\n      <td>5.938462</td>\n      <td>0.051723</td>\n      <td>5.942553</td>\n      <td>0.053340</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>891 rows Ã 16 columns</p>\n</div>"},"metadata":{}}],"execution_count":40},{"cell_type":"code","source":"test_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ntestClass = tangiD_BinaryClassification(test_data, train_data, \"Survived\", \"TestData!\", ['Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch','Ticket', 'Fare', 'Cabin', 'Embarked'])\ntestData = testClass.medianIntifying(10)\ntestData","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-22T11:44:45.788248Z","iopub.execute_input":"2024-10-22T11:44:45.788652Z","iopub.status.idle":"2024-10-22T11:44:57.770441Z","shell.execute_reply.started":"2024-10-22T11:44:45.788599Z","shell.execute_reply":"2024-10-22T11:44:57.769130Z"}},"outputs":[{"name":"stdout","text":"Removed columns due to all NaN or only 1 unique value: []\nNumeric columns: ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\nNon-numeric columns: ['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked']\nMissing data columns: ['Age', 'Fare', 'Cabin']\nDone inputing missing numeric values with median!\nMissing data columns: ['Cabin']\nColumns with high uniques: ['Name', 'Ticket', 'Cabin']\n[No NA values][Less Unique Values] Categoric columns Converted to Integer: Sex\n[No NA values][Less Unique Values] Categoric columns Converted to Integer: Embarked\n[No NA values][Multiple Unique Values] Categoric columns Converted to Distance Based On Cluster: Name\n[No NA values][Multiple Unique Values] Categoric columns Converted to Distance Based On Cluster: Ticket\n[NA values][Multiple Unique Values] Categoric columns Converted to Distance Based On Cluster and Missing Are Predicted: Cabin\nMappings: {'Sex': {'male': 0, 'female': 1}, 'Embarked': {'Q': 0, 'S': 1, 'C': 2}}\n","output_type":"stream"},{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"     Pclass  Sex   Age  SibSp  Parch      Fare  Embarked  Name_true_lev  \\\n0         3    0  34.5      0      0    7.8292         0      22.912281   \n1         3    1  47.0      1      0    7.0000         1      25.918129   \n2         2    0  62.0      0      0    9.6875         0      24.078947   \n3         3    0  27.0      0      0    8.6625         1      22.967836   \n4         3    1  22.0      1      1   12.2875         1      33.301170   \n..      ...  ...   ...    ...    ...       ...       ...            ...   \n413       3    0  27.0      0      0    8.0500         1      24.210526   \n414       1    1  39.0      0      0  108.9000         2      27.643275   \n415       3    0  38.5      0      0    7.2500         1      24.786550   \n416       3    0  27.0      0      0    8.0500         1      22.692982   \n417       3    0  27.0      1      1   22.3583         2      23.941520   \n\n     Name_true_reg  Name_false_lev  Name_false_reg  Ticket_true_lev  \\\n0         0.238211       16.633880        0.306957         6.153846   \n1         0.239974       24.313297        0.229048         5.996154   \n2         0.230024       19.597450        0.270204         5.926923   \n3         0.236747       17.063752        0.297216         5.869231   \n4         0.255400       33.191257        0.222822         6.250000   \n..             ...             ...             ...              ...   \n413       0.207600       17.950820        0.269737         8.253846   \n414       0.157274       24.136612        0.154155         6.823077   \n415       0.229698       20.810565        0.269645        15.423077   \n416       0.253124       17.306011        0.310065         6.180769   \n417       0.253106       20.278689        0.268949         5.769231   \n\n     Ticket_true_reg  Ticket_false_lev  Ticket_false_reg  \n0           0.068473          6.159574          0.047670  \n1           0.039048          5.887234          0.045391  \n2           0.035986          5.951064          0.039179  \n3           0.034021          5.912766          0.040689  \n4           0.065359          6.374468          0.064780  \n..               ...               ...               ...  \n413         0.049999          8.093617          0.064082  \n414         0.107360          7.487234          0.051602  \n415         0.062341         15.153191          0.076489  \n416         0.044993          5.985106          0.038481  \n417         0.051263          6.040426          0.046729  \n\n[418 rows x 15 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pclass</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Fare</th>\n      <th>Embarked</th>\n      <th>Name_true_lev</th>\n      <th>Name_true_reg</th>\n      <th>Name_false_lev</th>\n      <th>Name_false_reg</th>\n      <th>Ticket_true_lev</th>\n      <th>Ticket_true_reg</th>\n      <th>Ticket_false_lev</th>\n      <th>Ticket_false_reg</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>34.5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.8292</td>\n      <td>0</td>\n      <td>22.912281</td>\n      <td>0.238211</td>\n      <td>16.633880</td>\n      <td>0.306957</td>\n      <td>6.153846</td>\n      <td>0.068473</td>\n      <td>6.159574</td>\n      <td>0.047670</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>47.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>7.0000</td>\n      <td>1</td>\n      <td>25.918129</td>\n      <td>0.239974</td>\n      <td>24.313297</td>\n      <td>0.229048</td>\n      <td>5.996154</td>\n      <td>0.039048</td>\n      <td>5.887234</td>\n      <td>0.045391</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>62.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>9.6875</td>\n      <td>0</td>\n      <td>24.078947</td>\n      <td>0.230024</td>\n      <td>19.597450</td>\n      <td>0.270204</td>\n      <td>5.926923</td>\n      <td>0.035986</td>\n      <td>5.951064</td>\n      <td>0.039179</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>3</td>\n      <td>0</td>\n      <td>27.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8.6625</td>\n      <td>1</td>\n      <td>22.967836</td>\n      <td>0.236747</td>\n      <td>17.063752</td>\n      <td>0.297216</td>\n      <td>5.869231</td>\n      <td>0.034021</td>\n      <td>5.912766</td>\n      <td>0.040689</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>3</td>\n      <td>1</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>12.2875</td>\n      <td>1</td>\n      <td>33.301170</td>\n      <td>0.255400</td>\n      <td>33.191257</td>\n      <td>0.222822</td>\n      <td>6.250000</td>\n      <td>0.065359</td>\n      <td>6.374468</td>\n      <td>0.064780</td>\n    </tr>\n    <tr>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <td>413</td>\n      <td>3</td>\n      <td>0</td>\n      <td>27.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8.0500</td>\n      <td>1</td>\n      <td>24.210526</td>\n      <td>0.207600</td>\n      <td>17.950820</td>\n      <td>0.269737</td>\n      <td>8.253846</td>\n      <td>0.049999</td>\n      <td>8.093617</td>\n      <td>0.064082</td>\n    </tr>\n    <tr>\n      <td>414</td>\n      <td>1</td>\n      <td>1</td>\n      <td>39.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>108.9000</td>\n      <td>2</td>\n      <td>27.643275</td>\n      <td>0.157274</td>\n      <td>24.136612</td>\n      <td>0.154155</td>\n      <td>6.823077</td>\n      <td>0.107360</td>\n      <td>7.487234</td>\n      <td>0.051602</td>\n    </tr>\n    <tr>\n      <td>415</td>\n      <td>3</td>\n      <td>0</td>\n      <td>38.5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.2500</td>\n      <td>1</td>\n      <td>24.786550</td>\n      <td>0.229698</td>\n      <td>20.810565</td>\n      <td>0.269645</td>\n      <td>15.423077</td>\n      <td>0.062341</td>\n      <td>15.153191</td>\n      <td>0.076489</td>\n    </tr>\n    <tr>\n      <td>416</td>\n      <td>3</td>\n      <td>0</td>\n      <td>27.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8.0500</td>\n      <td>1</td>\n      <td>22.692982</td>\n      <td>0.253124</td>\n      <td>17.306011</td>\n      <td>0.310065</td>\n      <td>6.180769</td>\n      <td>0.044993</td>\n      <td>5.985106</td>\n      <td>0.038481</td>\n    </tr>\n    <tr>\n      <td>417</td>\n      <td>3</td>\n      <td>0</td>\n      <td>27.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>22.3583</td>\n      <td>2</td>\n      <td>23.941520</td>\n      <td>0.253106</td>\n      <td>20.278689</td>\n      <td>0.268949</td>\n      <td>5.769231</td>\n      <td>0.051263</td>\n      <td>6.040426</td>\n      <td>0.046729</td>\n    </tr>\n  </tbody>\n</table>\n<p>418 rows Ã 15 columns</p>\n</div>"},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntrain_data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-22T00:12:44.270448Z","iopub.execute_input":"2024-10-22T00:12:44.271030Z","iopub.status.idle":"2024-10-22T00:12:44.309366Z","shell.execute_reply.started":"2024-10-22T00:12:44.270955Z","shell.execute_reply":"2024-10-22T00:12:44.308248Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"   PassengerId  Survived  Pclass  \\\n0            1         0       3   \n1            2         1       1   \n2            3         1       3   \n3            4         1       1   \n4            5         0       3   \n\n                                                Name     Sex   Age  SibSp  \\\n0                            Braund, Mr. Owen Harris    male  22.0      1   \n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n2                             Heikkinen, Miss. Laina  female  26.0      0   \n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n4                           Allen, Mr. William Henry    male  35.0      0   \n\n   Parch            Ticket     Fare Cabin Embarked  \n0      0         A/5 21171   7.2500   NaN        S  \n1      0          PC 17599  71.2833   C85        C  \n2      0  STON/O2. 3101282   7.9250   NaN        S  \n3      0            113803  53.1000  C123        S  \n4      0            373450   8.0500   NaN        S  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Braund, Mr. Owen Harris</td>\n      <td>male</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>A/5 21171</td>\n      <td>7.2500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n      <td>female</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PC 17599</td>\n      <td>71.2833</td>\n      <td>C85</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>Heikkinen, Miss. Laina</td>\n      <td>female</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>STON/O2. 3101282</td>\n      <td>7.9250</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n      <td>female</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>113803</td>\n      <td>53.1000</td>\n      <td>C123</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>5</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Allen, Mr. William Henry</td>\n      <td>male</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>373450</td>\n      <td>8.0500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"test_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ntest_data.head()\ntest_data.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-22T00:12:44.311034Z","iopub.execute_input":"2024-10-22T00:12:44.311443Z","iopub.status.idle":"2024-10-22T00:12:44.328175Z","shell.execute_reply.started":"2024-10-22T00:12:44.311362Z","shell.execute_reply":"2024-10-22T00:12:44.326532Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"Index(['PassengerId', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch',\n       'Ticket', 'Fare', 'Cabin', 'Embarked'],\n      dtype='object')"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\n\n\n\ndef intersection_of_lists(list1, list2):\n    return list(set(list1) & set(list2))\n\n\ndef difference_of_lists(list1, list2):\n    return [item for item in list1 if item not in list2]\n\n\ndef get_numeric_and_non_numeric_columns(df):\n    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n    non_numeric_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()\n    print(f\"Numeric columns: {numeric_cols}\")\n    print(f\"Non-numeric columns: {non_numeric_cols}\")\n    return numeric_cols, non_numeric_cols\n\n\ndef remove_single_unique_or_all_nans(df):\n    removed_columns = []\n    for column in df.columns:\n        if df[column].nunique() <= 1 or df[column].isna().all():\n            removed_columns.append(column)\n            df = df.drop(columns=[column])\n    print(f\"Removed columns due to all NaN or only 1 unique value: {removed_columns}\")\n    return df, removed_columns\n\n\ndef columns_with_missing_values(df):\n    missing_cols = [col for col in df.columns if df[col].isna().any()]\n    print(f\"Missing data columns: {missing_cols}\")\n    return missing_cols\n\n\ndef fill_missingNumeric_with_median(df, missing_cols, numeric_cols):\n    for col in intersection_of_lists(missing_cols, numeric_cols):\n        median_value = df[col].median()\n        df[col].fillna(median_value, inplace=True)\n    print(\"Done inputing missing numeric values with median!\")\n    return df\n\n\ndef columnsCategory_with_more_than_X_percent_unique(df, categoric_cols, perc):\n    total_rows = len(df)\n    threshold = total_rows * 0.01 * perc  # 10% of the total number of rows\n    cols_with_high_uniques = [col for col in categoric_cols if df[col].nunique() > threshold]\n    print(f\"Columns with high uniques: {cols_with_high_uniques}\")\n    return cols_with_high_uniques\n    \n\n\ndef convert_and_create_integer_columns(df, new_columns, mappings, colName):\n    df[colName] = df[colName].astype('object')\n    df[colName], unique_values = pd.factorize(df[colName])\n    # Add the new column name to the list\n    new_columns.append(colName)\n    # Create a mapping dictionary for the column\n    mappings[colName] = {value: i for i, value in enumerate(unique_values)}\n    return df, new_columns, mappings\n\n\ndef fill_missing_and_predict(df, new_columns, mappings, usable_cols, column_name):\n    # Convert and create integer column\n    df, new_columns, mappings = convert_and_create_integer_columns(df, new_columns, mappings, column_name)\n    # Train the model to predict missing values\n    non_missing_idx = df[column_name] != -1  # Using -1 for factorized NaNs\n    missing_idx = df[column_name] == -1\n    if missing_idx.sum() > 0:\n        X_train = df.loc[non_missing_idx, usable_cols]\n        y_train = df.loc[non_missing_idx, column_name]\n        X_test = df.loc[missing_idx,  usable_cols]\n        model = LogisticRegression(max_iter=1000, solver ='lbfgs',  multi_class='auto')\n        model.fit(X_train, y_train)\n        # Predict the missing values\n        predicted = model.predict(X_test)\n        # Replace the missing values with the predicted values\n        df.loc[missing_idx, column_name] = predicted\n    return df, new_columns, mappings\n    \n\ndef get_bigrams(string):\n    # Generate bigrams from a string\n    return [string[i:i+2] for i in range(len(string)-1)]\n\ndef sorensen_dice(a, b):\n    # SÃ¸rensen-Dice coefficient for two sets\n    a_bigrams = set(get_bigrams(a))\n    b_bigrams = set(get_bigrams(b))\n    overlap = len(a_bigrams & b_bigrams)\n    total = len(a_bigrams) + len(b_bigrams)\n    if total == 0:\n        return 1.0 if a == b else 0.0  # Handle identical empty strings\n    return 2 * overlap / total\n\n\ndef calculate_meanDistanceFromAList(input_string, string_list):\n    sum_Levenshtein = 0\n    sum_sorensen_dice = 0\n    for string in string_list:\n        sum_Levenshtein = sum_Levenshtein + Levenshtein.distance(input_string, string)\n        sum_sorensen_dice = sum_sorensen_dice + sorensen_dice(input_string, string)\n    return float(sum_Levenshtein/len(string_list)),float(sum_sorensen_dice/len(string_list))\n    \n\ndef takeOut_stringList(df, target, variableCol):\n    return list(df[df[f\"{target}\"]==1][f\"{variableCol}\"].unique()),list(df[df[f\"{target}\"]==0][f\"{variableCol}\"].unique())\n\n\ndef apply_meanDistance(df, column_name, string_list):\n    # Calculate mean distances for each row and add a new column\n    df[['mean_Levenshtein', 'mean_sorensen_dice']] = df[column_name].apply(\n        lambda x: pd.Series(calculate_meanDistanceFromAList(x, string_list))\n    )\n    return df\n\n\ndef create_DistanceMetric(df, new_columns, usable_cols, colName, target, orig_data):\n    df[colName] = df[colName].astype('str')\n    true_NameList, false_NameList = takeOut_stringList(orig_data, target, colName)\n    new_columns.append(colName)\n    colName_true_lev = str(colName+\"_true_lev\")\n    colName_true_reg = str(colName+\"_true_reg\")\n    df[[colName_true_lev, colName_true_reg]] = df[colName].apply(\n        lambda x: pd.Series(calculate_meanDistanceFromAList(x, true_NameList))\n    )\n    colName_false_lev = str(colName+\"_false_lev\")\n    colName_false_reg = str(colName+\"_false_reg\")\n    df[[colName_false_lev, colName_false_reg]] = df[colName].apply(\n        lambda x: pd.Series(calculate_meanDistanceFromAList(x, false_NameList))\n    )\n    usable_cols = usable_cols + [colName_true_lev, colName_true_reg, colName_false_lev, colName_false_reg]\n    return df, new_columns, usable_cols\n\n\ndef convert_All_integer_columns(df, numeric_cols, missing_cols, categoric_cols, cols_with_high_uniques, target, orig_data):\n    new_columns = []\n    mappings = {}\n    usable_cols = numeric_cols \n    \n    categoric_nonNA_cols = difference_of_lists(categoric_cols, missing_cols)\n\n    categoric_nonNA_Few_cols = difference_of_lists(categoric_nonNA_cols, cols_with_high_uniques)\n    \n    categoric_nonNA_Multiple_cols = difference_of_lists(categoric_nonNA_cols, categoric_nonNA_Few_cols)\n    \n    categoric_NA_Few_cols = difference_of_lists(missing_cols, cols_with_high_uniques)\n\n    categoric_NA_Multiple_cols = difference_of_lists(missing_cols, categoric_NA_Few_cols)\n    \n    for col in categoric_nonNA_Few_cols:\n        df, new_columns, mappings = convert_and_create_integer_columns(df, new_columns, mappings, col)\n        print(f\"[No NA values][Less Unique Values] Categoric columns Converted to Integer: {col}\")\n    usable_cols = usable_cols + categoric_nonNA_Few_cols\n    for col in  categoric_NA_Few_cols:   \n        df, new_columns, mappings = fill_missing_and_predict(df, new_columns, mappings, usable_cols, col)\n        print(f\"[NA values][Less Unique Values] Categoric columns Converted to Integer and Missing Are Predicted: {col}\")\n        usable_cols = usable_cols + [col]    \n    for col in categoric_nonNA_Multiple_cols:   \n        df, new_columns, usable_cols = create_DistanceMetric(df, new_columns, usable_cols, col, target, orig_data)\n        df = df.drop(columns=[col])\n        print(f\"[No NA values][Multiple Unique Values] Categoric columns Converted to Distance Based On Cluster: {col}\")\n    for col in  categoric_NA_Multiple_cols:\n        df = df.drop(columns=[col])\n        print(f\"[NA values][Multiple Unique Values] Categoric columns Converted to Distance Based On Cluster and Missing Are Predicted: {col}\")  \n    print(f\"Mappings: {mappings}\")\n    return df, new_columns, mappings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-22T00:12:44.330310Z","iopub.execute_input":"2024-10-22T00:12:44.330707Z","iopub.status.idle":"2024-10-22T00:12:45.356110Z","shell.execute_reply.started":"2024-10-22T00:12:44.330639Z","shell.execute_reply":"2024-10-22T00:12:45.354697Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"df_cleaned, removed_columns = remove_single_unique_or_all_nans(train_data[['Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']].copy())\nnumeric_cols, non_numeric_cols = get_numeric_and_non_numeric_columns(df_cleaned)\nmissing_cols = columns_with_missing_values(df_cleaned)\nfilledNumeric_df = fill_missingNumeric_with_median(df_cleaned, missing_cols, numeric_cols)\nmissing_cols = columns_with_missing_values(filledNumeric_df)\nhigh_uniques = columnsCategory_with_more_than_X_percent_unique(filledNumeric_df, non_numeric_cols, 10)\nupdated_df_train, new_columns_train, mappings_train = convert_All_integer_columns(filledNumeric_df, numeric_cols, missing_cols, non_numeric_cols, high_uniques, 'Survived', train_data)\ntrain_df = pd.concat([updated_df_train, train_data['Survived']], axis=1)\ntrain_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-22T00:12:45.361000Z","iopub.execute_input":"2024-10-22T00:12:45.361577Z","iopub.status.idle":"2024-10-22T00:13:07.667364Z","shell.execute_reply.started":"2024-10-22T00:12:45.361483Z","shell.execute_reply":"2024-10-22T00:13:07.666181Z"}},"outputs":[{"name":"stdout","text":"Removed columns due to all NaN or only 1 unique value: []\nNumeric columns: ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\nNon-numeric columns: ['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked']\nMissing data columns: ['Age', 'Cabin', 'Embarked']\nDone inputing missing numeric values with median!\nMissing data columns: ['Cabin', 'Embarked']\nColumns with high uniques: ['Name', 'Ticket', 'Cabin']\n[No NA values][Less Unique Values] Categoric columns Converted to Integer: Sex\n[NA values][Less Unique Values] Categoric columns Converted to Integer and Missing Are Predicted: Embarked\n[No NA values][Multiple Unique Values] Categoric columns Converted to Distance Based On Cluster: Name\n[No NA values][Multiple Unique Values] Categoric columns Converted to Distance Based On Cluster: Ticket\n[NA values][Multiple Unique Values] Categoric columns Converted to Distance Based On Cluster and Missing Are Predicted: Cabin\nMappings: {'Sex': {'male': 0, 'female': 1}, 'Embarked': {'S': 0, 'C': 1, 'Q': 2}}\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"     Pclass  Sex   Age  SibSp  Parch     Fare  Embarked  Name_true_lev  \\\n0         3    0  22.0      1      0   7.2500         0      23.330409   \n1         1    1  38.0      1      0  71.2833         1      39.064327   \n2         3    1  26.0      0      0   7.9250         0      23.897661   \n3         1    1  35.0      1      0  53.1000         0      33.649123   \n4         3    0  35.0      0      0   8.0500         0      23.242690   \n..      ...  ...   ...    ...    ...      ...       ...            ...   \n886       2    0  27.0      0      0  13.0000         0      25.824561   \n887       1    1  19.0      0      0  30.0000         0      24.210526   \n888       3    1  28.0      1      2  23.4500         0      30.067251   \n889       1    0  26.0      0      0  30.0000         1      23.023392   \n890       3    0  32.0      0      0   7.7500         2      23.175439   \n\n     Name_true_reg  Name_false_lev  Name_false_reg  Ticket_true_lev  \\\n0         0.240875       18.391621        0.280404         8.076923   \n1         0.222573       39.794171        0.199117         6.919231   \n2         0.268535       20.091075        0.222354        13.261538   \n3         0.225927       33.755920        0.199984         5.938462   \n4         0.256739       18.566485        0.311537         6.100000   \n..             ...             ...             ...              ...   \n886       0.129386       20.513661        0.148259         5.811538   \n887       0.275001       22.284153        0.211829         5.846154   \n888       0.288539       30.038251        0.231516         9.200000   \n889       0.228507       18.191257        0.271455         5.961538   \n890       0.216775       17.273224        0.278064         5.938462   \n\n     Ticket_true_reg  Ticket_false_lev  Ticket_false_reg  Survived  \n0           0.061181          8.010638          0.059223         0  \n1           0.101571          7.493617          0.049166         1  \n2           0.064463         13.236170          0.071896         1  \n3           0.078559          6.265957          0.055481         1  \n4           0.059899          6.093617          0.097651         0  \n..               ...               ...               ...       ...  \n886         0.061989          6.168085          0.067192         0  \n887         0.060876          6.104255          0.057290         1  \n888         0.032052          9.157447          0.038314         0  \n889         0.093097          6.304255          0.077616         1  \n890         0.051723          5.942553          0.053340         0  \n\n[891 rows x 16 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pclass</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Fare</th>\n      <th>Embarked</th>\n      <th>Name_true_lev</th>\n      <th>Name_true_reg</th>\n      <th>Name_false_lev</th>\n      <th>Name_false_reg</th>\n      <th>Ticket_true_lev</th>\n      <th>Ticket_true_reg</th>\n      <th>Ticket_false_lev</th>\n      <th>Ticket_false_reg</th>\n      <th>Survived</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>7.2500</td>\n      <td>0</td>\n      <td>23.330409</td>\n      <td>0.240875</td>\n      <td>18.391621</td>\n      <td>0.280404</td>\n      <td>8.076923</td>\n      <td>0.061181</td>\n      <td>8.010638</td>\n      <td>0.059223</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>71.2833</td>\n      <td>1</td>\n      <td>39.064327</td>\n      <td>0.222573</td>\n      <td>39.794171</td>\n      <td>0.199117</td>\n      <td>6.919231</td>\n      <td>0.101571</td>\n      <td>7.493617</td>\n      <td>0.049166</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>3</td>\n      <td>1</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.9250</td>\n      <td>0</td>\n      <td>23.897661</td>\n      <td>0.268535</td>\n      <td>20.091075</td>\n      <td>0.222354</td>\n      <td>13.261538</td>\n      <td>0.064463</td>\n      <td>13.236170</td>\n      <td>0.071896</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>53.1000</td>\n      <td>0</td>\n      <td>33.649123</td>\n      <td>0.225927</td>\n      <td>33.755920</td>\n      <td>0.199984</td>\n      <td>5.938462</td>\n      <td>0.078559</td>\n      <td>6.265957</td>\n      <td>0.055481</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>3</td>\n      <td>0</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8.0500</td>\n      <td>0</td>\n      <td>23.242690</td>\n      <td>0.256739</td>\n      <td>18.566485</td>\n      <td>0.311537</td>\n      <td>6.100000</td>\n      <td>0.059899</td>\n      <td>6.093617</td>\n      <td>0.097651</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <td>886</td>\n      <td>2</td>\n      <td>0</td>\n      <td>27.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>13.0000</td>\n      <td>0</td>\n      <td>25.824561</td>\n      <td>0.129386</td>\n      <td>20.513661</td>\n      <td>0.148259</td>\n      <td>5.811538</td>\n      <td>0.061989</td>\n      <td>6.168085</td>\n      <td>0.067192</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>887</td>\n      <td>1</td>\n      <td>1</td>\n      <td>19.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>30.0000</td>\n      <td>0</td>\n      <td>24.210526</td>\n      <td>0.275001</td>\n      <td>22.284153</td>\n      <td>0.211829</td>\n      <td>5.846154</td>\n      <td>0.060876</td>\n      <td>6.104255</td>\n      <td>0.057290</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>888</td>\n      <td>3</td>\n      <td>1</td>\n      <td>28.0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>23.4500</td>\n      <td>0</td>\n      <td>30.067251</td>\n      <td>0.288539</td>\n      <td>30.038251</td>\n      <td>0.231516</td>\n      <td>9.200000</td>\n      <td>0.032052</td>\n      <td>9.157447</td>\n      <td>0.038314</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>889</td>\n      <td>1</td>\n      <td>0</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>30.0000</td>\n      <td>1</td>\n      <td>23.023392</td>\n      <td>0.228507</td>\n      <td>18.191257</td>\n      <td>0.271455</td>\n      <td>5.961538</td>\n      <td>0.093097</td>\n      <td>6.304255</td>\n      <td>0.077616</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>3</td>\n      <td>0</td>\n      <td>32.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.7500</td>\n      <td>2</td>\n      <td>23.175439</td>\n      <td>0.216775</td>\n      <td>17.273224</td>\n      <td>0.278064</td>\n      <td>5.938462</td>\n      <td>0.051723</td>\n      <td>5.942553</td>\n      <td>0.053340</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>891 rows Ã 16 columns</p>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"df_cleaned, removed_columns = remove_single_unique_or_all_nans(test_data[['Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']].copy())\nnumeric_cols, non_numeric_cols = get_numeric_and_non_numeric_columns(df_cleaned)\nmissing_cols = columns_with_missing_values(df_cleaned)\nfilledNumeric_df = fill_missingNumeric_with_median(df_cleaned, missing_cols, numeric_cols)\nmissing_cols = columns_with_missing_values(filledNumeric_df)\nhigh_uniques = columnsCategory_with_more_than_X_percent_unique(filledNumeric_df, non_numeric_cols, 10)\nupdated_df_test, new_columns_test, mappings_test = convert_All_integer_columns(filledNumeric_df, numeric_cols, missing_cols, non_numeric_cols, high_uniques, 'Survived', train_data)\nupdated_df_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-22T11:41:00.760911Z","iopub.execute_input":"2024-10-22T11:41:00.761334Z","iopub.status.idle":"2024-10-22T11:41:12.856440Z","shell.execute_reply.started":"2024-10-22T11:41:00.761252Z","shell.execute_reply":"2024-10-22T11:41:12.855216Z"}},"outputs":[{"name":"stdout","text":"Removed columns due to all NaN or only 1 unique value: []\nNumeric columns: ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\nNon-numeric columns: ['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked']\nMissing data columns: ['Age', 'Fare', 'Cabin']\nDone inputing missing numeric values with median!\nMissing data columns: ['Cabin']\nColumns with high uniques: ['Name', 'Ticket', 'Cabin']\n[No NA values][Less Unique Values] Categoric columns Converted to Integer: Sex\n[No NA values][Less Unique Values] Categoric columns Converted to Integer: Embarked\n[No NA values][Multiple Unique Values] Categoric columns Converted to Distance Based On Cluster: Name\n[No NA values][Multiple Unique Values] Categoric columns Converted to Distance Based On Cluster: Ticket\n[NA values][Multiple Unique Values] Categoric columns Converted to Distance Based On Cluster and Missing Are Predicted: Cabin\nMappings: {'Sex': {'male': 0, 'female': 1}, 'Embarked': {'Q': 0, 'S': 1, 'C': 2}}\n","output_type":"stream"},{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"     Pclass  Sex   Age  SibSp  Parch      Fare  Embarked  Name_true_lev  \\\n0         3    0  34.5      0      0    7.8292         0      22.912281   \n1         3    1  47.0      1      0    7.0000         1      25.918129   \n2         2    0  62.0      0      0    9.6875         0      24.078947   \n3         3    0  27.0      0      0    8.6625         1      22.967836   \n4         3    1  22.0      1      1   12.2875         1      33.301170   \n..      ...  ...   ...    ...    ...       ...       ...            ...   \n413       3    0  27.0      0      0    8.0500         1      24.210526   \n414       1    1  39.0      0      0  108.9000         2      27.643275   \n415       3    0  38.5      0      0    7.2500         1      24.786550   \n416       3    0  27.0      0      0    8.0500         1      22.692982   \n417       3    0  27.0      1      1   22.3583         2      23.941520   \n\n     Name_true_reg  Name_false_lev  Name_false_reg  Ticket_true_lev  \\\n0         0.238211       16.633880        0.306957         6.153846   \n1         0.239974       24.313297        0.229048         5.996154   \n2         0.230024       19.597450        0.270204         5.926923   \n3         0.236747       17.063752        0.297216         5.869231   \n4         0.255400       33.191257        0.222822         6.250000   \n..             ...             ...             ...              ...   \n413       0.207600       17.950820        0.269737         8.253846   \n414       0.157274       24.136612        0.154155         6.823077   \n415       0.229698       20.810565        0.269645        15.423077   \n416       0.253124       17.306011        0.310065         6.180769   \n417       0.253106       20.278689        0.268949         5.769231   \n\n     Ticket_true_reg  Ticket_false_lev  Ticket_false_reg  \n0           0.068473          6.159574          0.047670  \n1           0.039048          5.887234          0.045391  \n2           0.035986          5.951064          0.039179  \n3           0.034021          5.912766          0.040689  \n4           0.065359          6.374468          0.064780  \n..               ...               ...               ...  \n413         0.049999          8.093617          0.064082  \n414         0.107360          7.487234          0.051602  \n415         0.062341         15.153191          0.076489  \n416         0.044993          5.985106          0.038481  \n417         0.051263          6.040426          0.046729  \n\n[418 rows x 15 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pclass</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Fare</th>\n      <th>Embarked</th>\n      <th>Name_true_lev</th>\n      <th>Name_true_reg</th>\n      <th>Name_false_lev</th>\n      <th>Name_false_reg</th>\n      <th>Ticket_true_lev</th>\n      <th>Ticket_true_reg</th>\n      <th>Ticket_false_lev</th>\n      <th>Ticket_false_reg</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>34.5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.8292</td>\n      <td>0</td>\n      <td>22.912281</td>\n      <td>0.238211</td>\n      <td>16.633880</td>\n      <td>0.306957</td>\n      <td>6.153846</td>\n      <td>0.068473</td>\n      <td>6.159574</td>\n      <td>0.047670</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>47.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>7.0000</td>\n      <td>1</td>\n      <td>25.918129</td>\n      <td>0.239974</td>\n      <td>24.313297</td>\n      <td>0.229048</td>\n      <td>5.996154</td>\n      <td>0.039048</td>\n      <td>5.887234</td>\n      <td>0.045391</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>62.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>9.6875</td>\n      <td>0</td>\n      <td>24.078947</td>\n      <td>0.230024</td>\n      <td>19.597450</td>\n      <td>0.270204</td>\n      <td>5.926923</td>\n      <td>0.035986</td>\n      <td>5.951064</td>\n      <td>0.039179</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>3</td>\n      <td>0</td>\n      <td>27.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8.6625</td>\n      <td>1</td>\n      <td>22.967836</td>\n      <td>0.236747</td>\n      <td>17.063752</td>\n      <td>0.297216</td>\n      <td>5.869231</td>\n      <td>0.034021</td>\n      <td>5.912766</td>\n      <td>0.040689</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>3</td>\n      <td>1</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>12.2875</td>\n      <td>1</td>\n      <td>33.301170</td>\n      <td>0.255400</td>\n      <td>33.191257</td>\n      <td>0.222822</td>\n      <td>6.250000</td>\n      <td>0.065359</td>\n      <td>6.374468</td>\n      <td>0.064780</td>\n    </tr>\n    <tr>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <td>413</td>\n      <td>3</td>\n      <td>0</td>\n      <td>27.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8.0500</td>\n      <td>1</td>\n      <td>24.210526</td>\n      <td>0.207600</td>\n      <td>17.950820</td>\n      <td>0.269737</td>\n      <td>8.253846</td>\n      <td>0.049999</td>\n      <td>8.093617</td>\n      <td>0.064082</td>\n    </tr>\n    <tr>\n      <td>414</td>\n      <td>1</td>\n      <td>1</td>\n      <td>39.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>108.9000</td>\n      <td>2</td>\n      <td>27.643275</td>\n      <td>0.157274</td>\n      <td>24.136612</td>\n      <td>0.154155</td>\n      <td>6.823077</td>\n      <td>0.107360</td>\n      <td>7.487234</td>\n      <td>0.051602</td>\n    </tr>\n    <tr>\n      <td>415</td>\n      <td>3</td>\n      <td>0</td>\n      <td>38.5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.2500</td>\n      <td>1</td>\n      <td>24.786550</td>\n      <td>0.229698</td>\n      <td>20.810565</td>\n      <td>0.269645</td>\n      <td>15.423077</td>\n      <td>0.062341</td>\n      <td>15.153191</td>\n      <td>0.076489</td>\n    </tr>\n    <tr>\n      <td>416</td>\n      <td>3</td>\n      <td>0</td>\n      <td>27.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8.0500</td>\n      <td>1</td>\n      <td>22.692982</td>\n      <td>0.253124</td>\n      <td>17.306011</td>\n      <td>0.310065</td>\n      <td>6.180769</td>\n      <td>0.044993</td>\n      <td>5.985106</td>\n      <td>0.038481</td>\n    </tr>\n    <tr>\n      <td>417</td>\n      <td>3</td>\n      <td>0</td>\n      <td>27.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>22.3583</td>\n      <td>2</td>\n      <td>23.941520</td>\n      <td>0.253106</td>\n      <td>20.278689</td>\n      <td>0.268949</td>\n      <td>5.769231</td>\n      <td>0.051263</td>\n      <td>6.040426</td>\n      <td>0.046729</td>\n    </tr>\n  </tbody>\n</table>\n<p>418 rows Ã 15 columns</p>\n</div>"},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport joblib\nimport statsmodels.api as sm\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nimport optuna\nfrom sklearn.model_selection import cross_val_score\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\ndef very_fast_backward_feature_selection(data, target, n_features_to_select, models_dict, n_jobs):\n    # Prepare the feature matrix and target vector\n    X = data.drop(columns=[target])\n    y = data[target]\n    \n    results = {}\n    \n    for model_name, model in models_dict.items():\n        # Initialize SFS with the model\n        sfs = SFS(model, \n                  k_features=n_features_to_select, \n                  forward=False, \n                  floating=False, \n                  scoring='accuracy', \n                  cv=2, \n                  n_jobs=n_jobs)\n        \n        # Fit SFS\n        sfs = sfs.fit(X, y)\n        \n        # Get the names of the selected features\n        selected_features = list(sfs.k_feature_names_)\n        \n        # Fit model using statsmodels for p-values and coefficients\n        X_selected = sm.add_constant(X[selected_features])\n        sm_model = sm.OLS(y, X_selected).fit()\n        \n        summary = sm_model.summary2().tables[1]\n        \n        # Print the summary\n        print(f\"Model: {model_name}\")\n        print(sm_model.summary())\n        \n        # Store the selected features and model summary\n        results[model_name] = {\n            'selected_features': selected_features,\n            'model_summary': summary\n        }\n    \n    return results\n\n\ndef optimize_models(models_dict, X, y, n_trials=20, n_jobs=1):\n    best_models = {}\n    best_scores = {}\n\n    # Objective function to optimize\n    def objective(trial, model_name):\n        model = models_dict[model_name]\n\n        if model_name == 'RandomForestClassifier':\n            model.set_params(\n                n_estimators=trial.suggest_int('n_estimators', 1, 10),\n                max_depth=trial.suggest_int('max_depth', 1, 4),\n                min_samples_split=trial.suggest_int('min_samples_split', 2, 4),\n                min_samples_leaf=trial.suggest_int('min_samples_leaf', 1, 4)\n            )\n            \n        elif model_name == 'GradientBoostingClassifier':\n            model.set_params(\n                n_estimators=trial.suggest_int('n_estimators', 1, 10),\n                # learning_rate=trial.suggest_float('learning_rate', 0.01, 0.1),\n                max_depth=trial.suggest_int('max_depth', 1, 4)\n            )\n            \n        elif model_name == 'XGBClassifier':\n            model.set_params(\n                n_estimators=trial.suggest_int('n_estimators', 1, 10),\n                # learning_rate=trial.suggest_float('learning_rate', 0.01, 0.1),\n                max_depth=trial.suggest_int('max_depth', 1, 4)\n            )\n            \n        elif model_name == 'LGBMClassifier':\n            model.set_params(\n                n_estimators=trial.suggest_int('n_estimators', 1, 10),\n                # learning_rate=trial.suggest_float('learning_rate', 0.01, 0.1),\n                max_depth=trial.suggest_int('max_depth', 1, 4)\n            )\n            \n        elif model_name == 'KNeighborsClassifier':\n            model.set_params(\n                n_neighbors=trial.suggest_int('n_neighbors', 1, 10),\n                leaf_size=trial.suggest_int('leaf_size', 10, 30),\n                p=trial.suggest_int('p', 1, 2)\n            )\n            \n        elif model_name == 'SupportVectorClassifier':\n            model.set_params(\n                # C=trial.suggest_float('C', 0.1, 10.0),\n                kernel=trial.suggest_categorical('kernel', ['linear', 'rbf']),\n                gamma=trial.suggest_categorical('gamma', ['scale', 'auto'])\n            )\n\n        elif model_name == 'DecisionTreeClassifier':\n            model.set_params(\n                max_depth=trial.suggest_int('max_depth', 1, 3),\n                min_samples_split=trial.suggest_int('min_samples_split', 2, 4),\n                min_samples_leaf=trial.suggest_int('min_samples_leaf', 1, 4)\n            )\n\n        # Perform cross-validation\n        scores = cross_val_score(model, X, y, cv=3, scoring='accuracy')\n        return scores.mean()\n\n    # Create a study object and optimize the objective function for each model\n    for model_name in models_dict.keys():\n        print(f\"Optimizing {model_name}...\")\n        study = optuna.create_study(direction='maximize')\n        study.optimize(lambda trial: objective(trial, model_name), n_trials=n_trials, n_jobs=n_jobs)  # Use n_jobs for parallel execution\n        \n        # Store best hyperparameters and score\n        best_models[model_name] = study.best_params\n        best_scores[model_name] = study.best_value\n\n        # Print best hyperparameters and score\n        print(f\"Best hyperparameters for {model_name}: {study.best_params}\")\n        print(f\"Best score for {model_name}: {study.best_value}\\n\")\n\n    return best_models, best_scores\n\n\ndef update_model_params(models_dict, best_params):\n    # Update each model with the best hyperparameters\n    for model_name, params in best_params.items():\n        model = models_dict[model_name]\n        model.set_params(**params)\n    return models_dict\n\n\ndef fit_models(models_dict, X, y, save_path='/kaggle/working/'):\n    fitted_models = {}\n\n    for model_name, model in models_dict.items():\n        print(f\"Fitting {model_name}...\")\n        # Fit the model\n        model.fit(X, y)\n        \n        # Save the fitted model\n        model_filename = f\"{save_path}{model_name}.joblib\"\n        joblib.dump(model, model_filename)\n        \n        # Store the model in the dictionary\n        fitted_models[model_name] = model\n\n    return fitted_models","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-22T00:13:17.961962Z","iopub.execute_input":"2024-10-22T00:13:17.962412Z","iopub.status.idle":"2024-10-22T00:13:19.424907Z","shell.execute_reply.started":"2024-10-22T00:13:17.962334Z","shell.execute_reply":"2024-10-22T00:13:19.423815Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"FS_models_dict = {\n    'LogisticRegression': LogisticRegression(max_iter=1000, solver='liblinear', penalty='l2'),\n    'SupportVectorClassifier': SVC(probability=True, gamma='auto')\n}\nfinal_models_dict = {\n    'LogisticRegression': LogisticRegression(max_iter=1000, solver='liblinear', penalty='l2'),\n    'RandomForestClassifier': RandomForestClassifier(),\n    'SupportVectorClassifier': SVC(probability=True, C=1),\n    'KNeighborsClassifier': KNeighborsClassifier(),\n    'NaiveBayes': GaussianNB(),\n    'DecisionTreeClassifier': DecisionTreeClassifier(),\n    'GradientBoostingClassifier': GradientBoostingClassifier(learning_rate=0.01),\n    'XGBClassifier': XGBClassifier(use_label_encoder=False, eval_metric='logloss', learning_rate=0.01),\n    'LGBMClassifier': LGBMClassifier()\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-22T00:13:19.426858Z","iopub.execute_input":"2024-10-22T00:13:19.427196Z","iopub.status.idle":"2024-10-22T00:13:19.435783Z","shell.execute_reply.started":"2024-10-22T00:13:19.427134Z","shell.execute_reply":"2024-10-22T00:13:19.434567Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"results = very_fast_backward_feature_selection(train_df, 'Survived', n_features_to_select=1, models_dict=FS_models_dict, n_jobs=1)\nX = train_df.drop(columns=['Survived'])\ny = train_df['Survived']\nbest_models, best_scores = optimize_models(final_models_dict, X, y, n_trials=1, n_jobs=1)  # Using n_jobs=4 for parallel execution\nprint(\"Best models:\", best_models)\nprint(\"Best scores:\", best_scores)\nto_fitModels = update_model_params(final_models_dict, best_models)\nfitted_models = fit_models(to_fitModels, X, y, save_path='/kaggle/working/')\nprint(\"Models fitted and saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-22T00:13:19.437500Z","iopub.execute_input":"2024-10-22T00:13:19.437825Z","iopub.status.idle":"2024-10-22T00:13:19.450770Z","shell.execute_reply.started":"2024-10-22T00:13:19.437770Z","shell.execute_reply":"2024-10-22T00:13:19.449663Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import joblib\nimport pandas as pd\n\ndef predict_with_models(models_dict, X_new, save_path='/kaggle/working/'):\n    predictions = {}\n    \n    for model_name in models_dict.keys():\n        try:\n            # Load the fitted model\n            model_filename = f\"{save_path}{model_name}.joblib\"\n            model = joblib.load(model_filename)\n            \n            # Make predictions\n            predictions[model_name] = model.predict(X_new)\n        except:\n            pass\n    \n    # Convert predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions)\n    \n    return predictions_df\npredictions_df = predict_with_models(final_models_dict, updated_df_test, save_path='/kaggle/working/')\npredictions_df.head(418)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-22T00:13:19.453050Z","iopub.execute_input":"2024-10-22T00:13:19.453582Z","iopub.status.idle":"2024-10-22T00:13:19.464740Z","shell.execute_reply.started":"2024-10-22T00:13:19.453453Z","shell.execute_reply":"2024-10-22T00:13:19.463620Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"estimators1 = [\n    ('LogisticRegression', LogisticRegression(max_iter=1000, solver='liblinear', penalty='l2')),\n    ('RandomForestClassifier', RandomForestClassifier(n_estimators=2, max_depth=3, min_samples_split=4, min_samples_leaf=3)),\n    ('SupportVectorClassifier', SVC(probability=True, C=1, kernel='linear', gamma='scale')),\n    ('KNeighborsClassifier', KNeighborsClassifier(n_neighbors=7, leaf_size=23, p=2)),\n    ('NaiveBayes', GaussianNB()),\n    ('DecisionTreeClassifier', DecisionTreeClassifier(max_depth=1, min_samples_split=2, min_samples_leaf=3)),\n    ('GradientBoostingClassifier', GradientBoostingClassifier(n_estimators=5, max_depth=2, learning_rate=0.01)),\n    ('XGBClassifier', XGBClassifier(use_label_encoder=False, eval_metric='logloss',n_estimators=7, max_depth=3,learning_rate=0.01)),\n    ('LGBMClassifier', LGBMClassifier(n_estimators=2, max_depth=4))\n]\n\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nX = train_df.drop(columns=['Survived'])\ny = train_df['Survived']\nVotingClass = VotingClassifier(estimators=estimators1, voting='hard')\nVotingClass.fit(X, y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-22T00:14:04.089376Z","iopub.execute_input":"2024-10-22T00:14:04.089742Z","iopub.status.idle":"2024-10-22T00:14:14.431549Z","shell.execute_reply.started":"2024-10-22T00:14:04.089689Z","shell.execute_reply":"2024-10-22T00:14:14.430522Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"VotingClassifier(estimators=[('LogisticRegression',\n                              LogisticRegression(C=1.0, class_weight=None,\n                                                 dual=False, fit_intercept=True,\n                                                 intercept_scaling=1,\n                                                 l1_ratio=None, max_iter=1000,\n                                                 multi_class='warn',\n                                                 n_jobs=None, penalty='l2',\n                                                 random_state=None,\n                                                 solver='liblinear', tol=0.0001,\n                                                 verbose=0, warm_start=False)),\n                             ('RandomForestClassifier',\n                              RandomForestClassifier(bootstra...\n                                             importance_type='split',\n                                             learning_rate=0.1, max_depth=4,\n                                             min_child_samples=20,\n                                             min_child_weight=0.001,\n                                             min_split_gain=0.0, n_estimators=2,\n                                             n_jobs=-1, num_leaves=31,\n                                             objective=None, random_state=None,\n                                             reg_alpha=0.0, reg_lambda=0.0,\n                                             silent=True, subsample=1.0,\n                                             subsample_for_bin=200000,\n                                             subsample_freq=0))],\n                 flatten_transform=True, n_jobs=None, voting='hard',\n                 weights=None)"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"predictions = VotingClass.predict(updated_df_test)\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-21T23:58:16.078502Z","iopub.execute_input":"2024-10-21T23:58:16.078899Z","iopub.status.idle":"2024-10-21T23:58:18.364997Z","shell.execute_reply.started":"2024-10-21T23:58:16.078828Z","shell.execute_reply":"2024-10-21T23:58:18.363523Z"}},"outputs":[{"name":"stdout","text":"Your submission was successfully saved!\n","output_type":"stream"}],"execution_count":27}]}