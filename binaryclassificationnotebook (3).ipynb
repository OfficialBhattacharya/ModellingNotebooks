{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":29507,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_kg_hide-input":false,"trusted":true,"execution":{"iopub.status.busy":"2024-10-23T12:33:42.570319Z","iopub.execute_input":"2024-10-23T12:33:42.570706Z","iopub.status.idle":"2024-10-23T12:33:42.830444Z","shell.execute_reply.started":"2024-10-23T12:33:42.570641Z","shell.execute_reply":"2024-10-23T12:33:42.829319Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/titanic/train.csv\n/kaggle/input/titanic/test.csv\n/kaggle/input/titanic/gender_submission.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import LabelEncoder\nimport Levenshtein\nimport re\n\n\ndef intersection_of_lists(list1, list2):\n    return list(set(list1) & set(list2))\n\n\ndef difference_of_lists(list1, list2):\n    return [item for item in list1 if item not in list2]\n\n\ndef get_numeric_and_non_numeric_columns(df):\n    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n    non_numeric_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()\n    print(f\"Numeric columns: {numeric_cols}\")\n    print(f\"Non-numeric columns: {non_numeric_cols}\")\n    return numeric_cols, non_numeric_cols\n\n\ndef remove_single_unique_or_all_nans(df):\n    removed_columns = []\n    for column in df.columns:\n        if df[column].nunique() <= 1 or df[column].isna().all():\n            removed_columns.append(column)\n            df = df.drop(columns=[column])\n    print(f\"Removed columns due to all NaN or only 1 unique value: {removed_columns}\")\n    return df, removed_columns\n\n\ndef columns_with_missing_values(df):\n    missing_cols = [col for col in df.columns if df[col].isna().any()]\n    print(f\"Missing data columns: {missing_cols}\")\n    return missing_cols\n\n\ndef fill_missingNumeric_with_median(df, missing_cols, numeric_cols):\n    for col in intersection_of_lists(missing_cols, numeric_cols):\n        median_value = df[col].median()\n        df[col].fillna(median_value, inplace=True)\n    print(\"Done inputing missing numeric values with median!\")\n    return df\n\n\ndef columnsCategory_with_more_than_X_percent_unique(df, categoric_cols, perc):\n    total_rows = len(df)\n    threshold = total_rows * 0.01 * perc  # 10% of the total number of rows\n    cols_with_high_uniques = [col for col in categoric_cols if df[col].nunique() > threshold]\n    print(f\"Columns with high uniques: {cols_with_high_uniques}\")\n    return cols_with_high_uniques\n    \n\n\ndef convert_and_create_integer_columns(df, new_columns, mappings, colName):\n    df[colName] = df[colName].astype('object')\n    df[colName], unique_values = pd.factorize(df[colName])\n    # Add the new column name to the list\n    new_columns.append(colName)\n    # Create a mapping dictionary for the column\n    mappings[colName] = {value: i for i, value in enumerate(unique_values)}\n    return df, new_columns, mappings\n\n\ndef fill_missing_and_predict(df, new_columns, mappings, usable_cols, column_name):\n    # Convert and create integer column\n    df, new_columns, mappings = convert_and_create_integer_columns(df, new_columns, mappings, column_name)\n    # Train the model to predict missing values\n    non_missing_idx = df[column_name] != -1  # Using -1 for factorized NaNs\n    missing_idx = df[column_name] == -1\n    if missing_idx.sum() > 0:\n        X_train = df.loc[non_missing_idx, usable_cols]\n        y_train = df.loc[non_missing_idx, column_name]\n        X_test = df.loc[missing_idx,  usable_cols]\n        model = LogisticRegression(max_iter=1000, solver ='lbfgs',  multi_class='auto')\n        model.fit(X_train, y_train)\n        # Predict the missing values\n        predicted = model.predict(X_test)\n        # Replace the missing values with the predicted values\n        df.loc[missing_idx, column_name] = predicted\n    return df, new_columns, mappings\n    \n\ndef get_bigrams(string):\n    # Generate bigrams from a string\n    return [string[i:i+2] for i in range(len(string)-1)]\n\ndef sorensen_dice(a, b):\n    # SÃ¸rensen-Dice coefficient for two sets\n    a_bigrams = set(get_bigrams(a))\n    b_bigrams = set(get_bigrams(b))\n    overlap = len(a_bigrams & b_bigrams)\n    total = len(a_bigrams) + len(b_bigrams)\n    if total == 0:\n        return 1.0 if a == b else 0.0  # Handle identical empty strings\n    return 2 * overlap / total\n\n\ndef calculate_meanDistanceFromAList(input_string, string_list):\n    sum_Levenshtein = 0\n    sum_sorensen_dice = 0\n    for string in string_list:\n        sum_Levenshtein = sum_Levenshtein + Levenshtein.distance(input_string, string)\n        sum_sorensen_dice = sum_sorensen_dice + sorensen_dice(input_string, string)\n    return float(sum_Levenshtein/len(string_list)),float(sum_sorensen_dice/len(string_list))\n    \n\ndef takeOut_stringList(df, target, variableCol):\n    return list(df[df[f\"{target}\"]==1][f\"{variableCol}\"].unique()),list(df[df[f\"{target}\"]==0][f\"{variableCol}\"].unique())\n\n\ndef apply_meanDistance(df, column_name, string_list):\n    # Calculate mean distances for each row and add a new column\n    df[['mean_Levenshtein', 'mean_sorensen_dice']] = df[column_name].apply(\n        lambda x: pd.Series(calculate_meanDistanceFromAList(x, string_list))\n    )\n    return df\n\n\ndef create_DistanceMetric(df, new_columns, usable_cols, colName, target, orig_data):\n    df[colName] = df[colName].astype('str')\n    true_NameList, false_NameList = takeOut_stringList(orig_data, target, colName)\n    new_columns.append(colName)\n    colName_true_lev = str(colName+\"_true_lev\")\n    colName_true_reg = str(colName+\"_true_reg\")\n    df[[colName_true_lev, colName_true_reg]] = df[colName].apply(\n        lambda x: pd.Series(calculate_meanDistanceFromAList(x, true_NameList))\n    )\n    colName_false_lev = str(colName+\"_false_lev\")\n    colName_false_reg = str(colName+\"_false_reg\")\n    df[[colName_false_lev, colName_false_reg]] = df[colName].apply(\n        lambda x: pd.Series(calculate_meanDistanceFromAList(x, false_NameList))\n    )\n    usable_cols = usable_cols + [colName_true_lev, colName_true_reg, colName_false_lev, colName_false_reg]\n    return df, new_columns, usable_cols\n\n\ndef convert_All_integer_columns(df, numeric_cols, missing_cols, categoric_cols, cols_with_high_uniques, target, orig_data):\n    new_columns = []\n    mappings = {}\n    usable_cols = numeric_cols \n    \n    categoric_nonNA_cols = difference_of_lists(categoric_cols, missing_cols)\n\n    categoric_nonNA_Few_cols = difference_of_lists(categoric_nonNA_cols, cols_with_high_uniques)\n    \n    categoric_nonNA_Multiple_cols = difference_of_lists(categoric_nonNA_cols, categoric_nonNA_Few_cols)\n    \n    categoric_NA_Few_cols = difference_of_lists(missing_cols, cols_with_high_uniques)\n\n    categoric_NA_Multiple_cols = difference_of_lists(missing_cols, categoric_NA_Few_cols)\n    \n    for col in categoric_nonNA_Few_cols:\n        df, new_columns, mappings = convert_and_create_integer_columns(df, new_columns, mappings, col)\n        print(f\"[No NA values][Less Unique Values] Categoric columns Converted to Integer: {col}\")\n    usable_cols = usable_cols + categoric_nonNA_Few_cols\n    for col in  categoric_NA_Few_cols:   \n        df, new_columns, mappings = fill_missing_and_predict(df, new_columns, mappings, usable_cols, col)\n        print(f\"[NA values][Less Unique Values] Categoric columns Converted to Integer and Missing Are Predicted: {col}\")\n        usable_cols = usable_cols + [col]    \n    for col in categoric_nonNA_Multiple_cols:   \n        df, new_columns, usable_cols = create_DistanceMetric(df, new_columns, usable_cols, col, target, orig_data)\n        df = df.drop(columns=[col])\n        print(f\"[No NA values][Multiple Unique Values] Categoric columns Converted to Distance Based On Cluster: {col}\")\n    for col in  categoric_NA_Multiple_cols:\n        df = df.drop(columns=[col])\n        print(f\"[NA values][Multiple Unique Values] Categoric columns Converted to Distance Based On Cluster and Missing Are Predicted: {col}\")  \n    print(f\"Mappings: {mappings}\")\n    return df, new_columns, mappings\n\n\ndef oneHotEncoded(df, columns_to_oneHot, target, id):\n    # Perform one-hot encoding on specified columns\n    df_encoded = pd.get_dummies(df, columns=columns_to_oneHot, drop_first=True)\n    \n    # Ensure target and id columns are in the result\n    df_encoded[target] = df[target]\n    df_encoded[id] = df[id]\n    \n    return df_encoded\n\nfrom itertools import combinations\n\ndef subset_counts_oneHot(featureMaster, target, id):\n    # Filter rows where the target is 1\n    df_target_1 = featureMaster[featureMaster[target] == 1]\n    \n    # Drop target and id columns to get only the one-hot encoded columns\n    one_hot_columns = df_target_1.drop(columns=[target, id]).columns\n    \n    # Dictionary to store subsets and their counts\n    subset_counts = defaultdict(int)\n    \n    # Helper function to check if a subset has more than two one-hot columns from the same original column\n    def valid_subset(subset):\n        original_cols = [col.split('_')[0] for col in subset]\n        return all(original_cols.count(col) <= 1 for col in original_cols)\n    \n    # Iterate over all possible subsets of one-hot encoded columns\n    for r in range(1, len(one_hot_columns) + 1):\n        for subset in combinations(one_hot_columns, r):\n            if valid_subset(subset):\n                subset_df = df_target_1[list(subset)]\n                count = (subset_df.sum(axis=1) == len(subset)).sum()\n                subset_counts[subset] = count/len(df_target_1)\n    \n    return dict(subset_counts)\n\n\ndef filter_and_sort_subsets(subset_counts, threshold):\n    # Filter subsets based on the given threshold\n    filtered_subsets = {subset: count for subset, count in subset_counts.items() if count > threshold}\n    \n    # Sort the filtered subsets based on their counts in descending order\n    sorted_subsets = sorted(filtered_subsets.items(), key=lambda item: item[1], reverse=True)\n    \n    return sorted_subsets\n\ndef create_fractional_df(featureMaster, id_column, target_column, filtered_sorted_subsets):\n    # Create a copy of the dataframe to avoid modifying the original\n    df = featureMaster.copy()\n    \n    for subset, _ in filtered_sorted_subsets:\n        new_col_name = \"_\".join(subset) + \"_fraction\"\n        df[new_col_name] = df[list(subset)].mean(axis=1)\n    \n    # Return a dataframe with the id, target, and new fraction columns\n    new_fraction_columns = [(\"_\".join(subset) + \"_fraction\") for subset, _ in filtered_sorted_subsets]\n    selected_columns = [id_column, target_column] + new_fraction_columns\n    \n    return df[selected_columns]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-23T13:11:45.141704Z","iopub.execute_input":"2024-10-23T13:11:45.142190Z","iopub.status.idle":"2024-10-23T13:11:45.192045Z","shell.execute_reply.started":"2024-10-23T13:11:45.142131Z","shell.execute_reply":"2024-10-23T13:11:45.190986Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"class tangiD_BinaryClassification:\n    '''Takes in the data , target and features'''\n    def __init__(self, data, train, target, type, features):\n        self.data = data\n        self.target = target\n        self.type = type\n        if(self.type!=\"TestData!\"):\n            self.origData = self.data.copy()\n        else:\n            self.origData = train\n        self.allFeatures = features\n    \n    def medianIntifying(self, highUniq = 10):\n        self.cleanDF, self.removed_columns = remove_single_unique_or_all_nans(self.data[self.allFeatures].copy())\n        self.numeric_cols, self.non_numeric_cols = get_numeric_and_non_numeric_columns(self.cleanDF)\n        self.missing_cols = columns_with_missing_values(self.cleanDF)\n        self.filledNumeric_df = fill_missingNumeric_with_median(self.cleanDF, self.missing_cols, self.numeric_cols)\n        self.missing_cols = columns_with_missing_values(self.filledNumeric_df)\n        self.high_uniques = columnsCategory_with_more_than_X_percent_unique(self.filledNumeric_df, self.non_numeric_cols, highUniq)\n        self.updated_df, self.new_columns, self.mappings = convert_All_integer_columns(self.filledNumeric_df, self.numeric_cols, self.missing_cols, self.non_numeric_cols, self.high_uniques, self.target, self.origData)\n        if(self.type!=\"TestData!\"):\n            self.updated_df = pd.concat([self.updated_df, self.data[self.target]], axis=1)\n            self.updated_df['Index'] = self.updated_df.index\n            return self.updated_df\n        else:\n            self.updated_df['Index'] = self.updated_df.index\n            return self.updated_df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-23T12:36:40.103508Z","iopub.execute_input":"2024-10-23T12:36:40.103934Z","iopub.status.idle":"2024-10-23T12:36:40.116193Z","shell.execute_reply.started":"2024-10-23T12:36:40.103862Z","shell.execute_reply":"2024-10-23T12:36:40.115127Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntrainClass = tangiD_BinaryClassification(train_data, None, \"Survived\", \"TrainData!\", ['Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch','Ticket', 'Fare', 'Cabin', 'Embarked'])\ntrainData = trainClass.medianIntifying(10)\ntrainData","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-23T12:36:42.699544Z","iopub.execute_input":"2024-10-23T12:36:42.699936Z","iopub.status.idle":"2024-10-23T12:37:03.296257Z","shell.execute_reply.started":"2024-10-23T12:36:42.699863Z","shell.execute_reply":"2024-10-23T12:37:03.295328Z"}},"outputs":[{"name":"stdout","text":"Removed columns due to all NaN or only 1 unique value: []\nNumeric columns: ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\nNon-numeric columns: ['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked']\nMissing data columns: ['Age', 'Cabin', 'Embarked']\nDone inputing missing numeric values with median!\nMissing data columns: ['Cabin', 'Embarked']\nColumns with high uniques: ['Name', 'Ticket', 'Cabin']\n[No NA values][Less Unique Values] Categoric columns Converted to Integer: Sex\n[NA values][Less Unique Values] Categoric columns Converted to Integer and Missing Are Predicted: Embarked\n[No NA values][Multiple Unique Values] Categoric columns Converted to Distance Based On Cluster: Name\n[No NA values][Multiple Unique Values] Categoric columns Converted to Distance Based On Cluster: Ticket\n[NA values][Multiple Unique Values] Categoric columns Converted to Distance Based On Cluster and Missing Are Predicted: Cabin\nMappings: {'Sex': {'male': 0, 'female': 1}, 'Embarked': {'S': 0, 'C': 1, 'Q': 2}}\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"     Pclass  Sex   Age  SibSp  Parch     Fare  Embarked  Name_true_lev  \\\n0         3    0  22.0      1      0   7.2500         0      23.330409   \n1         1    1  38.0      1      0  71.2833         1      39.064327   \n2         3    1  26.0      0      0   7.9250         0      23.897661   \n3         1    1  35.0      1      0  53.1000         0      33.649123   \n4         3    0  35.0      0      0   8.0500         0      23.242690   \n..      ...  ...   ...    ...    ...      ...       ...            ...   \n886       2    0  27.0      0      0  13.0000         0      25.824561   \n887       1    1  19.0      0      0  30.0000         0      24.210526   \n888       3    1  28.0      1      2  23.4500         0      30.067251   \n889       1    0  26.0      0      0  30.0000         1      23.023392   \n890       3    0  32.0      0      0   7.7500         2      23.175439   \n\n     Name_true_reg  Name_false_lev  Name_false_reg  Ticket_true_lev  \\\n0         0.240875       18.391621        0.280404         8.076923   \n1         0.222573       39.794171        0.199117         6.919231   \n2         0.268535       20.091075        0.222354        13.261538   \n3         0.225927       33.755920        0.199984         5.938462   \n4         0.256739       18.566485        0.311537         6.100000   \n..             ...             ...             ...              ...   \n886       0.129386       20.513661        0.148259         5.811538   \n887       0.275001       22.284153        0.211829         5.846154   \n888       0.288539       30.038251        0.231516         9.200000   \n889       0.228507       18.191257        0.271455         5.961538   \n890       0.216775       17.273224        0.278064         5.938462   \n\n     Ticket_true_reg  Ticket_false_lev  Ticket_false_reg  Survived  Index  \n0           0.061181          8.010638          0.059223         0      0  \n1           0.101571          7.493617          0.049166         1      1  \n2           0.064463         13.236170          0.071896         1      2  \n3           0.078559          6.265957          0.055481         1      3  \n4           0.059899          6.093617          0.097651         0      4  \n..               ...               ...               ...       ...    ...  \n886         0.061989          6.168085          0.067192         0    886  \n887         0.060876          6.104255          0.057290         1    887  \n888         0.032052          9.157447          0.038314         0    888  \n889         0.093097          6.304255          0.077616         1    889  \n890         0.051723          5.942553          0.053340         0    890  \n\n[891 rows x 17 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pclass</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Fare</th>\n      <th>Embarked</th>\n      <th>Name_true_lev</th>\n      <th>Name_true_reg</th>\n      <th>Name_false_lev</th>\n      <th>Name_false_reg</th>\n      <th>Ticket_true_lev</th>\n      <th>Ticket_true_reg</th>\n      <th>Ticket_false_lev</th>\n      <th>Ticket_false_reg</th>\n      <th>Survived</th>\n      <th>Index</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>7.2500</td>\n      <td>0</td>\n      <td>23.330409</td>\n      <td>0.240875</td>\n      <td>18.391621</td>\n      <td>0.280404</td>\n      <td>8.076923</td>\n      <td>0.061181</td>\n      <td>8.010638</td>\n      <td>0.059223</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>71.2833</td>\n      <td>1</td>\n      <td>39.064327</td>\n      <td>0.222573</td>\n      <td>39.794171</td>\n      <td>0.199117</td>\n      <td>6.919231</td>\n      <td>0.101571</td>\n      <td>7.493617</td>\n      <td>0.049166</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>3</td>\n      <td>1</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.9250</td>\n      <td>0</td>\n      <td>23.897661</td>\n      <td>0.268535</td>\n      <td>20.091075</td>\n      <td>0.222354</td>\n      <td>13.261538</td>\n      <td>0.064463</td>\n      <td>13.236170</td>\n      <td>0.071896</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>53.1000</td>\n      <td>0</td>\n      <td>33.649123</td>\n      <td>0.225927</td>\n      <td>33.755920</td>\n      <td>0.199984</td>\n      <td>5.938462</td>\n      <td>0.078559</td>\n      <td>6.265957</td>\n      <td>0.055481</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>3</td>\n      <td>0</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8.0500</td>\n      <td>0</td>\n      <td>23.242690</td>\n      <td>0.256739</td>\n      <td>18.566485</td>\n      <td>0.311537</td>\n      <td>6.100000</td>\n      <td>0.059899</td>\n      <td>6.093617</td>\n      <td>0.097651</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <td>886</td>\n      <td>2</td>\n      <td>0</td>\n      <td>27.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>13.0000</td>\n      <td>0</td>\n      <td>25.824561</td>\n      <td>0.129386</td>\n      <td>20.513661</td>\n      <td>0.148259</td>\n      <td>5.811538</td>\n      <td>0.061989</td>\n      <td>6.168085</td>\n      <td>0.067192</td>\n      <td>0</td>\n      <td>886</td>\n    </tr>\n    <tr>\n      <td>887</td>\n      <td>1</td>\n      <td>1</td>\n      <td>19.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>30.0000</td>\n      <td>0</td>\n      <td>24.210526</td>\n      <td>0.275001</td>\n      <td>22.284153</td>\n      <td>0.211829</td>\n      <td>5.846154</td>\n      <td>0.060876</td>\n      <td>6.104255</td>\n      <td>0.057290</td>\n      <td>1</td>\n      <td>887</td>\n    </tr>\n    <tr>\n      <td>888</td>\n      <td>3</td>\n      <td>1</td>\n      <td>28.0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>23.4500</td>\n      <td>0</td>\n      <td>30.067251</td>\n      <td>0.288539</td>\n      <td>30.038251</td>\n      <td>0.231516</td>\n      <td>9.200000</td>\n      <td>0.032052</td>\n      <td>9.157447</td>\n      <td>0.038314</td>\n      <td>0</td>\n      <td>888</td>\n    </tr>\n    <tr>\n      <td>889</td>\n      <td>1</td>\n      <td>0</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>30.0000</td>\n      <td>1</td>\n      <td>23.023392</td>\n      <td>0.228507</td>\n      <td>18.191257</td>\n      <td>0.271455</td>\n      <td>5.961538</td>\n      <td>0.093097</td>\n      <td>6.304255</td>\n      <td>0.077616</td>\n      <td>1</td>\n      <td>889</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>3</td>\n      <td>0</td>\n      <td>32.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.7500</td>\n      <td>2</td>\n      <td>23.175439</td>\n      <td>0.216775</td>\n      <td>17.273224</td>\n      <td>0.278064</td>\n      <td>5.938462</td>\n      <td>0.051723</td>\n      <td>5.942553</td>\n      <td>0.053340</td>\n      <td>0</td>\n      <td>890</td>\n    </tr>\n  </tbody>\n</table>\n<p>891 rows Ã 17 columns</p>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"df = trainData.copy()\ncolumns_to_oneHot = ['Sex', 'Embarked', 'Pclass', 'SibSp', 'Parch']\ntarget='Survived'\nid='Index'\nfeatureMaster = oneHotEncoded(df[[id]+ [target]+ columns_to_oneHot], columns_to_oneHot, target, id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-23T13:04:31.923067Z","iopub.execute_input":"2024-10-23T13:04:31.923426Z","iopub.status.idle":"2024-10-23T13:04:31.942857Z","shell.execute_reply.started":"2024-10-23T13:04:31.923372Z","shell.execute_reply":"2024-10-23T13:04:31.941478Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"target='Survived'\nid='Index'\nsubset_counts = subset_counts_oneHot(featureMaster, target, id)\nthreshold = 0.2\nfiltered_sorted_subsets = filter_and_sort_subsets(subset_counts, threshold)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-23T13:31:18.657920Z","iopub.execute_input":"2024-10-23T13:31:18.658278Z","iopub.status.idle":"2024-10-23T13:31:20.867635Z","shell.execute_reply.started":"2024-10-23T13:31:18.658226Z","shell.execute_reply":"2024-10-23T13:31:20.866683Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"filtered_sorted_subsets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-23T13:31:22.353033Z","iopub.execute_input":"2024-10-23T13:31:22.353441Z","iopub.status.idle":"2024-10-23T13:31:22.360708Z","shell.execute_reply.started":"2024-10-23T13:31:22.353373Z","shell.execute_reply":"2024-10-23T13:31:22.359710Z"}},"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"[(('Sex_1',), 0.6812865497076024),\n (('Pclass_3',), 0.347953216374269),\n (('SibSp_1',), 0.32748538011695905),\n (('Embarked_1',), 0.2719298245614035),\n (('Pclass_2',), 0.2543859649122807),\n (('Sex_1', 'SibSp_1'), 0.23391812865497075),\n (('Sex_1', 'Pclass_3'), 0.21052631578947367),\n (('Sex_1', 'Pclass_2'), 0.2046783625730994)]"},"metadata":{}}],"execution_count":59},{"cell_type":"code","source":"new_df = create_fractional_df(featureMaster, id_column='Index', target_column='Survived', filtered_sorted_subsets=filtered_sorted_subsets)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-23T13:31:23.770070Z","iopub.execute_input":"2024-10-23T13:31:23.770408Z","iopub.status.idle":"2024-10-23T13:31:23.798460Z","shell.execute_reply.started":"2024-10-23T13:31:23.770358Z","shell.execute_reply":"2024-10-23T13:31:23.797436Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"new_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-23T13:31:30.887414Z","iopub.execute_input":"2024-10-23T13:31:30.887760Z","iopub.status.idle":"2024-10-23T13:31:30.914466Z","shell.execute_reply.started":"2024-10-23T13:31:30.887704Z","shell.execute_reply":"2024-10-23T13:31:30.913302Z"}},"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"     Index  Survived  Sex_1_fraction  Pclass_3_fraction  SibSp_1_fraction  \\\n0        0         0             0.0                1.0               1.0   \n1        1         1             1.0                0.0               1.0   \n2        2         1             1.0                1.0               0.0   \n3        3         1             1.0                0.0               1.0   \n4        4         0             0.0                1.0               0.0   \n..     ...       ...             ...                ...               ...   \n886    886         0             0.0                0.0               0.0   \n887    887         1             1.0                0.0               0.0   \n888    888         0             1.0                1.0               1.0   \n889    889         1             0.0                0.0               0.0   \n890    890         0             0.0                1.0               0.0   \n\n     Embarked_1_fraction  Pclass_2_fraction  Sex_1_SibSp_1_fraction  \\\n0                    0.0                0.0                     0.5   \n1                    1.0                0.0                     1.0   \n2                    0.0                0.0                     0.5   \n3                    0.0                0.0                     1.0   \n4                    0.0                0.0                     0.0   \n..                   ...                ...                     ...   \n886                  0.0                1.0                     0.0   \n887                  0.0                0.0                     0.5   \n888                  0.0                0.0                     1.0   \n889                  1.0                0.0                     0.0   \n890                  0.0                0.0                     0.0   \n\n     Sex_1_Pclass_3_fraction  Sex_1_Pclass_2_fraction  \n0                        0.5                      0.0  \n1                        0.5                      0.5  \n2                        1.0                      0.5  \n3                        0.5                      0.5  \n4                        0.5                      0.0  \n..                       ...                      ...  \n886                      0.0                      0.5  \n887                      0.5                      0.5  \n888                      1.0                      0.5  \n889                      0.0                      0.0  \n890                      0.5                      0.0  \n\n[891 rows x 10 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Index</th>\n      <th>Survived</th>\n      <th>Sex_1_fraction</th>\n      <th>Pclass_3_fraction</th>\n      <th>SibSp_1_fraction</th>\n      <th>Embarked_1_fraction</th>\n      <th>Pclass_2_fraction</th>\n      <th>Sex_1_SibSp_1_fraction</th>\n      <th>Sex_1_Pclass_3_fraction</th>\n      <th>Sex_1_Pclass_2_fraction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.5</td>\n      <td>1.0</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.5</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <td>886</td>\n      <td>886</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <td>887</td>\n      <td>887</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <td>888</td>\n      <td>888</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <td>889</td>\n      <td>889</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>890</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.5</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>891 rows Ã 10 columns</p>\n</div>"},"metadata":{}}],"execution_count":61},{"cell_type":"code","source":"# import pandas as pd\n# from itertools import combinations\n# from collections import defaultdict\n\n# def subset_counts_oneHot(featureMaster, target, id):\n#     # Filter rows where the target is 1\n#     df_target_1 = featureMaster[featureMaster[target] == 1]\n    \n#     # Drop target and id columns to get only the one-hot encoded columns\n#     one_hot_columns = df_target_1.drop(columns=[target, id]).columns\n    \n#     # Dictionary to store subsets and their counts\n#     subset_counts = defaultdict(int)\n    \n#     # Helper function to check if a subset has more than two one-hot columns from the same original column\n#     def valid_subset(subset):\n#         original_cols = [col.split('_')[0] for col in subset]\n#         return all(original_cols.count(col) <= 2 for col in original_cols)\n    \n#     # Iterate over all possible subsets of one-hot encoded columns\n#     for r in range(1, len(one_hot_columns) + 1):\n#         for subset in combinations(one_hot_columns, r):\n#             if valid_subset(subset):\n#                 subset_df = df_target_1[list(subset)]\n#                 count = (subset_df.sum(axis=1) == len(subset)).sum()\n#                 subset_counts[subset] = count\n    \n#     return dict(subset_counts)\n\n# Example usage\ndata = {\n    'id': [1, 2, 3, 4],\n    'feature_1_A': [1, 0, 1, 0],\n    'feature_1_B': [0, 1, 0, 1],\n    'feature_2_X': [1, 0, 0, 1],\n    'feature_2_Y': [0, 1, 1, 0],\n    'target': [0, 1, 0, 1]\n}\ndf_encoded = pd.DataFrame(data)\n\nsubset_counts = subset_counts_oneHot(df_encoded, target='target', id='id')\nprint(subset_counts)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-23T13:12:08.314852Z","iopub.execute_input":"2024-10-23T13:12:08.315207Z","iopub.status.idle":"2024-10-23T13:12:08.335770Z","shell.execute_reply.started":"2024-10-23T13:12:08.315146Z","shell.execute_reply":"2024-10-23T13:12:08.334697Z"}},"outputs":[{"name":"stdout","text":"{('feature_1_A',): 0.0, ('feature_1_B',): 1.0, ('feature_2_X',): 0.5, ('feature_2_Y',): 0.5}\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"def filter_and_sort_subsets(subset_counts, threshold):\n    # Filter subsets based on the given threshold\n    filtered_subsets = {subset: count for subset, count in subset_counts.items() if count > threshold}\n    \n    # Sort the filtered subsets based on their counts in descending order\n    sorted_subsets = sorted(filtered_subsets.items(), key=lambda item: item[1], reverse=True)\n    \n    return sorted_subsets\n\n# Example usage\nsubset_counts = {\n    ('feature_1_A',): 2,\n    ('feature_2_X',): 1,\n    ('feature_1_A', 'feature_2_X'): 1,\n    ('feature_1_B', 'feature_2_Y'): 2\n}\nthreshold = 1.0\n\nfiltered_sorted_subsets = filter_and_sort_subsets(subset_counts, threshold)\nprint(filtered_sorted_subsets)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-23T13:15:08.590077Z","iopub.execute_input":"2024-10-23T13:15:08.590414Z","iopub.status.idle":"2024-10-23T13:15:08.598905Z","shell.execute_reply.started":"2024-10-23T13:15:08.590364Z","shell.execute_reply":"2024-10-23T13:15:08.597832Z"}},"outputs":[{"name":"stdout","text":"[(('feature_1_A',), 2), (('feature_1_B', 'feature_2_Y'), 2)]\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"import pandas as pd\n\ndef create_fractional_df(featureMaster, id_column, target_column, filtered_sorted_subsets):\n    # Create a copy of the dataframe to avoid modifying the original\n    df = featureMaster.copy()\n    \n    for subset, _ in filtered_sorted_subsets:\n        new_col_name = \"_\".join(subset) + \"_fraction\"\n        df[new_col_name] = df[list(subset)].mean(axis=1)\n    \n    # Return a dataframe with the id, target, and new fraction columns\n    new_fraction_columns = [(\"_\".join(subset) + \"_fraction\") for subset, _ in filtered_sorted_subsets]\n    selected_columns = [id_column, target_column] + new_fraction_columns\n    \n    return df[selected_columns]\n\n# Example usage\ndata = {\n    'id': [1, 2, 3, 4],\n    'feature_1_A': [1, 1, 1, 1],\n    'feature_1_B': [0, 1, 0, 1],\n    'feature_2_X': [1, 0, 0, 1],\n    'feature_2_Y': [0, 1, 1, 0],\n    'target': [0, 1, 0, 1]\n}\ndf_encoded = pd.DataFrame(data)\n\nfiltered_sorted_subsets = [\n    (('feature_1_A',), 2),\n    (('feature_1_B', 'feature_2_Y'), 2)\n]\n\nnew_df = create_fractional_df(df_encoded, id_column='id', target_column='target', filtered_sorted_subsets=filtered_sorted_subsets)\nprint(new_df)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-23T13:28:37.311070Z","iopub.execute_input":"2024-10-23T13:28:37.311446Z","iopub.status.idle":"2024-10-23T13:28:37.334191Z","shell.execute_reply.started":"2024-10-23T13:28:37.311388Z","shell.execute_reply":"2024-10-23T13:28:37.332591Z"}},"outputs":[{"name":"stdout","text":"   id  target  feature_1_A_fraction  feature_1_B_feature_2_Y_fraction\n0   1       0                   1.0                               0.0\n1   2       1                   1.0                               1.0\n2   3       0                   1.0                               0.5\n3   4       1                   1.0                               0.5\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ntestClass = tangiD_BinaryClassification(test_data, train_data, \"Survived\", \"TestData!\", ['Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch','Ticket', 'Fare', 'Cabin', 'Embarked'])\ntestData = testClass.medianIntifying(10)\ntestData","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom itertools import combinations\nfrom collections import defaultdict\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\ndef valid_subset(subset):\n    original_cols = [col.split('_')[0] for col in subset]\n    return all(original_cols.count(col) <= 1 for col in original_cols)\n\ndef calculate_subset_count(df, subset):\n    subset_df = df[list(subset)]\n    count = (subset_df.sum(axis=1) == len(subset)).sum()\n    return subset, count\n\ndef subset_counts_oneHot(featureMaster, target, id, n_cores=4):\n    df_target_1 = featureMaster[featureMaster[target] == 1]\n    one_hot_columns = df_target_1.drop(columns=[target, id]).columns\n\n    subset_counts = defaultdict(int)\n    \n    all_combinations = []\n    for r in range(1, len(one_hot_columns) + 1):\n        for subset in combinations(one_hot_columns, r):\n            if valid_subset(subset):\n                all_combinations.append(subset)\n    \n    with ThreadPoolExecutor(max_workers=n_cores) as executor:\n        futures = [executor.submit(calculate_subset_count, df_target_1, subset) for subset in all_combinations]\n        for future in as_completed(futures):\n            subset, count = future.result()\n            subset_counts[subset] = count / len(df_target_1)\n    \n    return dict(subset_counts)\n\ndef filter_and_sort_subsets(subset_counts, threshold):\n    filtered_subsets = {subset: count for subset, count in subset_counts.items() if count > threshold}\n    sorted_subsets = sorted(filtered_subsets.items(), key=lambda item: item[1], reverse=True)\n    return sorted_subsets\n\ndef create_filtered_df(featureMaster, id_column, target_column, filtered_sorted_subsets):\n    df = featureMaster.copy()\n    \n    for subset, _ in filtered_sorted_subsets:\n        new_col_name = \"_\".join(subset) + \"_fraction\"\n        df[new_col_name] = df[list(subset)].mean(axis=1)\n    \n    new_fraction_columns = [(\"_\".join(subset) + \"_fraction\") for subset, _ in filtered_sorted_subsets]\n    selected_columns = [id_column, target_column] + new_fraction_columns\n    \n    return df[selected_columns]\n\n# Example usage\ndata = {\n    'id': [1, 2, 3, 4],\n    'feature_1_A': [1, 0, 1, 0],\n    'feature_1_B': [0, 1, 0, 1],\n    'feature_2_X': [1, 0, 0, 1],\n    'feature_2_Y': [0, 1, 1, 0],\n    'target': [0, 1, 0, 1]\n}\ndf_encoded = pd.DataFrame(data)\n\n# Calculate subset counts with multithreading\nsubset_counts = subset_counts_oneHot(df_encoded, target='target', id='id', n_cores=4)\n\n# Filter and sort subsets\nthreshold = 0.5\nfiltered_sorted_subsets = filter_and_sort_subsets(subset_counts, threshold)\n\n# Create the filtered dataframe\nnew_df = create_filtered_df(df_encoded, id_column='id', target_column='target', filtered_sorted_subsets=filtered_sorted_subsets)\nprint(new_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-23T13:54:07.680080Z","iopub.execute_input":"2024-10-23T13:54:07.680426Z","iopub.status.idle":"2024-10-23T13:54:07.722197Z","shell.execute_reply.started":"2024-10-23T13:54:07.680373Z","shell.execute_reply":"2024-10-23T13:54:07.721184Z"}},"outputs":[{"name":"stdout","text":"   id  target  feature_1_B_fraction\n0   1       0                   0.0\n1   2       1                   1.0\n2   3       0                   0.0\n3   4       1                   1.0\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntrain_data.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ntest_data.head()\ntest_data.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n\ndef intersection_of_lists(list1, list2):\n    return list(set(list1) & set(list2))\n\n\ndef difference_of_lists(list1, list2):\n    return [item for item in list1 if item not in list2]\n\n\ndef get_numeric_and_non_numeric_columns(df):\n    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n    non_numeric_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()\n    print(f\"Numeric columns: {numeric_cols}\")\n    print(f\"Non-numeric columns: {non_numeric_cols}\")\n    return numeric_cols, non_numeric_cols\n\n\ndef remove_single_unique_or_all_nans(df):\n    removed_columns = []\n    for column in df.columns:\n        if df[column].nunique() <= 1 or df[column].isna().all():\n            removed_columns.append(column)\n            df = df.drop(columns=[column])\n    print(f\"Removed columns due to all NaN or only 1 unique value: {removed_columns}\")\n    return df, removed_columns\n\n\ndef columns_with_missing_values(df):\n    missing_cols = [col for col in df.columns if df[col].isna().any()]\n    print(f\"Missing data columns: {missing_cols}\")\n    return missing_cols\n\n\ndef fill_missingNumeric_with_median(df, missing_cols, numeric_cols):\n    for col in intersection_of_lists(missing_cols, numeric_cols):\n        median_value = df[col].median()\n        df[col].fillna(median_value, inplace=True)\n    print(\"Done inputing missing numeric values with median!\")\n    return df\n\n\ndef columnsCategory_with_more_than_X_percent_unique(df, categoric_cols, perc):\n    total_rows = len(df)\n    threshold = total_rows * 0.01 * perc  # 10% of the total number of rows\n    cols_with_high_uniques = [col for col in categoric_cols if df[col].nunique() > threshold]\n    print(f\"Columns with high uniques: {cols_with_high_uniques}\")\n    return cols_with_high_uniques\n    \n\n\ndef convert_and_create_integer_columns(df, new_columns, mappings, colName):\n    df[colName] = df[colName].astype('object')\n    df[colName], unique_values = pd.factorize(df[colName])\n    # Add the new column name to the list\n    new_columns.append(colName)\n    # Create a mapping dictionary for the column\n    mappings[colName] = {value: i for i, value in enumerate(unique_values)}\n    return df, new_columns, mappings\n\n\ndef fill_missing_and_predict(df, new_columns, mappings, usable_cols, column_name):\n    # Convert and create integer column\n    df, new_columns, mappings = convert_and_create_integer_columns(df, new_columns, mappings, column_name)\n    # Train the model to predict missing values\n    non_missing_idx = df[column_name] != -1  # Using -1 for factorized NaNs\n    missing_idx = df[column_name] == -1\n    if missing_idx.sum() > 0:\n        X_train = df.loc[non_missing_idx, usable_cols]\n        y_train = df.loc[non_missing_idx, column_name]\n        X_test = df.loc[missing_idx,  usable_cols]\n        model = LogisticRegression(max_iter=1000, solver ='lbfgs',  multi_class='auto')\n        model.fit(X_train, y_train)\n        # Predict the missing values\n        predicted = model.predict(X_test)\n        # Replace the missing values with the predicted values\n        df.loc[missing_idx, column_name] = predicted\n    return df, new_columns, mappings\n    \n\ndef get_bigrams(string):\n    # Generate bigrams from a string\n    return [string[i:i+2] for i in range(len(string)-1)]\n\ndef sorensen_dice(a, b):\n    # SÃ¸rensen-Dice coefficient for two sets\n    a_bigrams = set(get_bigrams(a))\n    b_bigrams = set(get_bigrams(b))\n    overlap = len(a_bigrams & b_bigrams)\n    total = len(a_bigrams) + len(b_bigrams)\n    if total == 0:\n        return 1.0 if a == b else 0.0  # Handle identical empty strings\n    return 2 * overlap / total\n\n\ndef calculate_meanDistanceFromAList(input_string, string_list):\n    sum_Levenshtein = 0\n    sum_sorensen_dice = 0\n    for string in string_list:\n        sum_Levenshtein = sum_Levenshtein + Levenshtein.distance(input_string, string)\n        sum_sorensen_dice = sum_sorensen_dice + sorensen_dice(input_string, string)\n    return float(sum_Levenshtein/len(string_list)),float(sum_sorensen_dice/len(string_list))\n    \n\ndef takeOut_stringList(df, target, variableCol):\n    return list(df[df[f\"{target}\"]==1][f\"{variableCol}\"].unique()),list(df[df[f\"{target}\"]==0][f\"{variableCol}\"].unique())\n\n\ndef apply_meanDistance(df, column_name, string_list):\n    # Calculate mean distances for each row and add a new column\n    df[['mean_Levenshtein', 'mean_sorensen_dice']] = df[column_name].apply(\n        lambda x: pd.Series(calculate_meanDistanceFromAList(x, string_list))\n    )\n    return df\n\n\ndef create_DistanceMetric(df, new_columns, usable_cols, colName, target, orig_data):\n    df[colName] = df[colName].astype('str')\n    true_NameList, false_NameList = takeOut_stringList(orig_data, target, colName)\n    new_columns.append(colName)\n    colName_true_lev = str(colName+\"_true_lev\")\n    colName_true_reg = str(colName+\"_true_reg\")\n    df[[colName_true_lev, colName_true_reg]] = df[colName].apply(\n        lambda x: pd.Series(calculate_meanDistanceFromAList(x, true_NameList))\n    )\n    colName_false_lev = str(colName+\"_false_lev\")\n    colName_false_reg = str(colName+\"_false_reg\")\n    df[[colName_false_lev, colName_false_reg]] = df[colName].apply(\n        lambda x: pd.Series(calculate_meanDistanceFromAList(x, false_NameList))\n    )\n    usable_cols = usable_cols + [colName_true_lev, colName_true_reg, colName_false_lev, colName_false_reg]\n    return df, new_columns, usable_cols\n\n\ndef convert_All_integer_columns(df, numeric_cols, missing_cols, categoric_cols, cols_with_high_uniques, target, orig_data):\n    new_columns = []\n    mappings = {}\n    usable_cols = numeric_cols \n    \n    categoric_nonNA_cols = difference_of_lists(categoric_cols, missing_cols)\n\n    categoric_nonNA_Few_cols = difference_of_lists(categoric_nonNA_cols, cols_with_high_uniques)\n    \n    categoric_nonNA_Multiple_cols = difference_of_lists(categoric_nonNA_cols, categoric_nonNA_Few_cols)\n    \n    categoric_NA_Few_cols = difference_of_lists(missing_cols, cols_with_high_uniques)\n\n    categoric_NA_Multiple_cols = difference_of_lists(missing_cols, categoric_NA_Few_cols)\n    \n    for col in categoric_nonNA_Few_cols:\n        df, new_columns, mappings = convert_and_create_integer_columns(df, new_columns, mappings, col)\n        print(f\"[No NA values][Less Unique Values] Categoric columns Converted to Integer: {col}\")\n    usable_cols = usable_cols + categoric_nonNA_Few_cols\n    for col in  categoric_NA_Few_cols:   \n        df, new_columns, mappings = fill_missing_and_predict(df, new_columns, mappings, usable_cols, col)\n        print(f\"[NA values][Less Unique Values] Categoric columns Converted to Integer and Missing Are Predicted: {col}\")\n        usable_cols = usable_cols + [col]    \n    for col in categoric_nonNA_Multiple_cols:   \n        df, new_columns, usable_cols = create_DistanceMetric(df, new_columns, usable_cols, col, target, orig_data)\n        df = df.drop(columns=[col])\n        print(f\"[No NA values][Multiple Unique Values] Categoric columns Converted to Distance Based On Cluster: {col}\")\n    for col in  categoric_NA_Multiple_cols:\n        df = df.drop(columns=[col])\n        print(f\"[NA values][Multiple Unique Values] Categoric columns Converted to Distance Based On Cluster and Missing Are Predicted: {col}\")  \n    print(f\"Mappings: {mappings}\")\n    return df, new_columns, mappings","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_cleaned, removed_columns = remove_single_unique_or_all_nans(train_data[['Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']].copy())\nnumeric_cols, non_numeric_cols = get_numeric_and_non_numeric_columns(df_cleaned)\nmissing_cols = columns_with_missing_values(df_cleaned)\nfilledNumeric_df = fill_missingNumeric_with_median(df_cleaned, missing_cols, numeric_cols)\nmissing_cols = columns_with_missing_values(filledNumeric_df)\nhigh_uniques = columnsCategory_with_more_than_X_percent_unique(filledNumeric_df, non_numeric_cols, 10)\nupdated_df_train, new_columns_train, mappings_train = convert_All_integer_columns(filledNumeric_df, numeric_cols, missing_cols, non_numeric_cols, high_uniques, 'Survived', train_data)\ntrain_df = pd.concat([updated_df_train, train_data['Survived']], axis=1)\ntrain_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport dask.dataframe as dd\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nimport statsmodels.api as sm\n\ndef very_fast_backward_feature_selection(df, target, n_features_to_select, models_dict, n_jobs):\n    # Convert pandas DataFrame to Dask DataFrame\n    data = dd.from_pandas(df, npartitions=10)\n    \n    # Prepare the feature matrix and target vector\n    X = data.drop(columns=[target])\n    y = data[target]\n    \n    results = {}\n    all_selected_features = set()\n    \n    for model_name, model in models_dict.items():\n        try:\n            # Initialize SFS with the model\n            sfs = SFS(model, \n                      k_features=n_features_to_select, \n                      forward=False, \n                      floating=False, \n                      scoring='accuracy', \n                      cv=2, \n                      n_jobs=n_jobs)\n            \n            # Convert Dask DataFrame to pandas for fitting\n            X_pandas = X.compute()\n            y_pandas = y.compute()\n            \n            # Fit SFS\n            sfs = sfs.fit(X_pandas, y_pandas)\n            \n            # Get the names of the selected features\n            selected_features = list(sfs.k_feature_names_)\n            \n            # Add selected features to the union set\n            all_selected_features.update(selected_features)\n            \n            # Fit model using statsmodels for p-values and coefficients\n            X_selected = sm.add_constant(X_pandas[selected_features])\n            sm_model = sm.OLS(y_pandas, X_selected).fit()\n            \n            summary = sm_model.summary2().tables[1]\n            \n            # Print the summary\n            print(f\"Model: {model_name}\")\n            print(sm_model.summary())\n            \n            # Store the selected features and model summary\n            results[model_name] = {\n                'selected_features': selected_features,\n                'model_summary': summary\n            }\n            \n        except Exception as e:\n            print(f\"Error processing model {model_name}: {e}\")\n    \n    # Convert the set of all selected features to a list\n    all_selected_features = list(all_selected_features)\n    print(\"Union of all selected features:\", all_selected_features)\n    return results, all_selected_features","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"FS_models_dict = {\n    'NaiveBayes': GaussianNB(),\n    'DecisionTreeClassifier': DecisionTreeClassifier(),\n    'GradientBoostingClassifier': GradientBoostingClassifier(learning_rate=0.01),\n    'RandomForestClassifier': RandomForestClassifier(n_estimators=10, max_depth=2, random_state=42)\n}\nresults, selected_features = very_fast_backward_feature_selection(train_df, 'Survived', n_features_to_select=7, models_dict=FS_models_dict, n_jobs=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"selected_features","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport dask.dataframe as dd\nfrom dask_ml.model_selection import GridSearchCV\nimport optuna\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import make_scorer, accuracy_score\nfrom dask.distributed import Client\n\n# Start Dask client\nclient = Client()\n\ndef optimize_models(models_dict, X, y, n_trials=20, n_jobs=1):\n    best_models = {}\n    best_scores = {}\n\n    # Convert to Dask DataFrame\n    X = dd.from_pandas(X, npartitions=10)\n    y = dd.from_pandas(y, npartitions=10)\n\n    # Objective function to optimize\n    def objective(trial, model_name):\n        model = models_dict[model_name]\n\n        if model_name == 'RandomForestClassifier':\n            param_grid = {\n                'n_estimators': trial.suggest_int('n_estimators', 1, 10),\n                'max_depth': trial.suggest_int('max_depth', 1, 4),\n                'min_samples_split': trial.suggest_int('min_samples_split', 2, 4),\n                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 4)\n            }\n        elif model_name == 'GradientBoostingClassifier':\n            param_grid = {\n                'n_estimators': trial.suggest_int('n_estimators', 1, 10),\n                'max_depth': trial.suggest_int('max_depth', 1, 4)\n            }\n        \n        # Perform cross-validation\n        gs = GridSearchCV(model, param_grid, cv=3, scoring=make_scorer(accuracy_score), n_jobs=n_jobs)\n        gs.fit(X.compute(), y.compute())\n        return gs.best_score_\n\n    # Create a study object and optimize the objective function for each model\n    for model_name in models_dict.keys():\n        print(f\"Optimizing {model_name}...\")\n        study = optuna.create_study(direction='maximize')\n        study.optimize(lambda trial: objective(trial, model_name), n_trials=n_trials, n_jobs=n_jobs)\n\n        # Store best hyperparameters and score\n        best_models[model_name] = study.best_params\n        best_scores[model_name] = study.best_value\n\n        # Print best hyperparameters and score\n        print(f\"Best hyperparameters for {model_name}: {study.best_params}\")\n        print(f\"Best score for {model_name}: {study.best_value}\\n\")\n\n    return best_models, best_scores\n\ndef update_model_params(models_dict, best_params):\n    # Update each model with the best hyperparameters\n    for model_name, params in best_params.items():\n        model = models_dict[model_name]\n        model.set_params(**params)\n    return models_dict\n\n# Example usage\ndata = pd.DataFrame({\n    'feature_1': np.random.normal(size=1000000),\n    'feature_2': np.random.normal(size=1000000),\n    'feature_3': np.random.normal(size=1000000),\n    'response': np.random.randint(0, 2, size=1000000)\n})\n\nX = data.drop(columns=['response'])\ny = data['response']\n\nmodels_dict = {\n    'RandomForestClassifier': RandomForestClassifier(),\n    'GradientBoostingClassifier': GradientBoostingClassifier()\n}\n\nbest_models, best_scores = optimize_models(models_dict, X, y, n_trials=20, n_jobs=1)\nprint(\"Best models:\", best_models)\nprint(\"Best scores:\", best_scores)-1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_cleaned, removed_columns = remove_single_unique_or_all_nans(test_data[['Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']].copy())\nnumeric_cols, non_numeric_cols = get_numeric_and_non_numeric_columns(df_cleaned)\nmissing_cols = columns_with_missing_values(df_cleaned)\nfilledNumeric_df = fill_missingNumeric_with_median(df_cleaned, missing_cols, numeric_cols)\nmissing_cols = columns_with_missing_values(filledNumeric_df)\nhigh_uniques = columnsCategory_with_more_than_X_percent_unique(filledNumeric_df, non_numeric_cols, 10)\nupdated_df_test, new_columns_test, mappings_test = convert_All_integer_columns(filledNumeric_df, numeric_cols, missing_cols, non_numeric_cols, high_uniques, 'Survived', train_data)\nupdated_df_test","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport joblib\nimport statsmodels.api as sm\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nimport optuna\nfrom sklearn.model_selection import cross_val_score\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\ndef very_fast_backward_feature_selection(data, target, n_features_to_select, models_dict, n_jobs):\n    # Prepare the feature matrix and target vector\n    X = data.drop(columns=[target])\n    y = data[target]\n    \n    results = {}\n    \n    for model_name, model in models_dict.items():\n        # Initialize SFS with the model\n        sfs = SFS(model, \n                  k_features=n_features_to_select, \n                  forward=False, \n                  floating=False, \n                  scoring='accuracy', \n                  cv=2, \n                  n_jobs=n_jobs)\n        \n        # Fit SFS\n        sfs = sfs.fit(X, y)\n        \n        # Get the names of the selected features\n        selected_features = list(sfs.k_feature_names_)\n        \n        # Fit model using statsmodels for p-values and coefficients\n        X_selected = sm.add_constant(X[selected_features])\n        sm_model = sm.OLS(y, X_selected).fit()\n        \n        summary = sm_model.summary2().tables[1]\n        \n        # Print the summary\n        print(f\"Model: {model_name}\")\n        print(sm_model.summary())\n        \n        # Store the selected features and model summary\n        results[model_name] = {\n            'selected_features': selected_features,\n            'model_summary': summary\n        }\n    \n    return results\n\n\ndef optimize_models(models_dict, X, y, n_trials=20, n_jobs=1):\n    best_models = {}\n    best_scores = {}\n\n    # Objective function to optimize\n    def objective(trial, model_name):\n        model = models_dict[model_name]\n\n        if model_name == 'RandomForestClassifier':\n            model.set_params(\n                n_estimators=trial.suggest_int('n_estimators', 1, 10),\n                max_depth=trial.suggest_int('max_depth', 1, 4),\n                min_samples_split=trial.suggest_int('min_samples_split', 2, 4),\n                min_samples_leaf=trial.suggest_int('min_samples_leaf', 1, 4)\n            )\n            \n        elif model_name == 'GradientBoostingClassifier':\n            model.set_params(\n                n_estimators=trial.suggest_int('n_estimators', 1, 10),\n                # learning_rate=trial.suggest_float('learning_rate', 0.01, 0.1),\n                max_depth=trial.suggest_int('max_depth', 1, 4)\n            )\n            \n        elif model_name == 'XGBClassifier':\n            model.set_params(\n                n_estimators=trial.suggest_int('n_estimators', 1, 10),\n                # learning_rate=trial.suggest_float('learning_rate', 0.01, 0.1),\n                max_depth=trial.suggest_int('max_depth', 1, 4)\n            )\n            \n        elif model_name == 'LGBMClassifier':\n            model.set_params(\n                n_estimators=trial.suggest_int('n_estimators', 1, 10),\n                # learning_rate=trial.suggest_float('learning_rate', 0.01, 0.1),\n                max_depth=trial.suggest_int('max_depth', 1, 4)\n            )\n            \n        elif model_name == 'KNeighborsClassifier':\n            model.set_params(\n                n_neighbors=trial.suggest_int('n_neighbors', 1, 10),\n                leaf_size=trial.suggest_int('leaf_size', 10, 30),\n                p=trial.suggest_int('p', 1, 2)\n            )\n            \n        elif model_name == 'SupportVectorClassifier':\n            model.set_params(\n                # C=trial.suggest_float('C', 0.1, 10.0),\n                kernel=trial.suggest_categorical('kernel', ['linear', 'rbf']),\n                gamma=trial.suggest_categorical('gamma', ['scale', 'auto'])\n            )\n\n        elif model_name == 'DecisionTreeClassifier':\n            model.set_params(\n                max_depth=trial.suggest_int('max_depth', 1, 3),\n                min_samples_split=trial.suggest_int('min_samples_split', 2, 4),\n                min_samples_leaf=trial.suggest_int('min_samples_leaf', 1, 4)\n            )\n\n        # Perform cross-validation\n        scores = cross_val_score(model, X, y, cv=3, scoring='accuracy')\n        return scores.mean()\n\n    # Create a study object and optimize the objective function for each model\n    for model_name in models_dict.keys():\n        print(f\"Optimizing {model_name}...\")\n        study = optuna.create_study(direction='maximize')\n        study.optimize(lambda trial: objective(trial, model_name), n_trials=n_trials, n_jobs=n_jobs)  # Use n_jobs for parallel execution\n        \n        # Store best hyperparameters and score\n        best_models[model_name] = study.best_params\n        best_scores[model_name] = study.best_value\n\n        # Print best hyperparameters and score\n        print(f\"Best hyperparameters for {model_name}: {study.best_params}\")\n        print(f\"Best score for {model_name}: {study.best_value}\\n\")\n\n    return best_models, best_scores\n\n\ndef update_model_params(models_dict, best_params):\n    # Update each model with the best hyperparameters\n    for model_name, params in best_params.items():\n        model = models_dict[model_name]\n        model.set_params(**params)\n    return models_dict\n\n\ndef fit_models(models_dict, X, y, save_path='/kaggle/working/'):\n    fitted_models = {}\n\n    for model_name, model in models_dict.items():\n        print(f\"Fitting {model_name}...\")\n        # Fit the model\n        model.fit(X, y)\n        \n        # Save the fitted model\n        model_filename = f\"{save_path}{model_name}.joblib\"\n        joblib.dump(model, model_filename)\n        \n        # Store the model in the dictionary\n        fitted_models[model_name] = model\n\n    return fitted_models","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport dask.dataframe as dd\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nimport statsmodels.api as sm\n\ndef very_fast_backward_feature_selection(df, target, n_features_to_select, models_dict, n_jobs):\n    # Convert pandas DataFrame to Dask DataFrame\n    data = dd.from_pandas(df, npartitions=10)\n    \n    # Prepare the feature matrix and target vector\n    X = data.drop(columns=[target])\n    y = data[target]\n    \n    results = {}\n    all_selected_features = set()\n    \n    for model_name, model in models_dict.items():\n        try:\n            # Initialize SFS with the model\n            sfs = SFS(model, \n                      k_features=n_features_to_select, \n                      forward=False, \n                      floating=False, \n                      scoring='accuracy', \n                      cv=2, \n                      n_jobs=n_jobs)\n            \n            # Convert Dask DataFrame to pandas for fitting\n            X_pandas = X.compute()\n            y_pandas = y.compute()\n            \n            # Fit SFS\n            sfs = sfs.fit(X_pandas, y_pandas)\n            \n            # Get the names of the selected features\n            selected_features = list(sfs.k_feature_names_)\n            \n            # Add selected features to the union set\n            all_selected_features.update(selected_features)\n            \n            # Fit model using statsmodels for p-values and coefficients\n            X_selected = sm.add_constant(X_pandas[selected_features])\n            sm_model = sm.OLS(y_pandas, X_selected).fit()\n            \n            summary = sm_model.summary2().tables[1]\n            \n            # Print the summary\n            print(f\"Model: {model_name}\")\n            print(sm_model.summary())\n            \n            # Store the selected features and model summary\n            results[model_name] = {\n                'selected_features': selected_features,\n                'model_summary': summary\n            }\n            \n        except Exception as e:\n            print(f\"Error processing model {model_name}: {e}\")\n    \n    # Convert the set of all selected features to a list\n    all_selected_features = list(all_selected_features)\n    \n    return results, all_selected_features\n\n# Example usage\n# Sample large dataset\ndata = pd.DataFrame({\n    'feature_1': np.random.normal(size=1000000),\n    'feature_2': np.random.normal(size=1000000),\n    'feature_3': np.random.normal(size=1000000),\n    'response': np.random.randint(0, 2, size=1000000)\n})\n\nmodels_dict = {\n    'LogisticRegression': LogisticRegression(max_iter=1000),\n    'RandomForestClassifier': RandomForestClassifier(n_estimators=10, max_depth=2, random_state=42),\n}\n\nresults, all_selected_features = very_fast_backward_feature_selection(data, 'response', n_features_to_select=2, models_dict=models_dict, n_jobs=1)\nprint(\"Results:\", results)\nprint(\"Union of all selected features:\", all_selected_features)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"FS_models_dict = {\n    'LogisticRegression': LogisticRegression(max_iter=1000, solver='liblinear', penalty='l2'),\n    'SupportVectorClassifier': SVC(probability=True, gamma='auto')\n}\nfinal_models_dict = {\n    'LogisticRegression': LogisticRegression(max_iter=1000, solver='liblinear', penalty='l2'),\n    'RandomForestClassifier': RandomForestClassifier(),\n    'SupportVectorClassifier': SVC(probability=True, C=1),\n    'KNeighborsClassifier': KNeighborsClassifier(),\n    'NaiveBayes': GaussianNB(),\n    'DecisionTreeClassifier': DecisionTreeClassifier(),\n    'GradientBoostingClassifier': GradientBoostingClassifier(learning_rate=0.01),\n    'XGBClassifier': XGBClassifier(use_label_encoder=False, eval_metric='logloss', learning_rate=0.01),\n    'LGBMClassifier': LGBMClassifier()\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = very_fast_backward_feature_selection(train_df, 'Survived', n_features_to_select=1, models_dict=FS_models_dict, n_jobs=1)\nX = train_df.drop(columns=['Survived'])\ny = train_df['Survived']\nbest_models, best_scores = optimize_models(final_models_dict, X, y, n_trials=1, n_jobs=1)  # Using n_jobs=4 for parallel execution\nprint(\"Best models:\", best_models)\nprint(\"Best scores:\", best_scores)\nto_fitModels = update_model_params(final_models_dict, best_models)\nfitted_models = fit_models(to_fitModels, X, y, save_path='/kaggle/working/')\nprint(\"Models fitted and saved.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import joblib\nimport pandas as pd\n\ndef predict_with_models(models_dict, X_new, save_path='/kaggle/working/'):\n    predictions = {}\n    \n    for model_name in models_dict.keys():\n        try:\n            # Load the fitted model\n            model_filename = f\"{save_path}{model_name}.joblib\"\n            model = joblib.load(model_filename)\n            \n            # Make predictions\n            predictions[model_name] = model.predict(X_new)\n        except:\n            pass\n    \n    # Convert predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions)\n    \n    return predictions_df\npredictions_df = predict_with_models(final_models_dict, updated_df_test, save_path='/kaggle/working/')\npredictions_df.head(418)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"estimators1 = [\n    ('LogisticRegression', LogisticRegression(max_iter=1000, solver='liblinear', penalty='l2')),\n    ('RandomForestClassifier', RandomForestClassifier(n_estimators=2, max_depth=3, min_samples_split=4, min_samples_leaf=3)),\n    ('SupportVectorClassifier', SVC(probability=True, C=1, kernel='linear', gamma='scale')),\n    ('KNeighborsClassifier', KNeighborsClassifier(n_neighbors=7, leaf_size=23, p=2)),\n    ('NaiveBayes', GaussianNB()),\n    ('DecisionTreeClassifier', DecisionTreeClassifier(max_depth=1, min_samples_split=2, min_samples_leaf=3)),\n    ('GradientBoostingClassifier', GradientBoostingClassifier(n_estimators=5, max_depth=2, learning_rate=0.01)),\n    ('XGBClassifier', XGBClassifier(use_label_encoder=False, eval_metric='logloss',n_estimators=7, max_depth=3,learning_rate=0.01)),\n    ('LGBMClassifier', LGBMClassifier(n_estimators=2, max_depth=4))\n]\n\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nX = train_df.drop(columns=['Survived'])\ny = train_df['Survived']\nVotingClass = VotingClassifier(estimators=estimators1, voting='hard')\nVotingClass.fit(X, y)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions = VotingClass.predict(updated_df_test)\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}